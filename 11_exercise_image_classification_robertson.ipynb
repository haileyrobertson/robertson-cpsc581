{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haileyrobertson/robertson-cpsc581/blob/main/11_exercise_image_classification_robertson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WYwyrVHN7iO"
      },
      "source": [
        "**Exercise 11: Image classification with Neural Networks**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "*Student: Hailey Robertson*\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "\n",
        "4. Move the 11_exercise_exercise_image_classification.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/11_exercise_exercise_image_classification.ipynb\n",
        "```\n",
        "\n",
        "5. Prior to starting this exercise, please create a directory called 'data' within your 'Exercises' directory and within 'data' create a directory called 'exercise_11', i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/data/exercise_11\n",
        "```\n",
        "\n",
        "6. Set up GPU runtime by selecting `Runtime` on the top tool bar, then selecting `Change runtime type` in the drop-down menu, selecting `GPU` under Hardware accelerator and clicking `Save`.\n",
        "\n",
        "\n",
        "\n",
        "In this exercise, we will create a simple neural network model and a logistic regression model for classifying images. We will experiment with learning rate, batch size, and different configurations of layers within the network. We will demonstrate this on the CIFAR-10 dataset. Note: Accuracy of Neural Network should exceed 52%.\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training and testing scores.\n",
        "\n",
        "```\n",
        "Training neural_network model\n",
        "Epoch=1/50  Loss: 2.241\n",
        "Epoch=2/50  Loss: 1.969\n",
        "Epoch=3/50  Loss: 1.809\n",
        "Epoch=4/50  Loss: 1.708\n",
        "Epoch=5/50  Loss: 1.640\n",
        "Epoch=6/50  Loss: 1.589\n",
        "Epoch=7/50  Loss: 1.537\n",
        "Epoch=8/50  Loss: 1.496\n",
        "Epoch=9/50  Loss: 1.465\n",
        "Epoch=10/50  Loss: 1.435\n",
        "Epoch=11/50  Loss: 1.399\n",
        "Epoch=12/50  Loss: 1.383\n",
        "Epoch=13/50  Loss: 1.337\n",
        "Epoch=14/50  Loss: 1.317\n",
        "Epoch=15/50  Loss: 1.286\n",
        "Epoch=16/50  Loss: 1.264\n",
        "Epoch=17/50  Loss: 1.228\n",
        "Epoch=18/50  Loss: 1.219\n",
        "Epoch=19/50  Loss: 1.185\n",
        "Epoch=20/50  Loss: 1.169\n",
        "Epoch=21/50  Loss: 1.137\n",
        "Epoch=22/50  Loss: 1.120\n",
        "Epoch=23/50  Loss: 1.092\n",
        "Epoch=24/50  Loss: 1.070\n",
        "Epoch=25/50  Loss: 1.039\n",
        "Epoch=26/50  Loss: 1.022\n",
        "Epoch=27/50  Loss: 0.994\n",
        "Epoch=28/50  Loss: 0.975\n",
        "Epoch=29/50  Loss: 0.945\n",
        "Epoch=30/50  Loss: 0.928\n",
        "Epoch=31/50  Loss: 0.895\n",
        "Epoch=32/50  Loss: 0.882\n",
        "Epoch=33/50  Loss: 0.849\n",
        "Epoch=34/50  Loss: 0.824\n",
        "Epoch=35/50  Loss: 0.797\n",
        "Epoch=36/50  Loss: 0.782\n",
        "Epoch=37/50  Loss: 0.747\n",
        "Epoch=38/50  Loss: 0.728\n",
        "Epoch=39/50  Loss: 0.700\n",
        "Epoch=40/50  Loss: 0.682\n",
        "Epoch=41/50  Loss: 0.651\n",
        "Epoch=42/50  Loss: 0.629\n",
        "Epoch=43/50  Loss: 0.597\n",
        "Epoch=44/50  Loss: 0.588\n",
        "Epoch=45/50  Loss: 0.553\n",
        "Epoch=46/50  Loss: 0.541\n",
        "Epoch=47/50  Loss: 0.507\n",
        "Epoch=48/50  Loss: 0.491\n",
        "Epoch=49/50  Loss: 0.463\n",
        "Epoch=50/50  Loss: 0.449\n",
        "\n",
        "Evaluating neural_network model from ./checkpoint-neural_network.pth\n",
        "Mean accuracy over 10000 images: 55.310%\n",
        "\n",
        "---\n",
        "\n",
        "Training logistic_regression model\n",
        "Epoch=1/50  Loss: 1.973\n",
        "Epoch=2/50  Loss: 1.880\n",
        "Epoch=3/50  Loss: 1.835\n",
        "Epoch=4/50  Loss: 1.824\n",
        "Epoch=5/50  Loss: 1.796\n",
        "Epoch=6/50  Loss: 1.794\n",
        "Epoch=7/50  Loss: 1.773\n",
        "Epoch=8/50  Loss: 1.768\n",
        "Epoch=9/50  Loss: 1.740\n",
        "Epoch=10/50  Loss: 1.740\n",
        "Epoch=11/50  Loss: 1.719\n",
        "Epoch=12/50  Loss: 1.729\n",
        "Epoch=13/50  Loss: 1.718\n",
        "Epoch=14/50  Loss: 1.710\n",
        "Epoch=15/50  Loss: 1.705\n",
        "Epoch=16/50  Loss: 1.702\n",
        "Epoch=17/50  Loss: 1.690\n",
        "Epoch=18/50  Loss: 1.694\n",
        "Epoch=19/50  Loss: 1.687\n",
        "Epoch=20/50  Loss: 1.685\n",
        "Epoch=21/50  Loss: 1.676\n",
        "Epoch=22/50  Loss: 1.672\n",
        "Epoch=23/50  Loss: 1.681\n",
        "Epoch=24/50  Loss: 1.669\n",
        "Epoch=25/50  Loss: 1.662\n",
        "Epoch=26/50  Loss: 1.660\n",
        "Epoch=27/50  Loss: 1.663\n",
        "Epoch=28/50  Loss: 1.658\n",
        "Epoch=29/50  Loss: 1.658\n",
        "Epoch=30/50  Loss: 1.655\n",
        "Epoch=31/50  Loss: 1.652\n",
        "Epoch=32/50  Loss: 1.652\n",
        "Epoch=33/50  Loss: 1.650\n",
        "Epoch=34/50  Loss: 1.646\n",
        "Epoch=35/50  Loss: 1.643\n",
        "Epoch=36/50  Loss: 1.643\n",
        "Epoch=37/50  Loss: 1.643\n",
        "Epoch=38/50  Loss: 1.641\n",
        "Epoch=39/50  Loss: 1.640\n",
        "Epoch=40/50  Loss: 1.639\n",
        "Epoch=41/50  Loss: 1.636\n",
        "Epoch=42/50  Loss: 1.637\n",
        "Epoch=43/50  Loss: 1.635\n",
        "Epoch=44/50  Loss: 1.635\n",
        "Epoch=45/50  Loss: 1.632\n",
        "Epoch=46/50  Loss: 1.632\n",
        "Epoch=47/50  Loss: 1.632\n",
        "Epoch=48/50  Loss: 1.631\n",
        "Epoch=49/50  Loss: 1.631\n",
        "Epoch=50/50  Loss: 1.630\n",
        "\n",
        "Evaluating logistic_regression model from ./checkpoint-logistic_regression.pth\n",
        "Mean accuracy over 10000 images: 40.560%\n",
        "\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: N/A\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjvZjKr-N_qh"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0VFIdrB0u58",
        "outputId": "ffd8e59c-0dd5-4e7f-abf0-417b5e9ce9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "oGW3cccIofOE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch, torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pft5E9Cy4w4I"
      },
      "source": [
        "Hyper-parameters for training neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "uZzL5RcuTJQa"
      },
      "outputs": [],
      "source": [
        "# TODO: Choose hyper-parameters\n",
        "\n",
        "# Model - either neural network or logistic regression\n",
        "MODEL_NAME = 'logistic_regression'\n",
        "\n",
        "# Batch size - number of images within a training batch of one training iteration\n",
        "N_BATCH = 128\n",
        "\n",
        "# Training epoch - number of passes through the full training dataset\n",
        "N_EPOCH = 50\n",
        "\n",
        "# Learning rate - step size to update parameters\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Learning rate decay - scaling factor to decrease learning rate at the end of each decay period\n",
        "LEARNING_RATE_DECAY = 0.9\n",
        "\n",
        "# Learning rate decay period - number of epochs before reducing/decaying learning rate\n",
        "LEARNING_RATE_DECAY_PERIOD = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHOCFhqgVtO8"
      },
      "source": [
        "Define Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "9YxBsFGBost_"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(torch.nn.Module):\n",
        "    '''\n",
        "    Neural network class of fully connected layers\n",
        "\n",
        "    Arg(s):\n",
        "        n_input_feature : int\n",
        "            number of input features\n",
        "        n_output : int\n",
        "            number of output classes\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_input_feature, n_output):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        # Create your 6-layer neural network using fully connected layers with ReLU activations\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
        "\n",
        "        # DONE: Instantiate 5 fully connected layers\n",
        "        self.fully_connected_layer_1 = torch.nn.Linear(n_input_feature, 1024)\n",
        "        self.fully_connected_layer_2 = torch.nn.Linear(1024, 512)\n",
        "        self.fully_connected_layer_3 = torch.nn.Linear(512, 256)\n",
        "        self.fully_connected_layer_4 = torch.nn.Linear(256, 128)\n",
        "        self.fully_connected_layer_5 = torch.nn.Linear(128, 64)\n",
        "\n",
        "        # DONE: Define output layer\n",
        "        self.output = torch.nn.Linear(64, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward pass through the neural network\n",
        "\n",
        "        Arg(s):\n",
        "            x : torch.Tensor[float32]\n",
        "                tensor of N x d\n",
        "        Returns:\n",
        "            torch.Tensor[float32]\n",
        "                tensor of n_output predicted class\n",
        "        '''\n",
        "\n",
        "        # DONE?: Implement forward function\n",
        "        output_fc1 = torch.relu(self.fully_connected_layer_1(x))\n",
        "        output_fc2 = torch.relu(self.fully_connected_layer_2(output_fc1))\n",
        "        output_fc3 = torch.relu(self.fully_connected_layer_3(output_fc2))\n",
        "        output_fc4 = torch.relu(self.fully_connected_layer_4(output_fc3))\n",
        "        output_fc5 = torch.relu(self.fully_connected_layer_5(output_fc4))\n",
        "\n",
        "        output_logits = self.output(output_fc5)\n",
        "\n",
        "        return output_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Rw2cyDGfw5kE"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    '''\n",
        "    Logistic regression class\n",
        "\n",
        "    Arg(s):\n",
        "        n_input_feature : int\n",
        "            number of input features\n",
        "        n_output : int\n",
        "            number of output classes\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_input_feature, n_output):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "\n",
        "        # Create your logistic regression model using a fully connected layer\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "\n",
        "        # DONE: Define linear layer\n",
        "        self.linear = torch.nn.Linear(n_input_feature, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward pass through the neural network\n",
        "\n",
        "        Arg(s):\n",
        "            x : torch.Tensor[float32]\n",
        "                tensor of N x d\n",
        "        Returns:\n",
        "            torch.Tensor[float32]\n",
        "                tensor of n_output predicted class\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement forward function\n",
        "        output_logits = self.linear(x)\n",
        "\n",
        "        return output_logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdnyLTLbZxTb"
      },
      "source": [
        "Define training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "zTEHZSxEZxsE"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          dataloader,\n",
        "          n_epoch,\n",
        "          optimizer,\n",
        "          learning_rate_decay,\n",
        "          learning_rate_decay_period,\n",
        "          device):\n",
        "    '''\n",
        "    Trains the model using optimizer and specified learning rate schedule\n",
        "\n",
        "    Arg(s):\n",
        "        model : torch.nn.Module\n",
        "            neural network or logistic regression\n",
        "        dataloader : torch.utils.data.DataLoader\n",
        "            # https://pytorch.org/docs/stable/data.html\n",
        "            dataloader for training data\n",
        "        n_epoch : int\n",
        "            number of epochs to train\n",
        "        optimizer : torch.optim\n",
        "            https://pytorch.org/docs/stable/optim.html\n",
        "            optimizer to use for updating weights\n",
        "        learning_rate_decay : float\n",
        "            rate of learning rate decay\n",
        "        learning_rate_decay_period : int\n",
        "            period to reduce learning rate based on decay e.g. every 2 epoch\n",
        "        device : str\n",
        "            device to run on\n",
        "    Returns:\n",
        "        torch.nn.Module : trained network\n",
        "    '''\n",
        "\n",
        "    device = 'cuda' if device == 'gpu' or device == 'cuda' else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    # DONE: Move model to device using .to()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # DONE: Define cross entropy loss (note: torch.nn.CrossEntropyLoss takes logits as inputs)\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "\n",
        "        # Accumulate total loss for each epoch\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # DONE: Decrease learning rate when learning rate decay period is met\n",
        "        # Directly modify param_groups in optimizer to set new learning rate\n",
        "        # e.g. decrease learning rate by a factor of decay rate every 2 epoch\n",
        "        if epoch % learning_rate_decay_period == 0 and epoch != 0:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= learning_rate_decay\n",
        "\n",
        "        # DONE?: Enumerate through batches of (images, labels) from dataloader\n",
        "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "            # DONE: Move images and labels to device using .to()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # DONE?: Vectorize images from (N, H, W, C) to (N, d)\n",
        "            n_dim = images.shape[1] * images.shape[2] * images.shape[3]\n",
        "            images = images.view(-1, n_dim)\n",
        "\n",
        "            # DONE: Forward through the model\n",
        "            outputs = model(images)\n",
        "\n",
        "            # DONE: Clear gradients so we don't accumlate them from previous batches\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # DONE: Compute loss function\n",
        "            loss = loss_func(outputs, labels)\n",
        "\n",
        "            # DONE: Update parameters by backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # DONE: Accumulate total loss for the epoch\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "        # DONE: Compute average loss for the epoch\n",
        "        mean_loss = total_loss / len(dataloader)\n",
        "\n",
        "        # Log average loss over the epoch\n",
        "        print('Epoch={}/{}  Loss: {:.3f}'.format(epoch + 1, n_epoch, mean_loss))\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XKF-hNYa7ea"
      },
      "source": [
        "Define evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "pa7O3vsuZ9S7"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, class_names, device):\n",
        "    '''\n",
        "    Evaluates the network on a dataset\n",
        "\n",
        "    Arg(s):\n",
        "        model : torch.nn.Module\n",
        "            neural network or logistic regression\n",
        "        dataloader : torch.utils.data.DataLoader\n",
        "            # https://pytorch.org/docs/stable/data.html\n",
        "            dataloader for training data\n",
        "        class_names : list[str]\n",
        "            list of class names to be used in plot\n",
        "        device : str\n",
        "            device to run on\n",
        "    '''\n",
        "\n",
        "    device = 'cuda' if device == 'gpu' or device == 'cuda' else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    # DONE: Move model to device using .to()\n",
        "    model = model.to(device)\n",
        "\n",
        "    n_correct = 0\n",
        "    n_sample = 0\n",
        "\n",
        "    # Make sure we do not backpropagate\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # DONE: Iterate through samples (images, labels) from dataloader\n",
        "        for images, labels in dataloader:\n",
        "\n",
        "            # DONE: Move images and labels to device using .to()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # DONE: Vectorize images from (N, H, W, C) to (N, d)\n",
        "            n_dim = images.shape[1] * images.shape[2] * images.shape[3]\n",
        "            images = images.view(-1, n_dim)\n",
        "\n",
        "            # DONE: Forward through the model\n",
        "            outputs = model(images)\n",
        "\n",
        "            # DONE?: Take the argmax over the outputs\n",
        "            outputs = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # DONE?: Accumulate number of samples\n",
        "            # maybe labels.size(0)\n",
        "            n_sample = n_sample + outputs.size(0)\n",
        "\n",
        "            # DONE?: Check if our prediction is correct\n",
        "            n_correct = n_correct + (outputs == labels).sum()\n",
        "\n",
        "    # DONE: Compute mean accuracy\n",
        "    mean_accuracy = (n_correct / n_sample) * 100\n",
        "\n",
        "    print('Mean accuracy over {} images: {:.3f}%'.format(n_sample, mean_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMVEF-Ajn691"
      },
      "source": [
        "Training a neural network and logistic regression for image classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "LxKRWZZQn5VY"
      },
      "outputs": [],
      "source": [
        "# Create transformations convert data to torch tensor\n",
        "# https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Set path to save checkpoint\n",
        "checkpoint_path = './checkpoint-{}.pth'.format(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "8byUq18XoAN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f8568f-84ee-4ca3-9748-7b0546b3c25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training logistic_regression model\n",
            "Epoch=1/50  Loss: 1.973\n",
            "Epoch=2/50  Loss: 1.880\n",
            "Epoch=3/50  Loss: 1.835\n",
            "Epoch=4/50  Loss: 1.824\n",
            "Epoch=5/50  Loss: 1.796\n",
            "Epoch=6/50  Loss: 1.794\n",
            "Epoch=7/50  Loss: 1.773\n",
            "Epoch=8/50  Loss: 1.768\n",
            "Epoch=9/50  Loss: 1.740\n",
            "Epoch=10/50  Loss: 1.740\n",
            "Epoch=11/50  Loss: 1.719\n",
            "Epoch=12/50  Loss: 1.729\n",
            "Epoch=13/50  Loss: 1.718\n",
            "Epoch=14/50  Loss: 1.710\n",
            "Epoch=15/50  Loss: 1.705\n",
            "Epoch=16/50  Loss: 1.702\n",
            "Epoch=17/50  Loss: 1.690\n",
            "Epoch=18/50  Loss: 1.694\n",
            "Epoch=19/50  Loss: 1.687\n",
            "Epoch=20/50  Loss: 1.685\n",
            "Epoch=21/50  Loss: 1.676\n",
            "Epoch=22/50  Loss: 1.672\n",
            "Epoch=23/50  Loss: 1.681\n",
            "Epoch=24/50  Loss: 1.669\n",
            "Epoch=25/50  Loss: 1.662\n",
            "Epoch=26/50  Loss: 1.660\n",
            "Epoch=27/50  Loss: 1.663\n",
            "Epoch=28/50  Loss: 1.658\n",
            "Epoch=29/50  Loss: 1.658\n",
            "Epoch=30/50  Loss: 1.655\n",
            "Epoch=31/50  Loss: 1.652\n",
            "Epoch=32/50  Loss: 1.652\n",
            "Epoch=33/50  Loss: 1.650\n",
            "Epoch=34/50  Loss: 1.646\n",
            "Epoch=35/50  Loss: 1.643\n",
            "Epoch=36/50  Loss: 1.643\n",
            "Epoch=37/50  Loss: 1.643\n",
            "Epoch=38/50  Loss: 1.641\n",
            "Epoch=39/50  Loss: 1.640\n",
            "Epoch=40/50  Loss: 1.639\n",
            "Epoch=41/50  Loss: 1.636\n",
            "Epoch=42/50  Loss: 1.637\n",
            "Epoch=43/50  Loss: 1.635\n",
            "Epoch=44/50  Loss: 1.635\n",
            "Epoch=45/50  Loss: 1.632\n",
            "Epoch=46/50  Loss: 1.632\n",
            "Epoch=47/50  Loss: 1.632\n",
            "Epoch=48/50  Loss: 1.631\n",
            "Epoch=49/50  Loss: 1.631\n",
            "Epoch=50/50  Loss: 1.630\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Set up dataloading\n",
        "'''\n",
        "# Download and setup CIFAR10 training set using preconfigured torchvision.datasets.CIFAR10\n",
        "cifar10_train = torchvision.datasets.CIFAR10(\n",
        "    root=os.path.join('data', 'exercise_11'),\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms)\n",
        "\n",
        "# DONE?: Setup a dataloader (iterator) to fetch from the training set using\n",
        "# torch.utils.data.DataLoader and set shuffle=True, drop_last=True, num_workers=2\n",
        "# Set your batch size to the hyperparameter N_BATCH\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    cifar10_train,\n",
        "    batch_size=N_BATCH,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2)\n",
        "\n",
        "# Define the possible classes in CIFAR10\n",
        "class_names = [\n",
        "    'plane',\n",
        "    'car',\n",
        "    'bird',\n",
        "    'cat',\n",
        "    'deer',\n",
        "    'dog',\n",
        "    'frog',\n",
        "    'horse',\n",
        "    'ship',\n",
        "    'truck'\n",
        "]\n",
        "\n",
        "# CIFAR10 has 10 classes\n",
        "n_class = len(class_names)\n",
        "\n",
        "'''\n",
        "Set up model and optimizer\n",
        "'''\n",
        "# DONE: Compute number of input features. Hint: They are RGB images of size 32 x 32\n",
        "n_input_feature = 3 * 32 * 32\n",
        "\n",
        "if MODEL_NAME == 'neural_network':\n",
        "    # DONE: Instantiate neural network\n",
        "    model = NeuralNetwork(n_input_feature, n_class)\n",
        "elif MODEL_NAME == 'logistic_regression':\n",
        "    # DONE: Instantiate logistic regression\n",
        "    model = LogisticRegression(n_input_feature, n_class)\n",
        "else:\n",
        "    raise('Unsupported model name: {}'.format(MODEL_NAME))\n",
        "\n",
        "print('Training {} model'.format(MODEL_NAME))\n",
        "\n",
        "# DONE?: Setup learning rate SGD optimizer and step function scheduler\n",
        "# https://pytorch.org/docs/stable/optim.html?#torch.optim.SGD\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4\n",
        "    )\n",
        "\n",
        "\n",
        "'''\n",
        "Train model and store weights\n",
        "'''\n",
        "# DONE: Set model to training mode\n",
        "model.train()\n",
        "\n",
        "# DONE: Train model with device='cuda'\n",
        "model = train(\n",
        "    model=model,\n",
        "    dataloader=dataloader_train,\n",
        "    n_epoch=N_EPOCH,\n",
        "    optimizer=optimizer,\n",
        "    learning_rate_decay=LEARNING_RATE_DECAY,\n",
        "    learning_rate_decay_period=LEARNING_RATE_DECAY_PERIOD,\n",
        "    device='cuda')\n",
        "\n",
        "# DONE: Save weights into checkpoint path\n",
        "torch.save(model.state_dict(), checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WubtddGkwiSo"
      },
      "source": [
        "Testing the trained neural network on image classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "KByIHECHA3m6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b03f8f1-5dde-4947-f7f2-70efd635ff1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating logistic_regression model from ./checkpoint-logistic_regression.pth\n",
            "Mean accuracy over 10000 images: 40.560%\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Set up dataloading\n",
        "'''\n",
        "# DONE: Download and setup CIFAR10 testing set using\n",
        "# preconfigured torchvision.datasets.CIFAR10\n",
        "cifar10_test = torchvision.datasets.CIFAR10(\n",
        "    root=os.path.join('data', 'exercise_11'),\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms)\n",
        "\n",
        "# DONE: Setup a dataloader (iterator) to fetch from the testing set using\n",
        "# torch.utils.data.DataLoader and set shuffle=False, drop_last=False, num_workers=2\n",
        "# Set batch_size to 25\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    cifar10_test,\n",
        "    batch_size=25,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=2)\n",
        "\n",
        "'''\n",
        "Set up model\n",
        "'''\n",
        "# DONE: Compute number of input features. Hint: They are RGB images of size 32 x 32\n",
        "n_input_feature = 3 * 32 * 32\n",
        "\n",
        "if MODEL_NAME == 'neural_network':\n",
        "    # DONE: Instantiate neural network\n",
        "    model = NeuralNetwork(n_input_feature, n_class)\n",
        "elif MODEL_NAME == 'logistic_regression':\n",
        "    # DONE: Instantiate logistic regression\n",
        "    model = LogisticRegression(n_input_feature, n_class)\n",
        "else:\n",
        "    raise('Unsupported model name: {}'.format(MODEL_NAME))\n",
        "\n",
        "print('Evaluating {} model from {}'.format(MODEL_NAME, checkpoint_path))\n",
        "\n",
        "'''\n",
        "Restore weights and evaluate model\n",
        "'''\n",
        "# DONE: Load model from checkpoint\n",
        "checkpoint = model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "\n",
        "# DONE: Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# DONE: Evaluate model on testing set with device='cuda'\n",
        "evaluate(\n",
        "    model=model,\n",
        "    dataloader=dataloader_test,\n",
        "    class_names=class_names,\n",
        "    device='cuda')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}