{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r37l-FPc9o0h"
      },
      "source": [
        "**Assignment 2: Stochastic Gradient Descent and Momentum**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "*Student: Hailey Robertson*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "\n",
        "4. Move the 02_assignment_stochastic_gradient_descent.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments/02_assignment_stochastic_gradient_descent.ipynb\n",
        "```\n",
        "\n",
        "In this assignment, we will optimize a linear function for the logistic regression task using the stochastic gradient descent and its momentum variant. We will test them on several binary classification datasets (breast cancer, digits larger or less than 5, and fir and pine coverage). We will implement a training and validation loop for the binary classification task and test it on the testing split for each dataset.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training, validation, and testing scores.\n",
        "\n",
        "```\n",
        "Report training, validation, and testing scores here.\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: N/A\n",
        "```\n",
        "\n",
        "\n",
        "**IMPORTANT**:\n",
        "\n",
        "- For full credit, your mean classification accuracies for all trained models across all datasets should be no more than 8% worse the scores achieved by sci-kit learn's logistic regression model across training, validation and testing splits.\n",
        "\n",
        "- You may not use batch sizes of more than 10% of the dataset size for stochastic gradient descent and momentum stochastic gradient descent.\n",
        "\n",
        "- You will only need to experiment with gradient descent (GD) and momentum gradient descent (momentum GD) on breast cancer and digits (toy) datasets. It will take too long to run them on fir and pine coverage (realistic) dataset to get reasonable numbers. Of course, you may try them on fir and pine coverage :) but they will not count towards your grade.\n",
        "\n",
        "- Note the run time speed up when comparing GD and momemtum GD with stochastic gradient descent (SGD) and momentum stochastic gradient descent (momentum SGD)! Even though they are faster and observing batches instead of the full dataset at each time step, they can still achieving similar accuracies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "koDraeo69YZH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn.linear_model import LogisticRegression as LogisticRegressionSciKit\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAR4bm26XZiJ"
      },
      "source": [
        "Implementation of stochastic gradient descent optimizer for logistic loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "bI62IQ3d9SpW"
      },
      "outputs": [],
      "source": [
        "class Optimizer(object):\n",
        "\n",
        "    def __init__(self, alpha, eta_decay_factor, beta, optimizer_type):\n",
        "        '''\n",
        "        Arg(s):\n",
        "            alpha : float\n",
        "                initial learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "        '''\n",
        "\n",
        "        self.__alpha = alpha\n",
        "        self.__eta_decay_factor = eta_decay_factor\n",
        "        self.__beta = beta\n",
        "        self.__optimizer_type = optimizer_type\n",
        "        self.__momentum = None\n",
        "\n",
        "    def __compute_gradients(self, w, x, y, loss_func='logistic'):\n",
        "        '''\n",
        "        Returns the gradient of a loss function\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            numpy[float32] : d x 1 gradients\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement compute_gradient function\n",
        "        # COME BACK\n",
        "\n",
        "        if loss_func == 'logistic':\n",
        "\n",
        "            N = x.shape[1] # number of samples, 341\n",
        "            z = np.dot(w.T, x)\n",
        "            gradient = -np.sum((y * x) / (1 + np.exp(y * z)))/N\n",
        "\n",
        "            return gradient\n",
        "        \n",
        "        else:\n",
        "            raise ValueError('Unupported loss function: {}'.format(loss_func))\n",
        "\n",
        "    def __polynomial_decay(self, time_step):\n",
        "        '''\n",
        "        Computes the polynomial decay factor t^{-a}\n",
        "\n",
        "        Arg(s):\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            float : polynomial decay to adjust (reduce) initial learning rate\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement polynomial decay to adjust the initial learning rate\n",
        "        if self.__eta_decay_factor is None:\n",
        "            return self.__alpha\n",
        "        else:\n",
        "            decay_factor = time_step ** (-self.__eta_decay_factor)\n",
        "            decayed_alpha = self.__alpha * decay_factor\n",
        "\n",
        "        return decayed_alpha\n",
        "\n",
        "    def update(self,\n",
        "               w,\n",
        "               x,\n",
        "               y,\n",
        "               loss_func,\n",
        "               batch_size,\n",
        "               time_step):\n",
        "        '''\n",
        "        Updates the weight vector based on\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, should be 'logistic' for the purpose of the assignment\n",
        "            batch_size : int\n",
        "                batch size for stochastic and momentum stochastic gradient descent\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            numpy[float32]: d x 1 weights\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement the optimizer update function\n",
        "        # For each optimizer type, compute gradients and update weights\n",
        "        eta = self.__polynomial_decay(time_step) # step size\n",
        "        gradient = self.__compute_gradients(w, x, y, loss_func)\n",
        "\n",
        "        if self.__optimizer_type == 'gradient_descent':\n",
        "            w = w - eta * gradient\n",
        "            return w\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_gradient_descent':\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros(w.shape)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1-self.__beta) * gradient\n",
        "            w = w - eta * self.__momentum\n",
        "            return w\n",
        "\n",
        "        elif self.__optimizer_type == 'stochastic_gradient_descent':\n",
        "            indices = np.random.choice(x.shape[1], batch_size, replace=False)\n",
        "            x_batch = x[:, indices]\n",
        "            y_batch = y[:, indices]\n",
        "            gradient = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            w = w - eta * gradient\n",
        "            return w\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_stochastic_gradient_descent':\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros(w.shape)\n",
        "            indices = np.random.choice(x.shape[1], batch_size, replace=False)\n",
        "            x_batch = x[:, indices]\n",
        "            y_batch = y[:, indices]\n",
        "            gradient = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1-self.__beta) * gradient\n",
        "            w = w - eta * self.__momentum\n",
        "            return w\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported optimizer type: {}'.format(self.__optimizer_type))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRT2kC4GqAp_"
      },
      "source": [
        "Implementation of our logistic regression model for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "vOaTyJ5VqBYt"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Define private variables\n",
        "        self.__weights = None\n",
        "        self.__optimizer = None\n",
        "\n",
        "    def fit(self,\n",
        "            x,\n",
        "            y,\n",
        "            T,\n",
        "            alpha,\n",
        "            eta_decay_factor,\n",
        "            beta,\n",
        "            batch_size,\n",
        "            optimizer_type,\n",
        "            loss_func='logistic'):\n",
        "        '''\n",
        "        Fits the model to x and y by updating the weight vector\n",
        "        using gradient descent\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            T : int\n",
        "                number of iterations to train\n",
        "            alpha : float\n",
        "                learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            batch_size : int\n",
        "                number of examples per batch\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        '''\n",
        "\n",
        "        # DONE: Instantiate optimizer and weights\n",
        "        self.__optimizer = Optimizer(alpha, eta_decay_factor, beta, optimizer_type)\n",
        "        self.__weights = np.random.randn(x.shape[0], 1) * 0.01 \n",
        "\n",
        "        for t in range(1, T + 1):\n",
        "\n",
        "            # DONE: Compute loss function\n",
        "            loss = self.__compute_loss(x, y, loss_func)\n",
        "\n",
        "\n",
        "            if (t % 100) == 0:\n",
        "                print('Step={}  Loss={}'.format(t, loss))\n",
        "\n",
        "            # DONE: Update weights\n",
        "            self.__weights = self.__optimizer.update(self.__weights, x, y, loss_func, batch_size, t)\n",
        "            # print(\"loss\", loss, \"weights\", self.__weights)\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predicts the label for each feature vector x\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : 1 x N vector\n",
        "        '''\n",
        "\n",
        "        # DONE: Implements the predict function\n",
        "        # Hint: logistic regression predicts a value between 0 and 1\n",
        "        z = np.dot(self.__weights.T, x)\n",
        "        sigmoid = 1 / (1+ np.exp(-z))\n",
        "        predictions = np.where(sigmoid >= 0.5, 1, -1)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def __compute_loss(self, x, y, loss_func):\n",
        "        '''\n",
        "        Computes the logistic loss\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            float : loss\n",
        "        '''\n",
        "\n",
        "        # DONE: Implements the __compute_loss function\n",
        "        if loss_func == 'logistic':\n",
        "            \n",
        "            N = x.shape[1] # number of samples, 341\n",
        "            z = np.dot(self.__weights.T, x)\n",
        "            loss = np.sum(np.log(1 + np.exp(-y * z))) / N\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported loss function: {}'.format(loss_func))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zusvzb2xJJzi"
      },
      "source": [
        "Training, validating and testing logistic regression for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI7TMva6JIh8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing the breast cancer dataset (569 samples, 30 feature dimensions)\n",
            "***** Results on the breast cancer dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9648\n",
            "Validation set mean accuracy: 0.9474\n",
            "Testing set mean accuracy: 0.9649\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=1e-10 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=100000\n",
            "Step=100  Loss=4.642339621545029\n",
            "Step=200  Loss=4.635500043021177\n",
            "Step=300  Loss=4.628660733179506\n",
            "Step=400  Loss=4.621821694077731\n",
            "Step=500  Loss=4.614982927790081\n",
            "Step=600  Loss=4.608144436407441\n",
            "Step=700  Loss=4.60130622203749\n",
            "Step=800  Loss=4.59446828680484\n",
            "Step=900  Loss=4.5876306328511784\n",
            "Step=1000  Loss=4.580793262335415\n",
            "Step=1100  Loss=4.573956177433814\n",
            "Step=1200  Loss=4.567119380340159\n",
            "Step=1300  Loss=4.560282873265875\n",
            "Step=1400  Loss=4.553446658440194\n",
            "Step=1500  Loss=4.546610738110299\n",
            "Step=1600  Loss=4.53977511454148\n",
            "Step=1700  Loss=4.532939790017262\n",
            "Step=1800  Loss=4.526104766839588\n",
            "Step=1900  Loss=4.5192700473289555\n",
            "Step=2000  Loss=4.512435633824579\n",
            "Step=2100  Loss=4.505601528684535\n",
            "Step=2200  Loss=4.498767734285944\n",
            "Step=2300  Loss=4.4919342530250965\n",
            "Step=2400  Loss=4.4851010873176405\n",
            "Step=2500  Loss=4.478268239598741\n",
            "Step=2600  Loss=4.471435712323229\n",
            "Step=2700  Loss=4.464603507965779\n",
            "Step=2800  Loss=4.457771629021069\n",
            "Step=2900  Loss=4.450940078003954\n",
            "Step=3000  Loss=4.444108857449627\n",
            "Step=3100  Loss=4.437277969913793\n",
            "Step=3200  Loss=4.430447417972853\n",
            "Step=3300  Loss=4.423617204224048\n",
            "Step=3400  Loss=4.416787331285673\n",
            "Step=3500  Loss=4.4099578017972245\n",
            "Step=3600  Loss=4.403128618419582\n",
            "Step=3700  Loss=4.396299783835203\n",
            "Step=3800  Loss=4.389471300748286\n",
            "Step=3900  Loss=4.382643171884963\n",
            "Step=4000  Loss=4.3758153999934875\n",
            "Step=4100  Loss=4.368987987844405\n",
            "Step=4200  Loss=4.362160938230759\n",
            "Step=4300  Loss=4.355334253968269\n",
            "Step=4400  Loss=4.34850793789552\n",
            "Step=4500  Loss=4.341681992874156\n",
            "Step=4600  Loss=4.334856421789079\n",
            "Step=4700  Loss=4.328031227548639\n",
            "Step=4800  Loss=4.321206413084836\n",
            "Step=4900  Loss=4.314381981353512\n",
            "Step=5000  Loss=4.307557935334557\n",
            "Step=5100  Loss=4.30073427803211\n",
            "Step=5200  Loss=4.293911012474763\n",
            "Step=5300  Loss=4.287088141715762\n",
            "Step=5400  Loss=4.280265668833225\n",
            "Step=5500  Loss=4.273443596930341\n",
            "Step=5600  Loss=4.266621929135589\n",
            "Step=5700  Loss=4.259800668602936\n",
            "Step=5800  Loss=4.252979818512073\n",
            "Step=5900  Loss=4.246159382068619\n",
            "Step=6000  Loss=4.239339362504337\n",
            "Step=6100  Loss=4.232519763077355\n",
            "Step=6200  Loss=4.2257005870723985\n",
            "Step=6300  Loss=4.218881837801006\n",
            "Step=6400  Loss=4.212063518601748\n",
            "Step=6500  Loss=4.2052456328404695\n",
            "Step=6600  Loss=4.198428183910511\n",
            "Step=6700  Loss=4.191611175232936\n",
            "Step=6800  Loss=4.1847946102567715\n",
            "Step=6900  Loss=4.177978492459248\n",
            "Step=7000  Loss=4.1711628253460304\n",
            "Step=7100  Loss=4.164347612451449\n",
            "Step=7200  Loss=4.157532857338758\n",
            "Step=7300  Loss=4.150718563600366\n",
            "Step=7400  Loss=4.143904734858082\n",
            "Step=7500  Loss=4.137091374763383\n",
            "Step=7600  Loss=4.1302784869976366\n",
            "Step=7700  Loss=4.123466075272368\n",
            "Step=7800  Loss=4.116654143329507\n",
            "Step=7900  Loss=4.109842694941653\n",
            "Step=8000  Loss=4.103031733912328\n",
            "Step=8100  Loss=4.0962212640762345\n",
            "Step=8200  Loss=4.0894112892995285\n",
            "Step=8300  Loss=4.082601813480066\n",
            "Step=8400  Loss=4.075792840547688\n",
            "Step=8500  Loss=4.068984374464485\n",
            "Step=8600  Loss=4.062176419225058\n",
            "Step=8700  Loss=4.055368978856811\n",
            "Step=8800  Loss=4.04856205742022\n",
            "Step=8900  Loss=4.041755659009099\n",
            "Step=9000  Loss=4.034949787750895\n",
            "Step=9100  Loss=4.028144447806968\n",
            "Step=9200  Loss=4.0213396433728805\n",
            "Step=9300  Loss=4.014535378678672\n",
            "Step=9400  Loss=4.007731657989172\n",
            "Step=9500  Loss=4.00092848560427\n",
            "Step=9600  Loss=3.99412586585922\n",
            "Step=9700  Loss=3.9873238031249434\n",
            "Step=9800  Loss=3.980522301808322\n",
            "Step=9900  Loss=3.9737213663525086\n",
            "Step=10000  Loss=3.9669210012372234\n",
            "Step=10100  Loss=3.960121210979068\n",
            "Step=10200  Loss=3.953322000131829\n",
            "Step=10300  Loss=3.9465233732868064\n",
            "Step=10400  Loss=3.9397253350731054\n",
            "Step=10500  Loss=3.9329278901579854\n",
            "Step=10600  Loss=3.9261310432471483\n",
            "Step=10700  Loss=3.91933479908508\n",
            "Step=10800  Loss=3.9125391624553854\n",
            "Step=10900  Loss=3.905744138181101\n",
            "Step=11000  Loss=3.89894973112503\n",
            "Step=11100  Loss=3.892155946190079\n",
            "Step=11200  Loss=3.8853627883195965\n",
            "Step=11300  Loss=3.878570262497715\n",
            "Step=11400  Loss=3.8717783737496876\n",
            "Step=11500  Loss=3.8649871271422387\n",
            "Step=11600  Loss=3.858196527783907\n",
            "Step=11700  Loss=3.8514065808254063\n",
            "Step=11800  Loss=3.844617291459971\n",
            "Step=11900  Loss=3.8378286649237245\n",
            "Step=12000  Loss=3.8310407064960184\n",
            "Step=12100  Loss=3.824253421499819\n",
            "Step=12200  Loss=3.8174668153020637\n",
            "Step=12300  Loss=3.8106808933140286\n",
            "Step=12400  Loss=3.8038956609916994\n",
            "Step=12500  Loss=3.797111123836155\n",
            "Step=12600  Loss=3.790327287393937\n",
            "Step=12700  Loss=3.7835441572574355\n",
            "Step=12800  Loss=3.776761739065274\n",
            "Step=12900  Loss=3.7699800385026903\n",
            "Step=13000  Loss=3.7631990613019477\n",
            "Step=13100  Loss=3.7564188132427\n",
            "Step=13200  Loss=3.7496393001524013\n",
            "Step=13300  Loss=3.7428605279067275\n",
            "Step=13400  Loss=3.7360825024299436\n",
            "Step=13500  Loss=3.7293052296953344\n",
            "Step=13600  Loss=3.7225287157256046\n",
            "Step=13700  Loss=3.715752966593305\n",
            "Step=13800  Loss=3.7089779884212373\n",
            "Step=13900  Loss=3.7022037873828713\n",
            "Step=14000  Loss=3.695430369702788\n",
            "Step=14100  Loss=3.6886577416570914\n",
            "Step=14200  Loss=3.6818859095738308\n",
            "Step=14300  Loss=3.6751148798334605\n",
            "Step=14400  Loss=3.6683446588692536\n",
            "Step=14500  Loss=3.661575253167762\n",
            "Step=14600  Loss=3.654806669269244\n",
            "Step=14700  Loss=3.6480389137681257\n",
            "Step=14800  Loss=3.641271993313446\n",
            "Step=14900  Loss=3.6345059146093086\n",
            "Step=15000  Loss=3.6277406844153486\n",
            "Step=15100  Loss=3.6209763095471996\n",
            "Step=15200  Loss=3.61421279687694\n",
            "Step=15300  Loss=3.6074501533335877\n",
            "Step=15400  Loss=3.600688385903559\n",
            "Step=15500  Loss=3.5939275016311467\n",
            "Step=15600  Loss=3.5871675076190064\n",
            "Step=15700  Loss=3.5804084110286367\n",
            "Step=15800  Loss=3.573650219080872\n",
            "Step=15900  Loss=3.56689293905638\n",
            "Step=16000  Loss=3.5601365782961407\n",
            "Step=16100  Loss=3.5533811442019747\n",
            "Step=16200  Loss=3.5466266442370165\n",
            "Step=16300  Loss=3.5398730859262386\n",
            "Step=16400  Loss=3.5331204768569786\n",
            "Step=16500  Loss=3.526368824679427\n",
            "Step=16600  Loss=3.519618137107169\n",
            "Step=16700  Loss=3.5128684219176933\n",
            "Step=16800  Loss=3.5061196869529336\n",
            "Step=16900  Loss=3.499371940119796\n",
            "Step=17000  Loss=3.4926251893906954\n",
            "Step=17100  Loss=3.4858794428040953\n",
            "Step=17200  Loss=3.479134708465059\n",
            "Step=17300  Loss=3.4723909945457923\n",
            "Step=17400  Loss=3.4656483092861925\n",
            "Step=17500  Loss=3.458906660994419\n",
            "Step=17600  Loss=3.452166058047449\n",
            "Step=17700  Loss=3.44542650889164\n",
            "Step=17800  Loss=3.4386880220433103\n",
            "Step=17900  Loss=3.431950606089299\n",
            "Step=18000  Loss=3.4252142696875603\n",
            "Step=18100  Loss=3.41847902156774\n",
            "Step=18200  Loss=3.4117448705317623\n",
            "Step=18300  Loss=3.4050118254544235\n",
            "Step=18400  Loss=3.398279895283992\n",
            "Step=18500  Loss=3.3915490890428046\n",
            "Step=18600  Loss=3.3848194158278804\n",
            "Step=18700  Loss=3.378090884811514\n",
            "Step=18800  Loss=3.3713635052419173\n",
            "Step=18900  Loss=3.364637286443815\n",
            "Step=19000  Loss=3.35791223781908\n",
            "Step=19100  Loss=3.351188368847363\n",
            "Step=19200  Loss=3.344465689086715\n",
            "Step=19300  Loss=3.337744208174244\n",
            "Step=19400  Loss=3.331023935826735\n",
            "Step=19500  Loss=3.3243048818413152\n",
            "Step=19600  Loss=3.3175870560961096\n",
            "Step=19700  Loss=3.3108704685508688\n",
            "Step=19800  Loss=3.304155129247667\n",
            "Step=19900  Loss=3.2974410483115504\n",
            "Step=20000  Loss=3.2907282359511965\n",
            "Step=20100  Loss=3.284016702459623\n",
            "Step=20200  Loss=3.2773064582148352\n",
            "Step=20300  Loss=3.270597513680533\n",
            "Step=20400  Loss=3.263889879406788\n",
            "Step=20500  Loss=3.257183566030757\n",
            "Step=20600  Loss=3.250478584277359\n",
            "Step=20700  Loss=3.243774944960002\n",
            "Step=20800  Loss=3.2370726589812837\n",
            "Step=20900  Loss=3.230371737333698\n",
            "Step=21000  Loss=3.2236721911003836\n",
            "Step=21100  Loss=3.216974031455813\n",
            "Step=21200  Loss=3.2102772696665576\n",
            "Step=21300  Loss=3.2035819170919915\n",
            "Step=21400  Loss=3.1968879851850573\n",
            "Step=21500  Loss=3.190195485492999\n",
            "Step=21600  Loss=3.1835044296581185\n",
            "Step=21700  Loss=3.1768148294185297\n",
            "Step=21800  Loss=3.170126696608924\n",
            "Step=21900  Loss=3.1634400431613128\n",
            "Step=22000  Loss=3.1567548811058392\n",
            "Step=22100  Loss=3.1500712225715226\n",
            "Step=22200  Loss=3.1433890797870583\n",
            "Step=22300  Loss=3.1367084650815937\n",
            "Step=22400  Loss=3.130029390885523\n",
            "Step=22500  Loss=3.123351869731287\n",
            "Step=22600  Loss=3.116675914254183\n",
            "Step=22700  Loss=3.110001537193162\n",
            "Step=22800  Loss=3.103328751391633\n",
            "Step=22900  Loss=3.0966575697983116\n",
            "Step=23000  Loss=3.089988005468018\n",
            "Step=23100  Loss=3.083320071562514\n",
            "Step=23200  Loss=3.076653781351339\n",
            "Step=23300  Loss=3.0699891482126467\n",
            "Step=23400  Loss=3.06332618563406\n",
            "Step=23500  Loss=3.056664907213502\n",
            "Step=23600  Loss=3.050005326660077\n",
            "Step=23700  Loss=3.0433474577949093\n",
            "Step=23800  Loss=3.0366913145520216\n",
            "Step=23900  Loss=3.0300369109792022\n",
            "Step=24000  Loss=3.023384261238882\n",
            "Step=24100  Loss=3.016733379609027\n",
            "Step=24200  Loss=3.0100842804840147\n",
            "Step=24300  Loss=3.003436978375522\n",
            "Step=24400  Loss=2.9967914879134456\n",
            "Step=24500  Loss=2.990147823846784\n",
            "Step=24600  Loss=2.9835060010445593\n",
            "Step=24700  Loss=2.976866034496732\n",
            "Step=24800  Loss=2.970227939315109\n",
            "Step=24900  Loss=2.9635917307342887\n",
            "Step=25000  Loss=2.9569574241125696\n",
            "Step=25100  Loss=2.9503250349329004\n",
            "Step=25200  Loss=2.9436945788038202\n",
            "Step=25300  Loss=2.9370660714603987\n",
            "Step=25400  Loss=2.930439528765194\n",
            "Step=25500  Loss=2.923814966709213\n",
            "Step=25600  Loss=2.9171924014128567\n",
            "Step=25700  Loss=2.9105718491269106\n",
            "Step=25800  Loss=2.90395332623351\n",
            "Step=25900  Loss=2.8973368492471083\n",
            "Step=26000  Loss=2.890722434815465\n",
            "Step=26100  Loss=2.8841100997206435\n",
            "Step=26200  Loss=2.877499860879999\n",
            "Step=26300  Loss=2.870891735347168\n",
            "Step=26400  Loss=2.8642857403130932\n",
            "Step=26500  Loss=2.857681893107013\n",
            "Step=26600  Loss=2.8510802111974867\n",
            "Step=26700  Loss=2.844480712193405\n",
            "Step=26800  Loss=2.8378834138450304\n",
            "Step=26900  Loss=2.8312883340450123\n",
            "Step=27000  Loss=2.8246954908294173\n",
            "Step=27100  Loss=2.818104902378791\n",
            "Step=27200  Loss=2.8115165870191787\n",
            "Step=27300  Loss=2.804930563223187\n",
            "Step=27400  Loss=2.7983468496110353\n",
            "Step=27500  Loss=2.791765464951615\n",
            "Step=27600  Loss=2.7851864281635463\n",
            "Step=27700  Loss=2.7786097583162617\n",
            "Step=27800  Loss=2.772035474631061\n",
            "Step=27900  Loss=2.765463596482202\n",
            "Step=28000  Loss=2.758894143397981\n",
            "Step=28100  Loss=2.7523271350618104\n",
            "Step=28200  Loss=2.745762591313323\n",
            "Step=28300  Loss=2.7392005321494532\n",
            "Step=28400  Loss=2.7326409777255543\n",
            "Step=28500  Loss=2.7260839483564876\n",
            "Step=28600  Loss=2.719529464517737\n",
            "Step=28700  Loss=2.7129775468465227\n",
            "Step=28800  Loss=2.7064282161429185\n",
            "Step=28900  Loss=2.6998814933709734\n",
            "Step=29000  Loss=2.6933373996598324\n",
            "Step=29100  Loss=2.6867959563048704\n",
            "Step=29200  Loss=2.6802571847688212\n",
            "Step=29300  Loss=2.673721106682919\n",
            "Step=29400  Loss=2.667187743848031\n",
            "Step=29500  Loss=2.660657118235814\n",
            "Step=29600  Loss=2.6541292519898523\n",
            "Step=29700  Loss=2.6476041674268083\n",
            "Step=29800  Loss=2.641081887037586\n",
            "Step=29900  Loss=2.6345624334884783\n",
            "Step=30000  Loss=2.6280458296223372\n",
            "Step=30100  Loss=2.6215320984597335\n",
            "Step=30200  Loss=2.6150212632001235\n",
            "Step=30300  Loss=2.608513347223022\n",
            "Step=30400  Loss=2.6020083740891757\n",
            "Step=30500  Loss=2.5955063675417356\n",
            "Step=30600  Loss=2.5890073515074405\n",
            "Step=30700  Loss=2.5825113500977954\n",
            "Step=30800  Loss=2.5760183876102523\n",
            "Step=30900  Loss=2.5695284885294\n",
            "Step=31000  Loss=2.563041677528155\n",
            "Step=31100  Loss=2.556557979468938\n",
            "Step=31200  Loss=2.550077419404887\n",
            "Step=31300  Loss=2.543600022581034\n",
            "Step=31400  Loss=2.5371258144355036\n",
            "Step=31500  Loss=2.5306548206007187\n",
            "Step=31600  Loss=2.5241870669045907\n",
            "Step=31700  Loss=2.51772257937172\n",
            "Step=31800  Loss=2.511261384224601\n",
            "Step=31900  Loss=2.5048035078848194\n",
            "Step=32000  Loss=2.49834897697426\n",
            "Step=32100  Loss=2.491897818316304\n",
            "Step=32200  Loss=2.48545005893704\n",
            "Step=32300  Loss=2.47900572606646\n",
            "Step=32400  Loss=2.4725648471396733\n",
            "Step=32500  Loss=2.466127449798103\n",
            "Step=32600  Loss=2.4596935618906968\n",
            "Step=32700  Loss=2.4532632114751274\n",
            "Step=32800  Loss=2.4468364268189955\n",
            "Step=32900  Loss=2.440413236401033\n",
            "Step=33000  Loss=2.4339936689123083\n",
            "Step=33100  Loss=2.4275777532574287\n",
            "Step=33200  Loss=2.421165518555731\n",
            "Step=33300  Loss=2.414756994142492\n",
            "Step=33400  Loss=2.4083522095701153\n",
            "Step=33500  Loss=2.4019511946093304\n",
            "Step=33600  Loss=2.3955539792503866\n",
            "Step=33700  Loss=2.389160593704241\n",
            "Step=33800  Loss=2.382771068403748\n",
            "Step=33900  Loss=2.3763854340048476\n",
            "Step=34000  Loss=2.3700037213877416\n",
            "Step=34100  Loss=2.363625961658076\n",
            "Step=34200  Loss=2.357252186148121\n",
            "Step=34300  Loss=2.3508824264179338\n",
            "Step=34400  Loss=2.344516714256535\n",
            "Step=34500  Loss=2.3381550816830687\n",
            "Step=34600  Loss=2.3317975609479653\n",
            "Step=34700  Loss=2.3254441845340907\n",
            "Step=34800  Loss=2.3190949851579044\n",
            "Step=34900  Loss=2.312749995770599\n",
            "Step=35000  Loss=2.30640924955924\n",
            "Step=35100  Loss=2.3000727799478997\n",
            "Step=35200  Loss=2.2937406205987876\n",
            "Step=35300  Loss=2.2874128054133673\n",
            "Step=35400  Loss=2.2810893685334745\n",
            "Step=35500  Loss=2.2747703443424214\n",
            "Step=35600  Loss=2.2684557674661\n",
            "Step=35700  Loss=2.2621456727740727\n",
            "Step=35800  Loss=2.2558400953806625\n",
            "Step=35900  Loss=2.2495390706460237\n",
            "Step=36000  Loss=2.24324263417721\n",
            "Step=36100  Loss=2.2369508218292387\n",
            "Step=36200  Loss=2.230663669706137\n",
            "Step=36300  Loss=2.2243812141619874\n",
            "Step=36400  Loss=2.2181034918019544\n",
            "Step=36500  Loss=2.211830539483301\n",
            "Step=36600  Loss=2.2055623943164115\n",
            "Step=36700  Loss=2.1992990936657786\n",
            "Step=36800  Loss=2.193040675150994\n",
            "Step=36900  Loss=2.1867871766477314\n",
            "Step=37000  Loss=2.180538636288703\n",
            "Step=37100  Loss=2.1742950924646083\n",
            "Step=37200  Loss=2.1680565838250834\n",
            "Step=37300  Loss=2.161823149279618\n",
            "Step=37400  Loss=2.1555948279984687\n",
            "Step=37500  Loss=2.1493716594135623\n",
            "Step=37600  Loss=2.1431536832193676\n",
            "Step=37700  Loss=2.136940939373775\n",
            "Step=37800  Loss=2.130733468098944\n",
            "Step=37900  Loss=2.124531309882136\n",
            "Step=38000  Loss=2.1183345054765508\n",
            "Step=38100  Loss=2.112143095902107\n",
            "Step=38200  Loss=2.10595712244625\n",
            "Step=38300  Loss=2.0997766266647115\n",
            "Step=38400  Loss=2.093601650382265\n",
            "Step=38500  Loss=2.087432235693455\n",
            "Step=38600  Loss=2.0812684249633153\n",
            "Step=38700  Loss=2.0751102608280587\n",
            "Step=38800  Loss=2.068957786195756\n",
            "Step=38900  Loss=2.0628110442469865\n",
            "Step=39000  Loss=2.056670078435474\n",
            "Step=39100  Loss=2.050534932488697\n",
            "Step=39200  Loss=2.0444056504084793\n",
            "Step=39300  Loss=2.038282276471555\n",
            "Step=39400  Loss=2.0321648552301164\n",
            "Step=39500  Loss=2.026053431512335\n",
            "Step=39600  Loss=2.01994805042285\n",
            "Step=39700  Loss=2.0138487573432533\n",
            "Step=39800  Loss=2.0077555979325257\n",
            "Step=39900  Loss=2.0016686181274634\n",
            "Step=40000  Loss=1.9955878641430702\n",
            "Step=40100  Loss=1.989513382472927\n",
            "Step=40200  Loss=1.9834452198895316\n",
            "Step=40300  Loss=1.977383423444614\n",
            "Step=40400  Loss=1.9713280404694167\n",
            "Step=40500  Loss=1.9652791185749532\n",
            "Step=40600  Loss=1.959236705652229\n",
            "Step=40700  Loss=1.9532008498724436\n",
            "Step=40800  Loss=1.9471715996871422\n",
            "Step=40900  Loss=1.9411490038283636\n",
            "Step=41000  Loss=1.9351331113087265\n",
            "Step=41100  Loss=1.9291239714215056\n",
            "Step=41200  Loss=1.9231216337406651\n",
            "Step=41300  Loss=1.917126148120852\n",
            "Step=41400  Loss=1.9111375646973676\n",
            "Step=41500  Loss=1.9051559338860966\n",
            "Step=41600  Loss=1.8991813063833978\n",
            "Step=41700  Loss=1.8932137331659658\n",
            "Step=41800  Loss=1.8872532654906466\n",
            "Step=41900  Loss=1.8812999548942253\n",
            "Step=42000  Loss=1.875353853193167\n",
            "Step=42100  Loss=1.8694150124833158\n",
            "Step=42200  Loss=1.8634834851395663\n",
            "Step=42300  Loss=1.8575593238154895\n",
            "Step=42400  Loss=1.8516425814429056\n",
            "Step=42500  Loss=1.845733311231431\n",
            "Step=42600  Loss=1.8398315666679743\n",
            "Step=42700  Loss=1.8339374015161949\n",
            "Step=42800  Loss=1.828050869815906\n",
            "Step=42900  Loss=1.8221720258824485\n",
            "Step=43000  Loss=1.8163009243060033\n",
            "Step=43100  Loss=1.810437619950871\n",
            "Step=43200  Loss=1.8045821679546972\n",
            "Step=43300  Loss=1.7987346237276534\n",
            "Step=43400  Loss=1.7928950429515669\n",
            "Step=43500  Loss=1.787063481579003\n",
            "Step=43600  Loss=1.7812399958323037\n",
            "Step=43700  Loss=1.775424642202557\n",
            "Step=43800  Loss=1.769617477448544\n",
            "Step=43900  Loss=1.763818558595604\n",
            "Step=44000  Loss=1.758027942934471\n",
            "Step=44100  Loss=1.7522456880200443\n",
            "Step=44200  Loss=1.746471851670106\n",
            "Step=44300  Loss=1.7407064919639876\n",
            "Step=44400  Loss=1.7349496672411806\n",
            "Step=44500  Loss=1.729201436099879\n",
            "Step=44600  Loss=1.7234618573954956\n",
            "Step=44700  Loss=1.7177309902390845\n",
            "Step=44800  Loss=1.7120088939957223\n",
            "Step=44900  Loss=1.7062956282828332\n",
            "Step=45000  Loss=1.7005912529684506\n",
            "Step=45100  Loss=1.6948958281694142\n",
            "Step=45200  Loss=1.6892094142495122\n",
            "Step=45300  Loss=1.6835320718175582\n",
            "Step=45400  Loss=1.6778638617253996\n",
            "Step=45500  Loss=1.6722048450658789\n",
            "Step=45600  Loss=1.6665550831707148\n",
            "Step=45700  Loss=1.6609146376083268\n",
            "Step=45800  Loss=1.6552835701815873\n",
            "Step=45900  Loss=1.6496619429255177\n",
            "Step=46000  Loss=1.64404981810491\n",
            "Step=46100  Loss=1.6384472582118823\n",
            "Step=46200  Loss=1.632854325963365\n",
            "Step=46300  Loss=1.6272710842985225\n",
            "Step=46400  Loss=1.6216975963760987\n",
            "Step=46500  Loss=1.6161339255717022\n",
            "Step=46600  Loss=1.6105801354750047\n",
            "Step=46700  Loss=1.6050362898868862\n",
            "Step=46800  Loss=1.5995024528164918\n",
            "Step=46900  Loss=1.5939786884782288\n",
            "Step=47000  Loss=1.5884650612886781\n",
            "Step=47100  Loss=1.5829616358634435\n",
            "Step=47200  Loss=1.5774684770139127\n",
            "Step=47300  Loss=1.5719856497439564\n",
            "Step=47400  Loss=1.5665132192465459\n",
            "Step=47500  Loss=1.5610512509002892\n",
            "Step=47600  Loss=1.5555998102658959\n",
            "Step=47700  Loss=1.550158963082567\n",
            "Step=47800  Loss=1.5447287752642958\n",
            "Step=47900  Loss=1.539309312896099\n",
            "Step=48000  Loss=1.5339006422301686\n",
            "Step=48100  Loss=1.5285028296819376\n",
            "Step=48200  Loss=1.5231159418260705\n",
            "Step=48300  Loss=1.5177400453923746\n",
            "Step=48400  Loss=1.5123752072616208\n",
            "Step=48500  Loss=1.5070214944612916\n",
            "Step=48600  Loss=1.5016789741612455\n",
            "Step=48700  Loss=1.4963477136692953\n",
            "Step=48800  Loss=1.4910277804267087\n",
            "Step=48900  Loss=1.485719242003615\n",
            "Step=49000  Loss=1.4804221660943444\n",
            "Step=49100  Loss=1.4751366205126626\n",
            "Step=49200  Loss=1.4698626731869422\n",
            "Step=49300  Loss=1.4646003921552302\n",
            "Step=49400  Loss=1.4593498455602485\n",
            "Step=49500  Loss=1.4541111016442907\n",
            "Step=49600  Loss=1.4488842287440455\n",
            "Step=49700  Loss=1.4436692952853318\n",
            "Step=49800  Loss=1.4384663697777431\n",
            "Step=49900  Loss=1.433275520809217\n",
            "Step=50000  Loss=1.428096817040492\n",
            "Step=50100  Loss=1.422930327199515\n",
            "Step=50200  Loss=1.4177761200757295\n",
            "Step=50300  Loss=1.412634264514295\n",
            "Step=50400  Loss=1.4075048294102122\n",
            "Step=50500  Loss=1.402387883702363\n",
            "Step=50600  Loss=1.39728349636746\n",
            "Step=50700  Loss=1.3921917364139162\n",
            "Step=50800  Loss=1.3871126728756211\n",
            "Step=50900  Loss=1.3820463748056284\n",
            "Step=51000  Loss=1.3769929112697672\n",
            "Step=51100  Loss=1.371952351340149\n",
            "Step=51200  Loss=1.366924764088606\n",
            "Step=51300  Loss=1.3619102185800278\n",
            "Step=51400  Loss=1.356908783865622\n",
            "Step=51500  Loss=1.3519205289760807\n",
            "Step=51600  Loss=1.34694552291467\n",
            "Step=51700  Loss=1.3419838346502224\n",
            "Step=51800  Loss=1.3370355331100512\n",
            "Step=51900  Loss=1.3321006871727816\n",
            "Step=52000  Loss=1.3271793656610902\n",
            "Step=52100  Loss=1.3222716373343617\n",
            "Step=52200  Loss=1.317377570881267\n",
            "Step=52300  Loss=1.3124972349122512\n",
            "Step=52400  Loss=1.3076306979519476\n",
            "Step=52500  Loss=1.3027780284314925\n",
            "Step=52600  Loss=1.2979392946807773\n",
            "Step=52700  Loss=1.293114564920609\n",
            "Step=52800  Loss=1.2883039072547888\n",
            "Step=52900  Loss=1.283507389662118\n",
            "Step=53000  Loss=1.2787250799883156\n",
            "Step=53100  Loss=1.2739570459378704\n",
            "Step=53200  Loss=1.269203355065801\n",
            "Step=53300  Loss=1.264464074769345\n",
            "Step=53400  Loss=1.2597392722795746\n",
            "Step=53500  Loss=1.2550290146529357\n",
            "Step=53600  Loss=1.2503333687627143\n",
            "Step=53700  Loss=1.2456524012904246\n",
            "Step=53800  Loss=1.240986178717127\n",
            "Step=53900  Loss=1.2363347673146825\n",
            "Step=54000  Loss=1.231698233136924\n",
            "Step=54100  Loss=1.2270766420107728\n",
            "Step=54200  Loss=1.2224700595272766\n",
            "Step=54300  Loss=1.217878551032587\n",
            "Step=54400  Loss=1.2133021816188734\n",
            "Step=54500  Loss=1.2087410161151584\n",
            "Step=54600  Loss=1.2041951190781226\n",
            "Step=54700  Loss=1.1996645547828124\n",
            "Step=54800  Loss=1.1951493872133065\n",
            "Step=54900  Loss=1.190649680053331\n",
            "Step=55000  Loss=1.1861654966768027\n",
            "Step=55100  Loss=1.181696900138326\n",
            "Step=55200  Loss=1.1772439531636347\n",
            "Step=55300  Loss=1.1728067181399828\n",
            "Step=55400  Loss=1.168385257106486\n",
            "Step=55500  Loss=1.163979631744412\n",
            "Step=55600  Loss=1.15958990336743\n",
            "Step=55700  Loss=1.1552161329118091\n",
            "Step=55800  Loss=1.1508583809265804\n",
            "Step=55900  Loss=1.1465167075636498\n",
            "Step=56000  Loss=1.142191172567887\n",
            "Step=56100  Loss=1.1378818352671591\n",
            "Step=56200  Loss=1.1335887545623469\n",
            "Step=56300  Loss=1.1293119889173142\n",
            "Step=56400  Loss=1.125051596348859\n",
            "Step=56500  Loss=1.120807634416626\n",
            "Step=56600  Loss=1.1165801602130028\n",
            "Step=56700  Loss=1.1123692303529835\n",
            "Step=56800  Loss=1.1081749009640178\n",
            "Step=56900  Loss=1.1039972276758376\n",
            "Step=57000  Loss=1.099836265610267\n",
            "Step=57100  Loss=1.0956920693710215\n",
            "Step=57200  Loss=1.0915646930334897\n",
            "Step=57300  Loss=1.0874541901345114\n",
            "Step=57400  Loss=1.0833606136621465\n",
            "Step=57500  Loss=1.0792840160454409\n",
            "Step=57600  Loss=1.0752244491441891\n",
            "Step=57700  Loss=1.0711819642387004\n",
            "Step=57800  Loss=1.0671566120195697\n",
            "Step=57900  Loss=1.0631484425774518\n",
            "Step=58000  Loss=1.05915750539285\n",
            "Step=58100  Loss=1.0551838493259114\n",
            "Step=58200  Loss=1.0512275226062449\n",
            "Step=58300  Loss=1.047288572822751\n",
            "Step=58400  Loss=1.0433670469134757\n",
            "Step=58500  Loss=1.0394629911554887\n",
            "Step=58600  Loss=1.0355764511547896\n",
            "Step=58700  Loss=1.0317074718362407\n",
            "Step=58800  Loss=1.0278560974335396\n",
            "Step=58900  Loss=1.0240223714792218\n",
            "Step=59000  Loss=1.0202063367947065\n",
            "Step=59100  Loss=1.0164080354803853\n",
            "Step=59200  Loss=1.0126275089057553\n",
            "Step=59300  Loss=1.0088647976996035\n",
            "Step=59400  Loss=1.0051199417402397\n",
            "Step=59500  Loss=1.0013929801457937\n",
            "Step=59600  Loss=0.9976839512645603\n",
            "Step=59700  Loss=0.9939928926654182\n",
            "Step=59800  Loss=0.990319841128303\n",
            "Step=59900  Loss=0.9866648326347611\n",
            "Step=60000  Loss=0.9830279023585691\n",
            "Step=60100  Loss=0.9794090846564297\n",
            "Step=60200  Loss=0.9758084130587497\n",
            "Step=60300  Loss=0.9722259202604996\n",
            "Step=60400  Loss=0.9686616381121598\n",
            "Step=60500  Loss=0.9651155976107568\n",
            "Step=60600  Loss=0.961587828890992\n",
            "Step=60700  Loss=0.9580783612164668\n",
            "Step=60800  Loss=0.9545872229710131\n",
            "Step=60900  Loss=0.9511144416501212\n",
            "Step=61000  Loss=0.9476600438524806\n",
            "Step=61100  Loss=0.9442240552716262\n",
            "Step=61200  Loss=0.9408065006877042\n",
            "Step=61300  Loss=0.9374074039593562\n",
            "Step=61400  Loss=0.9340267880157195\n",
            "Step=61500  Loss=0.930664674848558\n",
            "Step=61600  Loss=0.9273210855045219\n",
            "Step=61700  Loss=0.9239960400775351\n",
            "Step=61800  Loss=0.9206895577013228\n",
            "Step=61900  Loss=0.9174016565420781\n",
            "Step=62000  Loss=0.9141323537912677\n",
            "Step=62100  Loss=0.9108816656585897\n",
            "Step=62200  Loss=0.9076496073650768\n",
            "Step=62300  Loss=0.9044361931363539\n",
            "Step=62400  Loss=0.9012414361960543\n",
            "Step=62500  Loss=0.8980653487593961\n",
            "Step=62600  Loss=0.8949079420269176\n",
            "Step=62700  Loss=0.8917692261783856\n",
            "Step=62800  Loss=0.8886492103668718\n",
            "Step=62900  Loss=0.8855479027130003\n",
            "Step=63000  Loss=0.8824653102993777\n",
            "Step=63100  Loss=0.8794014391651994\n",
            "Step=63200  Loss=0.8763562943010387\n",
            "Step=63300  Loss=0.8733298796438276\n",
            "Step=63400  Loss=0.8703221980720213\n",
            "Step=63500  Loss=0.8673332514009611\n",
            "Step=63600  Loss=0.8643630403784285\n",
            "Step=63700  Loss=0.8614115646804021\n",
            "Step=63800  Loss=0.8584788229070158\n",
            "Step=63900  Loss=0.8555648125787203\n",
            "Step=64000  Loss=0.8526695301326539\n",
            "Step=64100  Loss=0.849792970919224\n",
            "Step=64200  Loss=0.846935129198902\n",
            "Step=64300  Loss=0.8440959981392331\n",
            "Step=64400  Loss=0.8412755698120664\n",
            "Step=64500  Loss=0.8384738351910052\n",
            "Step=64600  Loss=0.8356907841490837\n",
            "Step=64700  Loss=0.8329264054566655\n",
            "Step=64800  Loss=0.8301806867795732\n",
            "Step=64900  Loss=0.8274536146774526\n",
            "Step=65000  Loss=0.8247451746023626\n",
            "Step=65100  Loss=0.822055350897605\n",
            "Step=65200  Loss=0.8193841267967913\n",
            "Step=65300  Loss=0.8167314844231488\n",
            "Step=65400  Loss=0.8140974047890683\n",
            "Step=65500  Loss=0.8114818677958906\n",
            "Step=65600  Loss=0.8088848522339438\n",
            "Step=65700  Loss=0.8063063357828246\n",
            "Step=65800  Loss=0.8037462950119255\n",
            "Step=65900  Loss=0.8012047053812129\n",
            "Step=66000  Loss=0.7986815412422551\n",
            "Step=66100  Loss=0.796176775839506\n",
            "Step=66200  Loss=0.7936903813118338\n",
            "Step=66300  Loss=0.7912223286943116\n",
            "Step=66400  Loss=0.788772587920258\n",
            "Step=66500  Loss=0.786341127823536\n",
            "Step=66600  Loss=0.7839279161411061\n",
            "Step=66700  Loss=0.7815329195158389\n",
            "Step=66800  Loss=0.7791561034995821\n",
            "Step=66900  Loss=0.7767974325564887\n",
            "Step=67000  Loss=0.7744568700665997\n",
            "Step=67100  Loss=0.7721343783296893\n",
            "Step=67200  Loss=0.7698299185693623\n",
            "Step=67300  Loss=0.7675434509374155\n",
            "Step=67400  Loss=0.7652749345184546\n",
            "Step=67500  Loss=0.7630243273347666\n",
            "Step=67600  Loss=0.7607915863514522\n",
            "Step=67700  Loss=0.7585766674818135\n",
            "Step=67800  Loss=0.7563795255929964\n",
            "Step=67900  Loss=0.7542001145118895\n",
            "Step=68000  Loss=0.7520383870312736\n",
            "Step=68100  Loss=0.7498942949162287\n",
            "Step=68200  Loss=0.7477677889107904\n",
            "Step=68300  Loss=0.7456588187448524\n",
            "Step=68400  Loss=0.7435673331413236\n",
            "Step=68500  Loss=0.7414932798235284\n",
            "Step=68600  Loss=0.7394366055228544\n",
            "Step=68700  Loss=0.7373972559866422\n",
            "Step=68800  Loss=0.7353751759863143\n",
            "Step=68900  Loss=0.7333703093257485\n",
            "Step=69000  Loss=0.7313825988498829\n",
            "Step=69100  Loss=0.729411986453559\n",
            "Step=69200  Loss=0.7274584130905943\n",
            "Step=69300  Loss=0.7255218187830852\n",
            "Step=69400  Loss=0.7236021426309364\n",
            "Step=69500  Loss=0.7216993228216139\n",
            "Step=69600  Loss=0.719813296640119\n",
            "Step=69700  Loss=0.7179440004791781\n",
            "Step=69800  Loss=0.7160913698496484\n",
            "Step=69900  Loss=0.714255339391134\n",
            "Step=70000  Loss=0.7124358428828067\n",
            "Step=70100  Loss=0.7106328132544378\n",
            "Step=70200  Loss=0.7088461825976179\n",
            "Step=70300  Loss=0.7070758821771845\n",
            "Step=70400  Loss=0.7053218424428356\n",
            "Step=70500  Loss=0.7035839930409291\n",
            "Step=70600  Loss=0.7018622628264746\n",
            "Step=70700  Loss=0.7001565798752951\n",
            "Step=70800  Loss=0.6984668714963704\n",
            "Step=70900  Loss=0.6967930642443474\n",
            "Step=71000  Loss=0.6951350839322192\n",
            "Step=71100  Loss=0.6934928556441636\n",
            "Step=71200  Loss=0.6918663037485399\n",
            "Step=71300  Loss=0.6902553519110363\n",
            "Step=71400  Loss=0.688659923107965\n",
            "Step=71500  Loss=0.6870799396396984\n",
            "Step=71600  Loss=0.6855153231442429\n",
            "Step=71700  Loss=0.6839659946109445\n",
            "Step=71800  Loss=0.6824318743943191\n",
            "Step=71900  Loss=0.6809128822280053\n",
            "Step=72000  Loss=0.6794089372388348\n",
            "Step=72100  Loss=0.6779199579610089\n",
            "Step=72200  Loss=0.6764458623503827\n",
            "Step=72300  Loss=0.6749865677988478\n",
            "Step=72400  Loss=0.6735419911488103\n",
            "Step=72500  Loss=0.6721120487077534\n",
            "Step=72600  Loss=0.6706966562628871\n",
            "Step=72700  Loss=0.6692957290958698\n",
            "Step=72800  Loss=0.667909181997605\n",
            "Step=72900  Loss=0.6665369292831016\n",
            "Step=73000  Loss=0.6651788848063925\n",
            "Step=73100  Loss=0.6638349619755095\n",
            "Step=73200  Loss=0.6625050737675034\n",
            "Step=73300  Loss=0.6611891327435094\n",
            "Step=73400  Loss=0.6598870510638442\n",
            "Step=73500  Loss=0.6585987405031378\n",
            "Step=73600  Loss=0.6573241124654892\n",
            "Step=73700  Loss=0.6560630779996364\n",
            "Step=73800  Loss=0.6548155478141457\n",
            "Step=73900  Loss=0.6535814322926027\n",
            "Step=74000  Loss=0.6523606415088092\n",
            "Step=74100  Loss=0.6511530852419715\n",
            "Step=74200  Loss=0.6499586729918814\n",
            "Step=74300  Loss=0.6487773139940792\n",
            "Step=74400  Loss=0.6476089172349978\n",
            "Step=74500  Loss=0.6464533914670753\n",
            "Step=74600  Loss=0.6453106452238405\n",
            "Step=74700  Loss=0.6441805868349546\n",
            "Step=74800  Loss=0.6430631244412115\n",
            "Step=74900  Loss=0.6419581660094903\n",
            "Step=75000  Loss=0.6408656193476503\n",
            "Step=75100  Loss=0.6397853921193659\n",
            "Step=75200  Loss=0.6387173918588998\n",
            "Step=75300  Loss=0.6376615259858014\n",
            "Step=75400  Loss=0.6366177018195333\n",
            "Step=75500  Loss=0.6355858265940134\n",
            "Step=75600  Loss=0.6345658074720752\n",
            "Step=75700  Loss=0.633557551559835\n",
            "Step=75800  Loss=0.6325609659209639\n",
            "Step=75900  Loss=0.6315759575908623\n",
            "Step=76000  Loss=0.6306024335907257\n",
            "Step=76100  Loss=0.6296403009415056\n",
            "Step=76200  Loss=0.6286894666777528\n",
            "Step=76300  Loss=0.627749837861347\n",
            "Step=76400  Loss=0.6268213215950992\n",
            "Step=76500  Loss=0.6259038250362318\n",
            "Step=76600  Loss=0.6249972554097274\n",
            "Step=76700  Loss=0.6241015200215415\n",
            "Step=76800  Loss=0.6232165262716791\n",
            "Step=76900  Loss=0.6223421816671282\n",
            "Step=77000  Loss=0.6214783938346493\n",
            "Step=77100  Loss=0.620625070533414\n",
            "Step=77200  Loss=0.6197821196674927\n",
            "Step=77300  Loss=0.6189494492981856\n",
            "Step=77400  Loss=0.6181269676561966\n",
            "Step=77500  Loss=0.6173145831536442\n",
            "Step=77600  Loss=0.6165122043959073\n",
            "Step=77700  Loss=0.6157197401933058\n",
            "Step=77800  Loss=0.6149370995726074\n",
            "Step=77900  Loss=0.6141641917883657\n",
            "Step=78000  Loss=0.6134009263340807\n",
            "Step=78100  Loss=0.6126472129531824\n",
            "Step=78200  Loss=0.6119029616498353\n",
            "Step=78300  Loss=0.6111680826995606\n",
            "Step=78400  Loss=0.6104424866596763\n",
            "Step=78500  Loss=0.6097260843795488\n",
            "Step=78600  Loss=0.609018787010661\n",
            "Step=78700  Loss=0.608320506016489\n",
            "Step=78800  Loss=0.6076311531821902\n",
            "Step=78900  Loss=0.6069506406240992\n",
            "Step=79000  Loss=0.6062788807990316\n",
            "Step=79100  Loss=0.605615786513393\n",
            "Step=79200  Loss=0.6049612709320933\n",
            "Step=79300  Loss=0.6043152475872671\n",
            "Step=79400  Loss=0.6036776303867949\n",
            "Step=79500  Loss=0.6030483336226301\n",
            "Step=79600  Loss=0.6024272719789259\n",
            "Step=79700  Loss=0.6018143605399653\n",
            "Step=79800  Loss=0.601209514797893\n",
            "Step=79900  Loss=0.6006126506602485\n",
            "Step=80000  Loss=0.6000236844573001\n",
            "Step=80100  Loss=0.5994425329491814\n",
            "Step=80200  Loss=0.5988691133328273\n",
            "Step=80300  Loss=0.5983033432487148\n",
            "Step=80400  Loss=0.5977451407874028\n",
            "Step=80500  Loss=0.5971944244958756\n",
            "Step=80600  Loss=0.5966511133836904\n",
            "Step=80700  Loss=0.5961151269289262\n",
            "Step=80800  Loss=0.5955863850839384\n",
            "Step=80900  Loss=0.5950648082809178\n",
            "Step=81000  Loss=0.5945503174372566\n",
            "Step=81100  Loss=0.59404283396072\n",
            "Step=81200  Loss=0.5935422797544255\n",
            "Step=81300  Loss=0.5930485772216336\n",
            "Step=81400  Loss=0.5925616492703452\n",
            "Step=81500  Loss=0.5920814193177135\n",
            "Step=81600  Loss=0.591607811294267\n",
            "Step=81700  Loss=0.591140749647948\n",
            "Step=81800  Loss=0.5906801593479658\n",
            "Step=81900  Loss=0.5902259658884682\n",
            "Step=82000  Loss=0.5897780952920314\n",
            "Step=82100  Loss=0.5893364741129697\n",
            "Step=82200  Loss=0.5889010294404705\n",
            "Step=82300  Loss=0.5884716889015497\n",
            "Step=82400  Loss=0.5880483806638366\n",
            "Step=82500  Loss=0.5876310334381836\n",
            "Step=82600  Loss=0.58721957648111\n",
            "Step=82700  Loss=0.5868139395970724\n",
            "Step=82800  Loss=0.5864140531405746\n",
            "Step=82900  Loss=0.5860198480181097\n",
            "Step=83000  Loss=0.5856312556899419\n",
            "Step=83100  Loss=0.5852482081717295\n",
            "Step=83200  Loss=0.584870638035989\n",
            "Step=83300  Loss=0.5844984784134061\n",
            "Step=83400  Loss=0.5841316629939916\n",
            "Step=83500  Loss=0.5837701260280885\n",
            "Step=83600  Loss=0.5834138023272312\n",
            "Step=83700  Loss=0.5830626272648579\n",
            "Step=83800  Loss=0.5827165367768806\n",
            "Step=83900  Loss=0.5823754673621128\n",
            "Step=84000  Loss=0.5820393560825602\n",
            "Step=84100  Loss=0.5817081405635754\n",
            "Step=84200  Loss=0.5813817589938785\n",
            "Step=84300  Loss=0.5810601501254448\n",
            "Step=84400  Loss=0.5807432532732686\n",
            "Step=84500  Loss=0.580431008314996\n",
            "Step=84600  Loss=0.5801233556904382\n",
            "Step=84700  Loss=0.5798202364009594\n",
            "Step=84800  Loss=0.5795215920087514\n",
            "Step=84900  Loss=0.5792273646359877\n",
            "Step=85000  Loss=0.5789374969638661\n",
            "Step=85100  Loss=0.5786519322315418\n",
            "Step=85200  Loss=0.5783706142349486\n",
            "Step=85300  Loss=0.578093487325518\n",
            "Step=85400  Loss=0.5778204964087916\n",
            "Step=85500  Loss=0.577551586942936\n",
            "Step=85600  Loss=0.5772867049371554\n",
            "Step=85700  Loss=0.577025796950013\n",
            "Step=85800  Loss=0.5767688100876557\n",
            "Step=85900  Loss=0.5765156920019476\n",
            "Step=86000  Loss=0.5762663908885184\n",
            "Step=86100  Loss=0.5760208554847226\n",
            "Step=86200  Loss=0.5757790350675166\n",
            "Step=86300  Loss=0.5755408794512543\n",
            "Step=86400  Loss=0.5753063389854042\n",
            "Step=86500  Loss=0.5750753645521899\n",
            "Step=86600  Loss=0.5748479075641564\n",
            "Step=86700  Loss=0.5746239199616658\n",
            "Step=86800  Loss=0.5744033542103217\n",
            "Step=86900  Loss=0.5741861632983285\n",
            "Step=87000  Loss=0.5739723007337845\n",
            "Step=87100  Loss=0.5737617205419134\n",
            "Step=87200  Loss=0.5735543772622351\n",
            "Step=87300  Loss=0.5733502259456773\n",
            "Step=87400  Loss=0.5731492221516342\n",
            "Step=87500  Loss=0.5729513219449671\n",
            "Step=87600  Loss=0.5727564818929565\n",
            "Step=87700  Loss=0.5725646590622037\n",
            "Step=87800  Loss=0.5723758110154844\n",
            "Step=87900  Loss=0.572189895808557\n",
            "Step=88000  Loss=0.5720068719869277\n",
            "Step=88100  Loss=0.5718266985825734\n",
            "Step=88200  Loss=0.5716493351106259\n",
            "Step=88300  Loss=0.5714747415660169\n",
            "Step=88400  Loss=0.5713028784200892\n",
            "Step=88500  Loss=0.5711337066171722\n",
            "Step=88600  Loss=0.5709671875711263\n",
            "Step=88700  Loss=0.5708032831618556\n",
            "Step=88800  Loss=0.5706419557317941\n",
            "Step=88900  Loss=0.5704831680823634\n",
            "Step=89000  Loss=0.5703268834704062\n",
            "Step=89100  Loss=0.5701730656045945\n",
            "Step=89200  Loss=0.5700216786418187\n",
            "Step=89300  Loss=0.5698726871835542\n",
            "Step=89400  Loss=0.5697260562722105\n",
            "Step=89500  Loss=0.569581751387462\n",
            "Step=89600  Loss=0.5694397384425648\n",
            "Step=89700  Loss=0.5692999837806576\n",
            "Step=89800  Loss=0.5691624541710512\n",
            "Step=89900  Loss=0.5690271168055058\n",
            "Step=90000  Loss=0.5688939392944986\n",
            "Step=90100  Loss=0.5687628896634839\n",
            "Step=90200  Loss=0.5686339363491434\n",
            "Step=90300  Loss=0.568507048195633\n",
            "Step=90400  Loss=0.568382194450824\n",
            "Step=90500  Loss=0.5682593447625407\n",
            "Step=90600  Loss=0.5681384691747949\n",
            "Step=90700  Loss=0.5680195381240218\n",
            "Step=90800  Loss=0.5679025224353129\n",
            "Step=90900  Loss=0.5677873933186511\n",
            "Step=91000  Loss=0.56767412236515\n",
            "Step=91100  Loss=0.5675626815432931\n",
            "Step=91200  Loss=0.5674530431951792\n",
            "Step=91300  Loss=0.5673451800327731\n",
            "Step=91400  Loss=0.5672390651341619\n",
            "Step=91500  Loss=0.5671346719398188\n",
            "Step=91600  Loss=0.567031974248875\n",
            "Step=91700  Loss=0.5669309462153999\n",
            "Step=91800  Loss=0.566831562344692\n",
            "Step=91900  Loss=0.5667337974895807\n",
            "Step=92000  Loss=0.5666376268467377\n",
            "Step=92100  Loss=0.5665430259530024\n",
            "Step=92200  Loss=0.56644997068172\n",
            "Step=92300  Loss=0.5663584372390925\n",
            "Step=92400  Loss=0.5662684021605445\n",
            "Step=92500  Loss=0.5661798423071046\n",
            "Step=92600  Loss=0.566092734861802\n",
            "Step=92700  Loss=0.566007057326079\n",
            "Step=92800  Loss=0.5659227875162215\n",
            "Step=92900  Loss=0.5658399035598064\n",
            "Step=93000  Loss=0.5657583838921669\n",
            "Step=93100  Loss=0.5656782072528772\n",
            "Step=93200  Loss=0.5655993526822561\n",
            "Step=93300  Loss=0.5655217995178887\n",
            "Step=93400  Loss=0.5654455273911719\n",
            "Step=93500  Loss=0.5653705162238764\n",
            "Step=93600  Loss=0.5652967462247319\n",
            "Step=93700  Loss=0.5652241978860342\n",
            "Step=93800  Loss=0.5651528519802724\n",
            "Step=93900  Loss=0.5650826895567794\n",
            "Step=94000  Loss=0.5650136919384056\n",
            "Step=94100  Loss=0.5649458407182135\n",
            "Step=94200  Loss=0.5648791177561975\n",
            "Step=94300  Loss=0.5648135051760266\n",
            "Step=94400  Loss=0.5647489853618108\n",
            "Step=94500  Loss=0.5646855409548909\n",
            "Step=94600  Loss=0.5646231548506543\n",
            "Step=94700  Loss=0.5645618101953743\n",
            "Step=94800  Loss=0.5645014903830746\n",
            "Step=94900  Loss=0.5644421790524178\n",
            "Step=95000  Loss=0.5643838600836211\n",
            "Step=95100  Loss=0.5643265175953958\n",
            "Step=95200  Loss=0.5642701359419127\n",
            "Step=95300  Loss=0.5642146997097942\n",
            "Step=95400  Loss=0.5641601937151307\n",
            "Step=95500  Loss=0.5641066030005242\n",
            "Step=95600  Loss=0.5640539128321576\n",
            "Step=95700  Loss=0.56400210869689\n",
            "Step=95800  Loss=0.5639511762993794\n",
            "Step=95900  Loss=0.5639011015592291\n",
            "Step=96000  Loss=0.5638518706081643\n",
            "Step=96100  Loss=0.5638034697872324\n",
            "Step=96200  Loss=0.5637558856440307\n",
            "Step=96300  Loss=0.5637091049299608\n",
            "Step=96400  Loss=0.5636631145975095\n",
            "Step=96500  Loss=0.5636179017975567\n",
            "Step=96600  Loss=0.5635734538767083\n",
            "Step=96700  Loss=0.5635297583746589\n",
            "Step=96800  Loss=0.5634868030215778\n",
            "Step=96900  Loss=0.5634445757355234\n",
            "Step=97000  Loss=0.5634030646198839\n",
            "Step=97100  Loss=0.5633622579608438\n",
            "Step=97200  Loss=0.5633221442248784\n",
            "Step=97300  Loss=0.5632827120562728\n",
            "Step=97400  Loss=0.563243950274669\n",
            "Step=97500  Loss=0.563205847872638\n",
            "Step=97600  Loss=0.5631683940132791\n",
            "Step=97700  Loss=0.5631315780278442\n",
            "Step=97800  Loss=0.5630953894133895\n",
            "Step=97900  Loss=0.5630598178304526\n",
            "Step=98000  Loss=0.5630248531007542\n",
            "Step=98100  Loss=0.5629904852049288\n",
            "Step=98200  Loss=0.5629567042802763\n",
            "Step=98300  Loss=0.5629235006185442\n",
            "Step=98400  Loss=0.5628908646637314\n",
            "Step=98500  Loss=0.5628587870099189\n",
            "Step=98600  Loss=0.5628272583991258\n",
            "Step=98700  Loss=0.5627962697191896\n",
            "Step=98800  Loss=0.5627658120016722\n",
            "Step=98900  Loss=0.5627358764197893\n",
            "Step=99000  Loss=0.5627064542863661\n",
            "Step=99100  Loss=0.562677537051816\n",
            "Step=99200  Loss=0.5626491163021448\n",
            "Step=99300  Loss=0.5626211837569778\n",
            "Step=99400  Loss=0.5625937312676118\n",
            "Step=99500  Loss=0.5625667508150907\n",
            "Step=99600  Loss=0.5625402345083044\n",
            "Step=99700  Loss=0.5625141745821114\n",
            "Step=99800  Loss=0.562488563395485\n",
            "Step=99900  Loss=0.5624633934296822\n",
            "Step=100000  Loss=0.5624386572864355\n",
            "Total training time: 4.420100 seconds\n",
            "Predictions [[ 1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1 -1  1  1  1  1  1 -1\n",
            "   1  1 -1 -1  1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1 -1  1 -1  1\n",
            "   1  1 -1  1  1 -1  1  1  1  1 -1 -1  1  1  1 -1 -1  1  1  1  1  1  1  1\n",
            "  -1  1 -1 -1 -1  1  1  1  1  1 -1  1 -1  1  1  1  1 -1 -1  1 -1 -1 -1  1\n",
            "  -1  1 -1  1 -1 -1  1  1 -1  1  1  1 -1  1  1  1 -1  1  1 -1  1  1 -1 -1\n",
            "  -1  1  1  1  1 -1 -1  1 -1  1  1 -1 -1  1 -1  1  1  1 -1  1 -1  1  1 -1\n",
            "  -1  1  1  1 -1  1 -1  1  1  1 -1 -1  1  1 -1 -1  1 -1  1  1 -1 -1  1  1\n",
            "  -1 -1 -1  1 -1 -1 -1 -1 -1  1  1 -1 -1  1 -1 -1  1 -1  1  1  1 -1  1  1\n",
            "  -1 -1  1  1  1  1  1  1  1  1  1  1 -1 -1  1 -1 -1 -1 -1 -1 -1  1  1  1\n",
            "   1  1 -1  1  1  1  1 -1  1 -1  1  1 -1 -1  1  1  1 -1  1  1  1  1  1  1\n",
            "  -1 -1 -1 -1  1 -1 -1  1  1  1 -1  1  1  1  1  1  1 -1 -1 -1  1 -1 -1 -1\n",
            "   1 -1  1 -1  1 -1  1  1  1  1  1 -1  1  1 -1  1  1  1  1 -1  1 -1  1  1\n",
            "   1  1  1  1 -1 -1 -1  1  1  1 -1 -1 -1  1  1  1  1  1 -1  1  1 -1  1  1\n",
            "   1 -1 -1 -1 -1 -1  1  1 -1 -1  1 -1 -1  1  1  1  1  1 -1  1  1 -1  1  1\n",
            "   1  1  1 -1 -1]]\n",
            "Ground truth [[ 1  1 -1  1  1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1\n",
            "   1  1 -1 -1  1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1 -1  1 -1  1\n",
            "   1  1 -1  1  1 -1  1  1  1  1 -1  1  1  1  1 -1 -1  1  1  1  1  1  1  1\n",
            "  -1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1 -1 -1  1\n",
            "  -1  1 -1  1  1 -1  1  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1\n",
            "  -1 -1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1 -1\n",
            "  -1  1  1  1 -1  1 -1  1  1  1  1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1\n",
            "  -1 -1  1  1  1  1 -1  1 -1  1  1  1 -1  1  1 -1  1  1  1  1 -1  1  1  1\n",
            "  -1 -1  1  1  1  1 -1 -1  1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1  1  1\n",
            "  -1  1 -1  1  1  1  1 -1  1 -1  1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1\n",
            "  -1 -1  1 -1  1 -1 -1  1  1  1  1 -1  1 -1  1  1 -1  1 -1 -1  1 -1 -1  1\n",
            "   1  1  1 -1  1 -1  1  1  1 -1  1 -1 -1  1  1  1 -1  1  1 -1  1 -1  1  1\n",
            "   1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1  1  1 -1  1\n",
            "   1 -1 -1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1\n",
            "   1 -1  1 -1  1]]\n",
            "Training set mean accuracy: 0.7977\n",
            "Validation set mean accuracy: 0.7544\n",
            "Testing set mean accuracy: 0.7544\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=1\n",
            "Total training time: 0.000064 seconds\n",
            "Predictions [[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1]]\n",
            "Ground truth [[ 1  1 -1  1  1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1\n",
            "   1  1 -1 -1  1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1 -1  1 -1  1\n",
            "   1  1 -1  1  1 -1  1  1  1  1 -1  1  1  1  1 -1 -1  1  1  1  1  1  1  1\n",
            "  -1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1 -1 -1  1\n",
            "  -1  1 -1  1  1 -1  1  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1\n",
            "  -1 -1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1 -1\n",
            "  -1  1  1  1 -1  1 -1  1  1  1  1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1\n",
            "  -1 -1  1  1  1  1 -1  1 -1  1  1  1 -1  1  1 -1  1  1  1  1 -1  1  1  1\n",
            "  -1 -1  1  1  1  1 -1 -1  1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1  1  1\n",
            "  -1  1 -1  1  1  1  1 -1  1 -1  1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1\n",
            "  -1 -1  1 -1  1 -1 -1  1  1  1  1 -1  1 -1  1  1 -1  1 -1 -1  1 -1 -1  1\n",
            "   1  1  1 -1  1 -1  1  1  1 -1  1 -1 -1  1  1  1 -1  1  1 -1  1 -1  1  1\n",
            "   1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1  1  1 -1  1\n",
            "   1 -1 -1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1\n",
            "   1 -1  1 -1  1]]\n",
            "Training set mean accuracy: 0.3548\n",
            "Validation set mean accuracy: 0.4123\n",
            "Testing set mean accuracy: 0.3860\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=None \n",
            "\t batch_size=56 \n",
            "\t T=1\n",
            "Total training time: 0.000154 seconds\n",
            "Predictions [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "Ground truth [[ 1  1 -1  1  1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1\n",
            "   1  1 -1 -1  1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1 -1  1 -1  1\n",
            "   1  1 -1  1  1 -1  1  1  1  1 -1  1  1  1  1 -1 -1  1  1  1  1  1  1  1\n",
            "  -1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1 -1 -1  1\n",
            "  -1  1 -1  1  1 -1  1  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1\n",
            "  -1 -1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1 -1\n",
            "  -1  1  1  1 -1  1 -1  1  1  1  1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1\n",
            "  -1 -1  1  1  1  1 -1  1 -1  1  1  1 -1  1  1 -1  1  1  1  1 -1  1  1  1\n",
            "  -1 -1  1  1  1  1 -1 -1  1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1  1  1\n",
            "  -1  1 -1  1  1  1  1 -1  1 -1  1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1\n",
            "  -1 -1  1 -1  1 -1 -1  1  1  1  1 -1  1 -1  1  1 -1  1 -1 -1  1 -1 -1  1\n",
            "   1  1  1 -1  1 -1  1  1  1 -1  1 -1 -1  1  1  1 -1  1  1 -1  1 -1  1  1\n",
            "   1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1  1  1 -1  1\n",
            "   1 -1 -1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1\n",
            "   1 -1  1 -1  1]]\n",
            "Training set mean accuracy: 0.6452\n",
            "Validation set mean accuracy: 0.5877\n",
            "Testing set mean accuracy: 0.6140\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=0.9 \n",
            "\t batch_size=56 \n",
            "\t T=1\n",
            "Total training time: 0.000086 seconds\n",
            "Predictions [[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1]]\n",
            "Ground truth [[ 1  1 -1  1  1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1\n",
            "   1  1 -1 -1  1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1 -1  1 -1  1\n",
            "   1  1 -1  1  1 -1  1  1  1  1 -1  1  1  1  1 -1 -1  1  1  1  1  1  1  1\n",
            "  -1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1 -1 -1  1\n",
            "  -1  1 -1  1  1 -1  1  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1\n",
            "  -1 -1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1 -1\n",
            "  -1  1  1  1 -1  1 -1  1  1  1  1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1\n",
            "  -1 -1  1  1  1  1 -1  1 -1  1  1  1 -1  1  1 -1  1  1  1  1 -1  1  1  1\n",
            "  -1 -1  1  1  1  1 -1 -1  1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1  1  1\n",
            "  -1  1 -1  1  1  1  1 -1  1 -1  1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1\n",
            "  -1 -1  1 -1  1 -1 -1  1  1  1  1 -1  1 -1  1  1 -1  1 -1 -1  1 -1 -1  1\n",
            "   1  1  1 -1  1 -1  1  1  1 -1  1 -1 -1  1  1  1 -1  1  1 -1  1 -1  1  1\n",
            "   1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1  1  1 -1  1 -1 -1  1  1  1 -1  1\n",
            "   1 -1 -1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1\n",
            "   1 -1  1 -1  1]]\n",
            "Training set mean accuracy: 0.3548\n",
            "Validation set mean accuracy: 0.4123\n",
            "Testing set mean accuracy: 0.3860\n",
            "\n",
            "Preprocessing the digits greater or less than 5 dataset (1797 samples, 64 feature dimensions)\n",
            "***** Results on the digits greater or less than 5 dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9165\n",
            "Validation set mean accuracy: 0.8663\n",
            "Testing set mean accuracy: 0.9056\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=1\n",
            "Total training time: 0.000230 seconds\n",
            "Predictions [[-1 -1  1 ...  1 -1 -1]]\n",
            "Ground truth [[-1  1  1 ...  1 -1  1]]\n",
            "Training set mean accuracy: 0.5436\n",
            "Validation set mean accuracy: 0.5265\n",
            "Testing set mean accuracy: 0.4972\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=1\n",
            "Total training time: 0.000185 seconds\n",
            "Predictions [[-1 -1 -1 ... -1 -1 -1]]\n",
            "Ground truth [[-1  1  1 ...  1 -1  1]]\n",
            "Training set mean accuracy: 0.5056\n",
            "Validation set mean accuracy: 0.5181\n",
            "Testing set mean accuracy: 0.4583\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=None \n",
            "\t batch_size=150 \n",
            "\t T=1\n",
            "Total training time: 0.000267 seconds\n",
            "Predictions [[-1 -1 -1 ... -1 -1 -1]]\n",
            "Ground truth [[-1  1  1 ...  1 -1  1]]\n",
            "Training set mean accuracy: 0.5056\n",
            "Validation set mean accuracy: 0.5181\n",
            "Testing set mean accuracy: 0.4583\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.1 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=0.9 \n",
            "\t batch_size=150 \n",
            "\t T=1\n",
            "Total training time: 0.000278 seconds\n",
            "Predictions [[1 1 1 ... 1 1 1]]\n",
            "Ground truth [[-1  1  1 ...  1 -1  1]]\n",
            "Training set mean accuracy: 0.4944\n",
            "Validation set mean accuracy: 0.4819\n",
            "Testing set mean accuracy: 0.5417\n",
            "\n",
            "Preprocessing the fir and pine coverage dataset (495141 samples, 54 feature dimensions)\n",
            "***** Results on the fir and pine coverage dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.7534\n",
            "Validation set mean accuracy: 0.7541\n",
            "Testing set mean accuracy: 0.7532\n",
            "***** Results of our logistic regression model trained on fir and pine coverage dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=None \n",
            "\t batch_size=49000 \n",
            "\t T=1\n",
            "Total training time: 0.135485 seconds\n",
            "Predictions [[1 1 1 ... 1 1 1]]\n",
            "Ground truth [[ 1  1  1 ... -1 -1 -1]]\n",
            "Training set mean accuracy: 0.4271\n",
            "Validation set mean accuracy: 0.4280\n",
            "Testing set mean accuracy: 0.4298\n",
            "***** Results of our logistic regression model trained on fir and pine coverage dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.1 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=0.9 \n",
            "\t batch_size=49000 \n",
            "\t T=1\n",
            "Total training time: 0.130785 seconds\n",
            "Predictions [[1 1 1 ... 1 1 1]]\n",
            "Ground truth [[ 1  1  1 ... -1 -1 -1]]\n",
            "Training set mean accuracy: 0.4271\n",
            "Validation set mean accuracy: 0.4280\n",
            "Testing set mean accuracy: 0.4298\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load breast cancer, digits, and tree coverage datasets\n",
        "datasets = [\n",
        "    skdata.load_breast_cancer(),\n",
        "    skdata.load_digits(),\n",
        "    skdata.fetch_covtype()\n",
        "]\n",
        "dataset_names = [\n",
        "    'breast cancer',\n",
        "    'digits greater or less than 5',\n",
        "    'fir and pine coverage',\n",
        "]\n",
        "\n",
        "# Loss functions to minimize\n",
        "dataset_optimizer_types = [\n",
        "    # For breast cancer dataset\n",
        "    [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    ], [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For fir and pine coverage dataset\n",
        "    ], [\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    ]\n",
        "]\n",
        "\n",
        "# DONE?: Select hyperparameters\n",
        "\n",
        "# Step size (always used)\n",
        "dataset_alphas = [\n",
        "    # For breast cancer dataset\n",
        "    [1e-10, 1e-4, 1e-4, 0.01],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [1e-4, 0.01, 0.01, 0.1],\n",
        "    # For fir and pine coverage dataset\n",
        "    [0.01, 0.1]\n",
        "]\n",
        "\n",
        "# How much the learning rate should decrease over time (only for SGD)\n",
        "dataset_eta_decay_factors = [\n",
        "    # For breast cancer dataset\n",
        "    [None, None, 0.99, 0.99],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, None, 0.99, 0.99],\n",
        "    # For fir and pine coverage dataset\n",
        "    [0.99, 0.99]\n",
        "]\n",
        "\n",
        "# Momentum (only for momentum-based duh)\n",
        "# None for no momentum\n",
        "dataset_betas = [\n",
        "    # For breast cancer dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For fir and pine coverage dataset\n",
        "    [None, 0.9]\n",
        "]\n",
        "\n",
        "# Samples (only for SGD)\n",
        "dataset_batch_sizes = [\n",
        "    # For breast cancer dataset\n",
        "    # N = 569\n",
        "    [None, None, 56, 56],\n",
        "    # For digits greater than or less than 5 dataset \n",
        "    # N = 1797\n",
        "    [None, None, 150, 150],\n",
        "    # For fir and pine coverage dataset \n",
        "    # # N = 495141\n",
        "    [49000, 49000]\n",
        "]\n",
        "\n",
        "\n",
        "# Iterations\n",
        "dataset_Ts = [\n",
        "    # For breast cancer dataset\n",
        "    [150000, 1, 1, 1],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [1, 1, 1, 1],\n",
        "    # For fir and pine coverage dataset\n",
        "    [1, 1]\n",
        "]\n",
        "\n",
        "# Zip up all dataset options\n",
        "dataset_options = zip(\n",
        "    datasets,\n",
        "    dataset_names,\n",
        "    dataset_optimizer_types,\n",
        "    dataset_alphas,\n",
        "    dataset_eta_decay_factors,\n",
        "    dataset_betas,\n",
        "    dataset_batch_sizes,\n",
        "    dataset_Ts)\n",
        "\n",
        "\n",
        "for options in dataset_options:\n",
        "\n",
        "    # Unpack dataset options\n",
        "    dataset, \\\n",
        "        dataset_name, \\\n",
        "        optimizer_types, \\\n",
        "        alphas, \\\n",
        "        eta_decay_factors, \\\n",
        "        betas, \\\n",
        "        batch_sizes, \\\n",
        "        Ts = options\n",
        "\n",
        "    '''\n",
        "    Create the training, validation and testing splits\n",
        "    '''\n",
        "    x = dataset.data\n",
        "    y = dataset.target\n",
        "\n",
        "    if dataset_name == 'digits greater or less than 5':\n",
        "        y[y < 5] = 1\n",
        "        y[y >= 5] = 0\n",
        "    elif dataset_name == 'fir and pine coverage':\n",
        "\n",
        "        idx_fir_or_pine = np.where(np.logical_or(y == 1, y == 2))[0]\n",
        "\n",
        "        x = x[idx_fir_or_pine, :]\n",
        "        y = y[idx_fir_or_pine]\n",
        "\n",
        "        # Pine class: 0; Fir class: 1\n",
        "        y[y == 2] = 0\n",
        "\n",
        "    print('Preprocessing the {} dataset ({} samples, {} feature dimensions)'.format(dataset_name, x.shape[0], x.shape[1]))\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(x.shape[0])\n",
        "\n",
        "    # Choose the first 60% as training set, next 20% as validation and the rest as testing\n",
        "    train_split_idx = int(0.60 * x.shape[0])\n",
        "    val_split_idx = int(0.80 * x.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:val_split_idx]\n",
        "    test_indices = shuffled_indices[val_split_idx:]\n",
        "\n",
        "    # Select the examples from x and y to construct our training, validation, testing sets\n",
        "    x_train, y_train = x[train_indices, :], y[train_indices]\n",
        "    x_val, y_val = x[val_indices, :], y[val_indices]\n",
        "    x_test, y_test = x[test_indices, :], y[test_indices]\n",
        "\n",
        "    '''\n",
        "    Trains and tests logistic regression model from scikit-learn\n",
        "    '''\n",
        "    model_scikit = LogisticRegressionSciKit(penalty=None, fit_intercept=False)\n",
        "\n",
        "    # DONE?: Train scikit-learn logistic regression model\n",
        "    model_scikit.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "    print('***** Results on the {} dataset using scikit-learn logistic regression model *****'.format(dataset_name))\n",
        "\n",
        "    # DONE: Score model using mean accuracy on training set\n",
        "    predictions_train = model_scikit.predict(x_train)\n",
        "    score_train = skmetrics.accuracy_score(y_train, predictions_train)\n",
        "    print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "    # DONE: Score model using mean accuracy on validation set\n",
        "    predictions_val = model_scikit.predict(x_val)\n",
        "    score_val = skmetrics.accuracy_score(y_val, predictions_val)\n",
        "    print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "    # DONE: Score model using mean accuracy on testing set\n",
        "    predictions_test = model_scikit.predict(x_test)\n",
        "    score_test = skmetrics.accuracy_score(y_test, predictions_test)\n",
        "    print('Testing set mean accuracy: {:.4f}'.format(score_test))\n",
        "\n",
        "    '''\n",
        "    Trains, validates, and tests our logistic regression model for binary classification\n",
        "    '''\n",
        "    # Take the transpose of the dataset to match the dimensions discussed in lecture\n",
        "    # i.e., (N x d) to (d x N)\n",
        "    x_train = np.transpose(x_train, axes=(1, 0))\n",
        "    x_val = np.transpose(x_val, axes=(1, 0))\n",
        "    x_test = np.transpose(x_test, axes=(1, 0))\n",
        "    y_train = np.expand_dims(y_train, axis=0)\n",
        "    y_val = np.expand_dims(y_val, axis=0)\n",
        "    y_test = np.expand_dims(y_test, axis=0)\n",
        "\n",
        "    # DONE: Set the ground truth to the appropriate classes (integers) according to lecture\n",
        "    # -1 and +1\n",
        "    y_train = np.where(y_train == 0, -1, 1)\n",
        "    y_val = np.where(y_val == 0, -1, 1)\n",
        "    y_test = np.where(y_test == 0, -1, 1)\n",
        "\n",
        "    model_options = zip(optimizer_types, alphas, eta_decay_factors, betas, batch_sizes, Ts)\n",
        "\n",
        "    for optimizer_type, alpha, eta_decay_factor, beta, batch_size, T in model_options:\n",
        "\n",
        "        # DONE: Initialize our logistic regression model\n",
        "        model_ours = LogisticRegression()\n",
        "\n",
        "        print('***** Results of our logistic regression model trained on {} dataset *****'.format(dataset_name))\n",
        "        print('\\t optimizer_type={} \\n\\t alpha={} \\n\\t eta_decay_factor={} \\n\\t beta={} \\n\\t batch_size={} \\n\\t T={}'.format(\n",
        "            optimizer_type, alpha, eta_decay_factor, beta, batch_size, T))\n",
        "\n",
        "        time_start = time.time()\n",
        "\n",
        "        # DONE: Train model on training set\n",
        "        model_ours.fit(\n",
        "            x_train,\n",
        "            y_train,\n",
        "            T,\n",
        "            alpha,\n",
        "            eta_decay_factor,\n",
        "            beta,\n",
        "            batch_size,\n",
        "            optimizer_type\n",
        "        )\n",
        "\n",
        "        time_elapsed = time.time() - time_start\n",
        "        print('Total training time: {:3f} seconds'.format(time_elapsed))\n",
        "\n",
        "        # DONE?: Score model using mean accuracy on training set\n",
        "        predictions_train = model_ours.predict(x_train)\n",
        "        score_train = np.mean(predictions_train == y_train)\n",
        "        print(\"Predictions\", predictions_train)\n",
        "        print(\"Ground truth\", y_train)\n",
        "        print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "        # DONE: Score model using mean accuracy on training set\n",
        "        predictions_val = model_ours.predict(x_val)\n",
        "        score_val = np.mean(predictions_val == y_val)\n",
        "        print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "        # DONE: Score model using mean accuracy on training set\n",
        "        predictions_test = model_ours.predict(x_test)\n",
        "        score_test = np.mean(predictions_test == y_test)\n",
        "        print('Testing set mean accuracy: {:.4f}'.format(score_test))\n",
        "\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
