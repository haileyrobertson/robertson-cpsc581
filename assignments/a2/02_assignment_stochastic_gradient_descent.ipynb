{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r37l-FPc9o0h"
      },
      "source": [
        "**Assignment 2: Stochastic Gradient Descent and Momentum**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "*Student: Hailey Robertson*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "\n",
        "4. Move the 02_assignment_stochastic_gradient_descent.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments/02_assignment_stochastic_gradient_descent.ipynb\n",
        "```\n",
        "\n",
        "In this assignment, we will optimize a linear function for the logistic regression task using the stochastic gradient descent and its momentum variant. We will test them on several binary classification datasets (breast cancer, digits larger or less than 5, and fir and pine coverage). We will implement a training and validation loop for the binary classification task and test it on the testing split for each dataset.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training, validation, and testing scores.\n",
        "\n",
        "```\n",
        "Report training, validation, and testing scores here.\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: N/A\n",
        "```\n",
        "\n",
        "\n",
        "**IMPORTANT**:\n",
        "\n",
        "- For full credit, your mean classification accuracies for all trained models across all datasets should be no more than 8% worse the scores achieved by sci-kit learn's logistic regression model across training, validation and testing splits.\n",
        "\n",
        "- You may not use batch sizes of more than 10% of the dataset size for stochastic gradient descent and momentum stochastic gradient descent.\n",
        "\n",
        "- You will only need to experiment with gradient descent (GD) and momentum gradient descent (momentum GD) on breast cancer and digits (toy) datasets. It will take too long to run them on fir and pine coverage (realistic) dataset to get reasonable numbers. Of course, you may try them on fir and pine coverage :) but they will not count towards your grade.\n",
        "\n",
        "- Note the run time speed up when comparing GD and momemtum GD with stochastic gradient descent (SGD) and momentum stochastic gradient descent (momentum SGD)! Even though they are faster and observing batches instead of the full dataset at each time step, they can still achieving similar accuracies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "koDraeo69YZH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn.linear_model import LogisticRegression as LogisticRegressionSciKit\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAR4bm26XZiJ"
      },
      "source": [
        "Implementation of stochastic gradient descent optimizer for logistic loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bI62IQ3d9SpW"
      },
      "outputs": [],
      "source": [
        "class Optimizer(object):\n",
        "\n",
        "    def __init__(self, alpha, eta_decay_factor, beta, optimizer_type):\n",
        "        '''\n",
        "        Arg(s):\n",
        "            alpha : float\n",
        "                initial learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "        '''\n",
        "\n",
        "        self.__alpha = alpha\n",
        "        self.__eta_decay_factor = eta_decay_factor\n",
        "        self.__beta = beta\n",
        "        self.__optimizer_type = optimizer_type\n",
        "        self.__momentum = None\n",
        "\n",
        "    def __compute_gradients(self, w, x, y, loss_func='logistic'):\n",
        "        '''\n",
        "        Returns the gradient of a loss function\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            numpy[float32] : d x 1 gradients\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement compute_gradient function\n",
        "\n",
        "        if loss_func == 'logistic':\n",
        "            # print('w shape: {}'.format(w.shape)) # (1, 30)\n",
        "            N = x.shape[1] # number of samples, 341\n",
        "            z = np.dot(w.T, x)\n",
        "            # print('y shape: {}'.format(y.shape)) # (1, 341)\n",
        "            # print('z shape: {}'.format(z.shape)) # (1, 341)\n",
        "            sigmoid = 1 / (1 + np.exp(-y * z)) \n",
        "            gradient = -np.dot(x, ((y * (1 - sigmoid))).T) / N\n",
        "\n",
        "            # print('gradient shape: {}'.format(gradient.shape)) \n",
        "\n",
        "            return gradient\n",
        "        \n",
        "        else:\n",
        "            raise ValueError('Unupported loss function: {}'.format(loss_func))\n",
        "\n",
        "    def __polynomial_decay(self, time_step):\n",
        "        '''\n",
        "        Computes the polynomial decay factor t^{-a}\n",
        "\n",
        "        Arg(s):\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            float : polynomial decay to adjust (reduce) initial learning rate\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement polynomial decay to adjust the initial learning rate\n",
        "        if self.__eta_decay_factor is None:\n",
        "            return self.__alpha\n",
        "        else:\n",
        "            decay_factor = time_step ** (-self.__eta_decay_factor)\n",
        "            decayed_alpha = self.__alpha * decay_factor\n",
        "\n",
        "        return decayed_alpha\n",
        "\n",
        "    def update(self,\n",
        "               w,\n",
        "               x,\n",
        "               y,\n",
        "               loss_func,\n",
        "               batch_size,\n",
        "               time_step):\n",
        "        '''\n",
        "        Updates the weight vector based on\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, should be 'logistic' for the purpose of the assignment\n",
        "            batch_size : int\n",
        "                batch size for stochastic and momentum stochastic gradient descent\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            numpy[float32]: d x 1 weights\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement the optimizer update function\n",
        "        # For each optimizer type, compute gradients and update weights\n",
        "        eta = self.__polynomial_decay(time_step) # step size\n",
        "        gradient = self.__compute_gradients(w, x, y, loss_func)\n",
        "\n",
        "        if self.__optimizer_type == 'gradient_descent':\n",
        "            # print('gradient shape: {}'.format(gradient.shape)) # (30, 1)\n",
        "            # print('w shape: {}'.format(w.shape)) # (30, 1)\n",
        "            # print(\"Before update: w[0]:\", w[0])  \n",
        "            w = w - eta * gradient\n",
        "            # print(\"After update: w[0]:\", w[0])  \n",
        "            return w\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_gradient_descent':\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros(w.shape)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1-self.__beta) * gradient\n",
        "            w = w - eta * self.__momentum\n",
        "            return w\n",
        "\n",
        "        elif self.__optimizer_type == 'stochastic_gradient_descent':\n",
        "            indices = np.random.choice(x.shape[1], batch_size, replace=False)\n",
        "            x_batch = x[:, indices]\n",
        "            y_batch = y[:, indices]\n",
        "            gradient = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            w = w - eta * gradient\n",
        "            return w\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_stochastic_gradient_descent':\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros(w.shape)\n",
        "            indices = np.random.choice(x.shape[1], batch_size, replace=False)\n",
        "            x_batch = x[:, indices]\n",
        "            y_batch = y[:, indices]\n",
        "            gradient = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1-self.__beta) * gradient\n",
        "            w = w - eta * self.__momentum\n",
        "            return w\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported optimizer type: {}'.format(self.__optimizer_type))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRT2kC4GqAp_"
      },
      "source": [
        "Implementation of our logistic regression model for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vOaTyJ5VqBYt"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Define private variables\n",
        "        self.__weights = None\n",
        "        self.__optimizer = None\n",
        "\n",
        "    def fit(self,\n",
        "            x,\n",
        "            y,\n",
        "            T,\n",
        "            alpha,\n",
        "            eta_decay_factor,\n",
        "            beta,\n",
        "            batch_size,\n",
        "            optimizer_type,\n",
        "            loss_func='logistic'):\n",
        "        '''\n",
        "        Fits the model to x and y by updating the weight vector\n",
        "        using gradient descent\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            T : int\n",
        "                number of iterations to train\n",
        "            alpha : float\n",
        "                learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            batch_size : int\n",
        "                number of examples per batch\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        '''\n",
        "\n",
        "        # DONE: Instantiate optimizer and weights\n",
        "        self.__optimizer = Optimizer(alpha, eta_decay_factor, beta, optimizer_type)\n",
        "        self.__weights = np.random.randn(x.shape[0], 1) * 0.01 \n",
        "\n",
        "        for t in range(1, T + 1):\n",
        "\n",
        "            # DONE: Compute loss function\n",
        "            loss = self.__compute_loss(x, y, loss_func)\n",
        "\n",
        "\n",
        "            if (t % 1000) == 0:\n",
        "                print('Step={}  Loss={}'.format(t, loss))\n",
        "\n",
        "            # DONE: Update weights\n",
        "            self.__weights = self.__optimizer.update(self.__weights, x, y, loss_func, batch_size, t)\n",
        "            # print(\"loss\", loss, \"weights\", self.__weights)\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predicts the label for each feature vector x\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : 1 x N vector\n",
        "        '''\n",
        "\n",
        "        # DONE: Implements the predict function\n",
        "        # Hint: logistic regression predicts a value between 0 and 1\n",
        "        z = np.dot(self.__weights.T, x)\n",
        "        sigmoid = 1 / (1+ np.exp(-z))\n",
        "        # print(sigmoid.shape)\n",
        "        predictions = np.where(sigmoid >= 0.5, 1, -1)\n",
        "        # print(predictions)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def __compute_loss(self, x, y, loss_func):\n",
        "        '''\n",
        "        Computes the logistic loss\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            float : loss\n",
        "        '''\n",
        "\n",
        "        # DONE: Implements the __compute_loss function\n",
        "        if loss_func == 'logistic':\n",
        "            \n",
        "            N = x.shape[1] # number of samples, 341\n",
        "            z = np.dot(self.__weights.T, x)\n",
        "            loss = np.sum(np.log(1 + np.exp(-y * z))) / N\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported loss function: {}'.format(loss_func))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zusvzb2xJJzi"
      },
      "source": [
        "Training, validating and testing logistic regression for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_scores(score_train, score_val, score_test, sk_score_train, sk_score_val, sk_score_test):\n",
        "    \"\"\"\n",
        "    Compare the performance of our logistic regression model with scikit-learn model.\n",
        "    If the performance is more than 8% worse for any dataset (train/validation/test),\n",
        "    print 'Rerun' because I don't want to assess this by hand.\n",
        "    \"\"\"\n",
        "    threshold = 0.08 \n",
        "\n",
        "    if score_train < sk_score_train * (1 - threshold):\n",
        "        print(f\"\\nTraining score is more than 8% worse. Ours: {score_train:.4f}, Scikit: {sk_score_train:.4f}. Rerun.\")\n",
        "\n",
        "    if score_val < sk_score_val * (1 - threshold):\n",
        "        print(f\"\\nValidation score is more than 8% worse. Ours: {score_val:.4f}, Scikit: {sk_score_val:.4f}. Rerun.\")\n",
        "\n",
        "    if score_test < sk_score_test * (1 - threshold):\n",
        "        print(f\"\\nTest score is more than 8% worse. Ours: {score_test:.4f}, Scikit: {sk_score_test:.4f}. Rerun.\")\n",
        "        \n",
        "    if (score_train >= sk_score_train * (1 - threshold)) and \\\n",
        "       (score_val >= sk_score_val * (1 - threshold)) and \\\n",
        "       (score_test >= sk_score_test * (1 - threshold)):\n",
        "        print(\"\\nPerformance is acceptable.\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EI7TMva6JIh8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing the breast cancer dataset (569 samples, 30 feature dimensions)\n",
            "***** Results on the breast cancer dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9531\n",
            "Validation set mean accuracy: 0.9649\n",
            "Testing set mean accuracy: 0.9737\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=1e-06 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=10000\n",
            "Step=1000  Loss=0.4270500224685998\n",
            "Step=2000  Loss=0.36478850477205127\n",
            "Step=3000  Loss=0.33337323820259707\n",
            "Step=4000  Loss=0.31303327228726313\n",
            "Step=5000  Loss=0.29829359870313954\n",
            "Step=6000  Loss=0.2869407988965641\n",
            "Step=7000  Loss=0.27785147000222793\n",
            "Step=8000  Loss=0.2703727118147251\n",
            "Step=9000  Loss=0.2640914146974563\n",
            "Step=10000  Loss=0.2587304473641291\n",
            "Total training time: 0.271522 seconds\n",
            "Training set mean accuracy: 0.9179\n",
            "Validation set mean accuracy: 0.9211\n",
            "Testing set mean accuracy: 0.9035\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=1e-06 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=10000\n",
            "Step=1000  Loss=0.41872181102313405\n",
            "Step=2000  Loss=0.35935853700887027\n",
            "Step=3000  Loss=0.32884081942389604\n",
            "Step=4000  Loss=0.30888403977348616\n",
            "Step=5000  Loss=0.29436364226160494\n",
            "Step=6000  Loss=0.2831676593069645\n",
            "Step=7000  Loss=0.27420674441552617\n",
            "Step=8000  Loss=0.2668408571383118\n",
            "Step=9000  Loss=0.2606622277298364\n",
            "Step=10000  Loss=0.2553961402822704\n",
            "Total training time: 0.286938 seconds\n",
            "Training set mean accuracy: 0.9238\n",
            "Validation set mean accuracy: 0.9298\n",
            "Testing set mean accuracy: 0.9123\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.001 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=None \n",
            "\t batch_size=24 \n",
            "\t T=10000\n",
            "Step=1000  Loss=0.35844075283384147\n",
            "Step=2000  Loss=0.33944414095967484\n",
            "Step=3000  Loss=0.3291189775275483\n",
            "Step=4000  Loss=0.3210265725550344\n",
            "Step=5000  Loss=0.3159328870095344\n",
            "Step=6000  Loss=0.3114689010270899\n",
            "Step=7000  Loss=0.30789855591945703\n",
            "Step=8000  Loss=0.3048316818599375\n",
            "Step=9000  Loss=0.30236655211556535\n",
            "Step=10000  Loss=0.2998336482716568\n",
            "Total training time: 0.485271 seconds\n",
            "Training set mean accuracy: 0.9120\n",
            "Validation set mean accuracy: 0.9123\n",
            "Testing set mean accuracy: 0.9035\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=0.9 \n",
            "\t batch_size=24 \n",
            "\t T=10000\n",
            "Step=1000  Loss=0.5771194328230421\n",
            "Step=2000  Loss=0.5311636065713808\n",
            "Step=3000  Loss=0.5100023180942997\n",
            "Step=4000  Loss=0.49107259383223995\n",
            "Step=5000  Loss=0.4790405515433979\n",
            "Step=6000  Loss=0.47106448042941645\n",
            "Step=7000  Loss=0.462635166864956\n",
            "Step=8000  Loss=0.4574672677518454\n",
            "Step=9000  Loss=0.45192069558004394\n",
            "Step=10000  Loss=0.45084615783322285\n",
            "Total training time: 0.507263 seconds\n",
            "Training set mean accuracy: 0.9150\n",
            "Validation set mean accuracy: 0.9211\n",
            "Testing set mean accuracy: 0.9298\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "\n",
            "Preprocessing the digits greater or less than 5 dataset (1797 samples, 64 feature dimensions)\n",
            "***** Results on the digits greater or less than 5 dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9156\n",
            "Validation set mean accuracy: 0.9025\n",
            "Testing set mean accuracy: 0.8861\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=10000\n",
            "Step=1000  Loss=0.4281522490299727\n",
            "Step=2000  Loss=0.37205103141668566\n",
            "Step=3000  Loss=0.34492371452292153\n",
            "Step=4000  Loss=0.3281520777210791\n",
            "Step=5000  Loss=0.3164679248980921\n",
            "Step=6000  Loss=0.3077450612352203\n",
            "Step=7000  Loss=0.30093628951788\n",
            "Step=8000  Loss=0.2954550770228764\n",
            "Step=9000  Loss=0.29094187269077343\n",
            "Step=10000  Loss=0.2871610619588243\n",
            "Total training time: 0.544074 seconds\n",
            "Training set mean accuracy: 0.8998\n",
            "Validation set mean accuracy: 0.9109\n",
            "Testing set mean accuracy: 0.8833\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=10000\n",
            "Step=1000  Loss=0.43702451358572564\n",
            "Step=2000  Loss=0.3745567724902582\n",
            "Step=3000  Loss=0.34592803346712225\n",
            "Step=4000  Loss=0.32862740201924195\n",
            "Step=5000  Loss=0.3167151784655617\n",
            "Step=6000  Loss=0.30788059471122886\n",
            "Step=7000  Loss=0.3010117679133514\n",
            "Step=8000  Loss=0.2954958828339183\n",
            "Step=9000  Loss=0.290961579330691\n",
            "Step=10000  Loss=0.28716751033122145\n",
            "Total training time: 0.561097 seconds\n",
            "Training set mean accuracy: 0.8989\n",
            "Validation set mean accuracy: 0.9136\n",
            "Testing set mean accuracy: 0.8833\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.95 \n",
            "\t beta=None \n",
            "\t batch_size=32 \n",
            "\t T=10000\n",
            "Step=1000  Loss=0.43149918261962306\n",
            "Step=2000  Loss=0.4218202712766845\n",
            "Step=3000  Loss=0.41655529131456714\n",
            "Step=4000  Loss=0.41308527014643365\n",
            "Step=5000  Loss=0.4104401581730333\n",
            "Step=6000  Loss=0.4083406886041121\n",
            "Step=7000  Loss=0.4065811947018845\n",
            "Step=8000  Loss=0.4051253344221095\n",
            "Step=9000  Loss=0.40383349072613606\n",
            "Step=10000  Loss=0.40270246290162337\n",
            "Total training time: 0.889526 seconds\n",
            "Training set mean accuracy: 0.8692\n",
            "Validation set mean accuracy: 0.8858\n",
            "Testing set mean accuracy: 0.8667\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.1 \n",
            "\t eta_decay_factor=0.95 \n",
            "\t beta=0.9 \n",
            "\t batch_size=32 \n",
            "\t T=10000\n",
            "Step=1000  Loss=0.29918906216155605\n",
            "Step=2000  Loss=0.29374963641814233\n",
            "Step=3000  Loss=0.291401847229874\n",
            "Step=4000  Loss=0.28942154658319824\n",
            "Step=5000  Loss=0.2881304060865366\n",
            "Step=6000  Loss=0.28722889678808117\n",
            "Step=7000  Loss=0.2862749165553029\n",
            "Step=8000  Loss=0.28569153567648764\n",
            "Step=9000  Loss=0.2849962753418624\n",
            "Step=10000  Loss=0.2845142622856062\n",
            "Total training time: 0.899369 seconds\n",
            "Training set mean accuracy: 0.9017\n",
            "Validation set mean accuracy: 0.9136\n",
            "Testing set mean accuracy: 0.8861\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "\n",
            "Preprocessing the fir and pine coverage dataset (495141 samples, 54 feature dimensions)\n",
            "***** Results on the fir and pine coverage dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.7564\n",
            "Validation set mean accuracy: 0.7572\n",
            "Testing set mean accuracy: 0.7571\n",
            "***** Results of our logistic regression model trained on fir and pine coverage dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.001 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=None \n",
            "\t batch_size=1000 \n",
            "\t T=3000\n",
            "Step=1000  Loss=1.619466284067763\n",
            "Step=2000  Loss=0.6060110305533307\n",
            "Step=3000  Loss=0.5872086299295305\n",
            "Total training time: 164.024928 seconds\n",
            "Training set mean accuracy: 0.7109\n",
            "Validation set mean accuracy: 0.7107\n",
            "Testing set mean accuracy: 0.7119\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "***** Results of our logistic regression model trained on fir and pine coverage dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=0.9 \n",
            "\t batch_size=1000 \n",
            "\t T=3000\n",
            "Step=1000  Loss=0.9585612130481179\n",
            "Step=2000  Loss=0.8596895839366846\n",
            "Step=3000  Loss=0.801505433007527\n",
            "Total training time: 163.885973 seconds\n",
            "Training set mean accuracy: 0.7180\n",
            "Validation set mean accuracy: 0.7189\n",
            "Testing set mean accuracy: 0.7197\n",
            "\n",
            "Performance is acceptable.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load breast cancer, digits, and tree coverage datasets\n",
        "datasets = [\n",
        "    skdata.load_breast_cancer(),\n",
        "    skdata.load_digits(),\n",
        "    skdata.fetch_covtype()\n",
        "]\n",
        "dataset_names = [\n",
        "    'breast cancer',\n",
        "    'digits greater or less than 5',\n",
        "    'fir and pine coverage',\n",
        "]\n",
        "\n",
        "# Loss functions to minimize\n",
        "dataset_optimizer_types = [\n",
        "    # For breast cancer dataset\n",
        "    [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    ], [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For fir and pine coverage dataset\n",
        "    ], [\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    ]\n",
        "]\n",
        "\n",
        "# DONE: Select hyperparameters\n",
        "\n",
        "# Step size (always used)\n",
        "dataset_alphas = [\n",
        "    # For breast cancer dataset\n",
        "    [1e-6, 1e-6, 0.001, 0.01],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [1e-4, 1e-4, 0.01, 0.1],\n",
        "    # For fir and pine coverage dataset\n",
        "    [0.001, 0.01]\n",
        "]\n",
        "\n",
        "# How much the learning rate should decrease over time (only for SGD)\n",
        "dataset_eta_decay_factors = [\n",
        "    # For breast cancer dataset\n",
        "    [None, None, 0.99, 0.99],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, None, 0.95, 0.95],\n",
        "    # For fir and pine coverage dataset\n",
        "    [0.99, 0.99]\n",
        "]\n",
        "\n",
        "# Momentum (only for momentum-based ..duh)\n",
        "# None for no momentum\n",
        "dataset_betas = [\n",
        "    # For breast cancer dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For fir and pine coverage dataset\n",
        "    [None, 0.9]\n",
        "]\n",
        "\n",
        "# Samples (only for SGD)\n",
        "dataset_batch_sizes = [\n",
        "    # For breast cancer dataset\n",
        "    # # N = 569\n",
        "    [None, None, 24, 24],\n",
        "    # For digits greater than or less than 5 dataset \n",
        "    # # N = 1797\n",
        "    [None, None, 32, 32],\n",
        "    # For fir and pine coverage dataset \n",
        "    # # N = 495141\n",
        "    [1000, 1000]\n",
        "]\n",
        "\n",
        "\n",
        "# Iterations\n",
        "dataset_Ts = [\n",
        "    # For breast cancer dataset\n",
        "    [10000, 10000, 10000, 10000],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [10000, 10000, 10000, 10000],\n",
        "    # For fir and pine coverage dataset\n",
        "    [3000, 3000]\n",
        "]\n",
        "\n",
        "# Zip up all dataset options\n",
        "dataset_options = zip(\n",
        "    datasets,\n",
        "    dataset_names,\n",
        "    dataset_optimizer_types,\n",
        "    dataset_alphas,\n",
        "    dataset_eta_decay_factors,\n",
        "    dataset_betas,\n",
        "    dataset_batch_sizes,\n",
        "    dataset_Ts)\n",
        "\n",
        "\n",
        "for options in dataset_options:\n",
        "\n",
        "    # Unpack dataset options\n",
        "    dataset, \\\n",
        "        dataset_name, \\\n",
        "        optimizer_types, \\\n",
        "        alphas, \\\n",
        "        eta_decay_factors, \\\n",
        "        betas, \\\n",
        "        batch_sizes, \\\n",
        "        Ts = options\n",
        "\n",
        "    '''\n",
        "    Create the training, validation and testing splits\n",
        "    '''\n",
        "    x = dataset.data\n",
        "    y = dataset.target\n",
        "\n",
        "    if dataset_name == 'digits greater or less than 5':\n",
        "        y[y < 5] = 1\n",
        "        y[y >= 5] = 0\n",
        "    elif dataset_name == 'fir and pine coverage':\n",
        "\n",
        "        idx_fir_or_pine = np.where(np.logical_or(y == 1, y == 2))[0]\n",
        "\n",
        "        x = x[idx_fir_or_pine, :]\n",
        "        y = y[idx_fir_or_pine]\n",
        "\n",
        "        # Pine class: 0; Fir class: 1\n",
        "        y[y == 2] = 0\n",
        "\n",
        "    print('Preprocessing the {} dataset ({} samples, {} feature dimensions)'.format(dataset_name, x.shape[0], x.shape[1]))\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(x.shape[0])\n",
        "\n",
        "    # Choose the first 60% as training set, next 20% as validation and the rest as testing\n",
        "    train_split_idx = int(0.60 * x.shape[0])\n",
        "    val_split_idx = int(0.80 * x.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:val_split_idx]\n",
        "    test_indices = shuffled_indices[val_split_idx:]\n",
        "\n",
        "    # Select the examples from x and y to construct our training, validation, testing sets\n",
        "    x_train, y_train = x[train_indices, :], y[train_indices]\n",
        "    x_val, y_val = x[val_indices, :], y[val_indices]\n",
        "    x_test, y_test = x[test_indices, :], y[test_indices]\n",
        "\n",
        "    '''\n",
        "    Trains and tests logistic regression model from scikit-learn\n",
        "    '''\n",
        "    model_scikit = LogisticRegressionSciKit(penalty=None, fit_intercept=False)\n",
        "\n",
        "    # DONE: Train scikit-learn logistic regression model\n",
        "    model_scikit.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "    print('***** Results on the {} dataset using scikit-learn logistic regression model *****'.format(dataset_name))\n",
        "\n",
        "    # DONE: Score model using mean accuracy on training set\n",
        "    predictions_train = model_scikit.predict(x_train)\n",
        "    sk_score_train = skmetrics.accuracy_score(y_train, predictions_train)\n",
        "    print('Training set mean accuracy: {:.4f}'.format(sk_score_train))\n",
        "\n",
        "    # DONE: Score model using mean accuracy on validation set\n",
        "    predictions_val = model_scikit.predict(x_val)\n",
        "    sk_score_val = skmetrics.accuracy_score(y_val, predictions_val)\n",
        "    print('Validation set mean accuracy: {:.4f}'.format(sk_score_val))\n",
        "\n",
        "    # DONE: Score model using mean accuracy on testing set\n",
        "    predictions_test = model_scikit.predict(x_test)\n",
        "    sk_score_test = skmetrics.accuracy_score(y_test, predictions_test)\n",
        "    print('Testing set mean accuracy: {:.4f}'.format(sk_score_test))\n",
        "\n",
        "    '''\n",
        "    Trains, validates, and tests our logistic regression model for binary classification\n",
        "    '''\n",
        "    # Take the transpose of the dataset to match the dimensions discussed in lecture\n",
        "    # i.e., (N x d) to (d x N)\n",
        "    x_train = np.transpose(x_train, axes=(1, 0))\n",
        "    x_val = np.transpose(x_val, axes=(1, 0))\n",
        "    x_test = np.transpose(x_test, axes=(1, 0))\n",
        "    y_train = np.expand_dims(y_train, axis=0)\n",
        "    y_val = np.expand_dims(y_val, axis=0)\n",
        "    y_test = np.expand_dims(y_test, axis=0)\n",
        "\n",
        "    # DONE: Set the ground truth to the appropriate classes (integers) according to lecture\n",
        "    # -1 and +1\n",
        "    y_train = np.where(y_train == 0, -1, 1)\n",
        "    y_val = np.where(y_val == 0, -1, 1)\n",
        "    y_test = np.where(y_test == 0, -1, 1)\n",
        "\n",
        "    model_options = zip(optimizer_types, alphas, eta_decay_factors, betas, batch_sizes, Ts)\n",
        "\n",
        "    for optimizer_type, alpha, eta_decay_factor, beta, batch_size, T in model_options:\n",
        "\n",
        "        # DONE: Initialize our logistic regression model\n",
        "        model_ours = LogisticRegression()\n",
        "\n",
        "        print('***** Results of our logistic regression model trained on {} dataset *****'.format(dataset_name))\n",
        "        print('\\t optimizer_type={} \\n\\t alpha={} \\n\\t eta_decay_factor={} \\n\\t beta={} \\n\\t batch_size={} \\n\\t T={}'.format(\n",
        "            optimizer_type, alpha, eta_decay_factor, beta, batch_size, T))\n",
        "\n",
        "        time_start = time.time()\n",
        "\n",
        "        # DONE: Train model on training set\n",
        "        model_ours.fit(\n",
        "            x_train,\n",
        "            y_train,\n",
        "            T,\n",
        "            alpha,\n",
        "            eta_decay_factor,\n",
        "            beta,\n",
        "            batch_size,\n",
        "            optimizer_type\n",
        "        )\n",
        "\n",
        "        time_elapsed = time.time() - time_start\n",
        "        print('Total training time: {:3f} seconds'.format(time_elapsed))\n",
        "\n",
        "        # DONE: Score model using mean accuracy on training set\n",
        "        predictions_train = model_ours.predict(x_train)\n",
        "        score_train = np.mean(predictions_train == y_train)\n",
        "        # print(\"Predictions\", predictions_train)\n",
        "        # print(\"Ground truth\", y_train)\n",
        "        print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "        # DONE: Score model using mean accuracy on training set\n",
        "        predictions_val = model_ours.predict(x_val)\n",
        "        score_val = np.mean(predictions_val == y_val)\n",
        "        print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "        # DONE: Score model using mean accuracy on training set\n",
        "        predictions_test = model_ours.predict(x_test)\n",
        "        score_test = np.mean(predictions_test == y_test)\n",
        "        print('Testing set mean accuracy: {:.4f}'.format(score_test))\n",
        "\n",
        "        compare_scores(\n",
        "            score_train, score_val, score_test,\n",
        "            sk_score_train, sk_score_val, sk_score_test\n",
        "        )\n",
        "\n",
        "    print('')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
