{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r37l-FPc9o0h"
      },
      "source": [
        "**Assignment 2: Stochastic Gradient Descent and Momentum**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "*Student: Hailey Robertson*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "\n",
        "4. Move the 02_assignment_stochastic_gradient_descent.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments/02_assignment_stochastic_gradient_descent.ipynb\n",
        "```\n",
        "\n",
        "In this assignment, we will optimize a linear function for the logistic regression task using the stochastic gradient descent and its momentum variant. We will test them on several binary classification datasets (breast cancer, digits larger or less than 5, and fir and pine coverage). We will implement a training and validation loop for the binary classification task and test it on the testing split for each dataset.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training, validation, and testing scores.\n",
        "\n",
        "```\n",
        "Report training, validation, and testing scores here.\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: N/A\n",
        "```\n",
        "\n",
        "\n",
        "**IMPORTANT**:\n",
        "\n",
        "- For full credit, your mean classification accuracies for all trained models across all datasets should be no more than 8% worse the scores achieved by sci-kit learn's logistic regression model across training, validation and testing splits.\n",
        "\n",
        "- You may not use batch sizes of more than 10% of the dataset size for stochastic gradient descent and momentum stochastic gradient descent.\n",
        "\n",
        "- You will only need to experiment with gradient descent (GD) and momentum gradient descent (momentum GD) on breast cancer and digits (toy) datasets. It will take too long to run them on fir and pine coverage (realistic) dataset to get reasonable numbers. Of course, you may try them on fir and pine coverage :) but they will not count towards your grade.\n",
        "\n",
        "- Note the run time speed up when comparing GD and momemtum GD with stochastic gradient descent (SGD) and momentum stochastic gradient descent (momentum SGD)! Even though they are faster and observing batches instead of the full dataset at each time step, they can still achieving similar accuracies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "koDraeo69YZH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn.linear_model import LogisticRegression as LogisticRegressionSciKit\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAR4bm26XZiJ"
      },
      "source": [
        "Implementation of stochastic gradient descent optimizer for logistic loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bI62IQ3d9SpW"
      },
      "outputs": [],
      "source": [
        "class Optimizer(object):\n",
        "\n",
        "    def __init__(self, alpha, eta_decay_factor, beta, optimizer_type):\n",
        "        '''\n",
        "        Arg(s):\n",
        "            alpha : float\n",
        "                initial learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "        '''\n",
        "\n",
        "        self.__alpha = alpha\n",
        "        self.__eta_decay_factor = eta_decay_factor\n",
        "        self.__beta = beta\n",
        "        self.__optimizer_type = optimizer_type\n",
        "        self.__momentum = None\n",
        "\n",
        "    def __compute_gradients(self, w, x, y, loss_func='logistic'):\n",
        "        '''\n",
        "        Returns the gradient of a loss function\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            numpy[float32] : d x 1 gradients\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement compute_gradient function\n",
        "        # NOTES:\n",
        "            # sigmoid = sigma(z) = 1/(1+exp(-z))\n",
        "            # loss = -1/N * sum(y*log(sigma(z)) + (1-y)*log(1-sigma(z)))\n",
        "            # gradient = 1/N * sum(x*(sigma(z) - y))\n",
        "\n",
        "        if loss_func == 'logistic':\n",
        "\n",
        "            N = x.shape[1]\n",
        "            z = np.dot(w.T, x)\n",
        "            sigmoid = 1/(1+ np.exp(-z)) # probabilities for each sample\n",
        "            gradient = np.dot(x, (sigmoid-y).T)/N\n",
        "            # gradient = np.dot(x, ((sigmoid - 1) * y).T)/N\n",
        "            # print(gradient.shape) # GOOD (30x1)\n",
        "\n",
        "            return gradient\n",
        "        \n",
        "        else:\n",
        "            raise ValueError('Unupported loss function: {}'.format(loss_func))\n",
        "\n",
        "    def __polynomial_decay(self, time_step):\n",
        "        '''\n",
        "        Computes the polynomial decay factor t^{-a}\n",
        "\n",
        "        Arg(s):\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            float : polynomial decay to adjust (reduce) initial learning rate\n",
        "        '''\n",
        "\n",
        "        # DONE?: Implement polynomial decay to adjust the initial learning rate\n",
        "        if self.__eta_decay_factor is None:\n",
        "            # print(self.__alpha)\n",
        "            return self.__alpha\n",
        "        else:\n",
        "            decay_factor = time_step ** (-self.__eta_decay_factor)\n",
        "            decayed_alpha = self.__alpha * decay_factor\n",
        "            # print(decayed_alpha)\n",
        "\n",
        "        return decayed_alpha\n",
        "\n",
        "    def update(self,\n",
        "               w,\n",
        "               x,\n",
        "               y,\n",
        "               loss_func,\n",
        "               batch_size,\n",
        "               time_step):\n",
        "        '''\n",
        "        Updates the weight vector based on\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, should be 'logistic' for the purpose of the assignment\n",
        "            batch_size : int\n",
        "                batch size for stochastic and momentum stochastic gradient descent\n",
        "            time_step : int\n",
        "                current step in optimization\n",
        "        Returns:\n",
        "            numpy[float32]: d x 1 weights\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement the optimizer update function\n",
        "        # For each optimizer type, compute gradients and update weights\n",
        "        eta = self.__polynomial_decay(time_step)\n",
        "        gradient = self.__compute_gradients(w, x, y, loss_func)\n",
        "        # print(\"w: \", w.shape)\n",
        "        # print(\"x: \", x.shape)\n",
        "        # print(\"y: \", y.shape)\n",
        "        # print(\"gradient: \", gradient.shape)\n",
        "        # all good\n",
        "\n",
        "        if self.__optimizer_type == 'gradient_descent':\n",
        "            w = w - eta * gradient\n",
        "            return w\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_gradient_descent':\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros(w.shape)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1-self.__beta) * gradient\n",
        "            w = w - eta * self.__momentum\n",
        "            return w\n",
        "\n",
        "        elif self.__optimizer_type == 'stochastic_gradient_descent':\n",
        "            indices = np.random.choice(x.shape[1], batch_size, replace=False)\n",
        "            x_batch = x[:, indices]\n",
        "            y_batch = y[:, indices]\n",
        "            gradient = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            w = w - eta * gradient\n",
        "            return w\n",
        "\n",
        "        elif self.__optimizer_type == 'momentum_stochastic_gradient_descent':\n",
        "            if self.__momentum is None:\n",
        "                self.__momentum = np.zeros(w.shape)\n",
        "            indices = np.random.choice(x.shape[1], batch_size, replace=False)\n",
        "            x_batch = x[:, indices]\n",
        "            y_batch = y[:, indices]\n",
        "            gradient = self.__compute_gradients(w, x_batch, y_batch, loss_func)\n",
        "            self.__momentum = self.__beta * self.__momentum + (1-self.__beta) * gradient\n",
        "            w = w - eta * self.__momentum\n",
        "            return w\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported optimizer type: {}'.format(self.__optimizer_type))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRT2kC4GqAp_"
      },
      "source": [
        "Implementation of our logistic regression model for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOaTyJ5VqBYt"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Define private variables\n",
        "        self.__weights = None\n",
        "        self.__optimizer = None\n",
        "\n",
        "    def fit(self,\n",
        "            x,\n",
        "            y,\n",
        "            T,\n",
        "            alpha,\n",
        "            eta_decay_factor,\n",
        "            beta,\n",
        "            batch_size,\n",
        "            optimizer_type,\n",
        "            loss_func='logistic'):\n",
        "        '''\n",
        "        Fits the model to x and y by updating the weight vector\n",
        "        using gradient descent\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            T : int\n",
        "                number of iterations to train\n",
        "            alpha : float\n",
        "                learning rate\n",
        "            eta_decay_factor : float\n",
        "                learning rate decay rate\n",
        "            beta : float\n",
        "                momentum discount rate\n",
        "            batch_size : int\n",
        "                number of examples per batch\n",
        "            optimizer_type : str\n",
        "                'gradient_descent',\n",
        "                'momentum_gradient_descent',\n",
        "                'stochastic_gradient_descent',\n",
        "                'momentum_stochastic_gradient_descent'\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        '''\n",
        "\n",
        "        # DONE?: Instantiate optimizer and weights\n",
        "        self.__optimizer = Optimizer(alpha, eta_decay_factor, beta, optimizer_type)\n",
        "\n",
        "        # Come back to this\n",
        "        self.__weights = np.zeros((x.shape[0], 1)) * 0.01\n",
        "\n",
        "        # print(x.shape, y.shape) # Good\n",
        "\n",
        "        for t in range(1, T + 1):\n",
        "\n",
        "            # DONE: Compute loss function\n",
        "            loss = self.__compute_loss(x, y, loss_func)\n",
        "\n",
        "\n",
        "            if (t % 100) == 0:\n",
        "                print('Step={}  Loss={}'.format(t, loss))\n",
        "\n",
        "            # DONE?: Update weights\n",
        "            self.__weights = self.__optimizer.update(self.__weights, x, y, loss_func, batch_size, t)\n",
        "            # print(\"loss\", loss, \"weights\", self.__weights)\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predicts the label for each feature vector x\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : 1 x N vector\n",
        "        '''\n",
        "\n",
        "        # DONE: Implements the predict function\n",
        "        # Hint: logistic regression predicts a value between 0 and 1\n",
        "        z = np.dot(self.__weights.T, x)\n",
        "        # print(\"z: \", z)\n",
        "        sigmoid = 1/(1+ np.exp(-z))\n",
        "\n",
        "        return np.where(sigmoid >= 0.5, 1, -1)\n",
        "\n",
        "    def __compute_loss(self, x, y, loss_func):\n",
        "        '''\n",
        "        Computes the logistic loss\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss function to use, by default is 'logistic' only for the purpose of the assignment\n",
        "        Returns:\n",
        "            float : loss\n",
        "        '''\n",
        "\n",
        "        # DONE: Implements the __compute_loss function\n",
        "\n",
        "        if loss_func == 'logistic':\n",
        "            N = x.shape[1]\n",
        "            z = np.dot(self.__weights.T, x)\n",
        "            sigmoid = 1/(1+ np.exp(-z))\n",
        "            loss = -np.sum(y * np.log(sigmoid) + (1 - y) * np.log(1 - sigmoid)) / N\n",
        "            \n",
        "        else:\n",
        "            raise ValueError('Unsupported loss function: {}'.format(loss_func))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zusvzb2xJJzi"
      },
      "source": [
        "Training, validating and testing logistic regression for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EI7TMva6JIh8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing the breast cancer dataset (569 samples, 30 feature dimensions)\n",
            "***** Results on the breast cancer dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9501\n",
            "Validation set mean accuracy: 0.9474\n",
            "Testing set mean accuracy: 0.9298\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=1e-10 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=50000\n",
            "Step=100  Loss=0.6924017719103187\n",
            "Step=200  Loss=0.6916761189857523\n",
            "Step=300  Loss=0.6909775636250408\n",
            "Step=400  Loss=0.6903057798448563\n",
            "Step=500  Loss=0.6896604416010723\n",
            "Step=600  Loss=0.6890412229694822\n",
            "Step=700  Loss=0.6884477983218861\n",
            "Step=800  Loss=0.6878798424973587\n",
            "Step=900  Loss=0.687337030968531\n",
            "Step=1000  Loss=0.6868190400027376\n",
            "Step=1100  Loss=0.6863255468178998\n",
            "Step=1200  Loss=0.6858562297330374\n",
            "Step=1300  Loss=0.6854107683133189\n",
            "Step=1400  Loss=0.684988843509578\n",
            "Step=1500  Loss=0.6845901377922431\n",
            "Step=1600  Loss=0.6842143352796441\n",
            "Step=1700  Loss=0.6838611218606775\n",
            "Step=1800  Loss=0.6835301853118224\n",
            "Step=1900  Loss=0.6832212154085249\n",
            "Step=2000  Loss=0.6829339040309702\n",
            "Step=2100  Loss=0.6826679452642865\n",
            "Step=2200  Loss=0.6824230354932277\n",
            "Step=2300  Loss=0.6821988734914034\n",
            "Step=2400  Loss=0.6819951605051249\n",
            "Step=2500  Loss=0.681811600331956\n",
            "Step=2600  Loss=0.6816478993940596\n",
            "Step=2700  Loss=0.6815037668064415\n",
            "Step=2800  Loss=0.6813789144402018\n",
            "Step=2900  Loss=0.6812730569809086\n",
            "Step=3000  Loss=0.681185911982215\n",
            "Step=3100  Loss=0.6811171999148485\n",
            "Step=3200  Loss=0.6810666442111002\n",
            "Step=3300  Loss=0.6810339713049509\n",
            "Step=3400  Loss=0.6810189106679713\n",
            "Step=3500  Loss=0.6810211948411345\n",
            "Step=3600  Loss=0.6810405594626858\n",
            "Step=3700  Loss=0.6810767432922079\n",
            "Step=3800  Loss=0.6811294882310293\n",
            "Step=3900  Loss=0.6811985393391168\n",
            "Step=4000  Loss=0.6812836448485967\n",
            "Step=4100  Loss=0.6813845561740455\n",
            "Step=4200  Loss=0.6815010279196927\n",
            "Step=4300  Loss=0.6816328178836766\n",
            "Step=4400  Loss=0.6817796870594878\n",
            "Step=4500  Loss=0.6819413996347392\n",
            "Step=4600  Loss=0.6821177229873956\n",
            "Step=4700  Loss=0.6823084276795924\n",
            "Step=4800  Loss=0.6825132874491731\n",
            "Step=4900  Loss=0.6827320791990691\n",
            "Step=5000  Loss=0.6829645829846469\n",
            "Step=5100  Loss=0.6832105819991371\n",
            "Step=5200  Loss=0.6834698625572644\n",
            "Step=5300  Loss=0.683742214077189\n",
            "Step=5400  Loss=0.6840274290608673\n",
            "Step=5500  Loss=0.6843253030729379\n",
            "Step=5600  Loss=0.6846356347182345\n",
            "Step=5700  Loss=0.6849582256180209\n",
            "Step=5800  Loss=0.6852928803850465\n",
            "Step=5900  Loss=0.6856394065975081\n",
            "Step=6000  Loss=0.6859976147720087\n",
            "Step=6100  Loss=0.6863673183355948\n",
            "Step=6200  Loss=0.6867483335969525\n",
            "Step=6300  Loss=0.6871404797168387\n",
            "Step=6400  Loss=0.6875435786778217\n",
            "Step=6500  Loss=0.687957455253399\n",
            "Step=6600  Loss=0.6883819369765607\n",
            "Step=6700  Loss=0.6888168541078604\n",
            "Step=6800  Loss=0.6892620396030551\n",
            "Step=6900  Loss=0.6897173290803698\n",
            "Step=7000  Loss=0.6901825607874439\n",
            "Step=7100  Loss=0.6906575755680067\n",
            "Step=7200  Loss=0.6911422168283348\n",
            "Step=7300  Loss=0.6916363305035355\n",
            "Step=7400  Loss=0.6921397650236983\n",
            "Step=7500  Loss=0.6926523712799602\n",
            "Step=7600  Loss=0.6931740025905165\n",
            "Step=7700  Loss=0.6937045146666205\n",
            "Step=7800  Loss=0.6942437655786008\n",
            "Step=7900  Loss=0.6947916157219309\n",
            "Step=8000  Loss=0.6953479277833803\n",
            "Step=8100  Loss=0.6959125667072753\n",
            "Step=8200  Loss=0.6964853996618942\n",
            "Step=8300  Loss=0.6970662960060235\n",
            "Step=8400  Loss=0.6976551272556969\n",
            "Step=8500  Loss=0.6982517670511348\n",
            "Step=8600  Loss=0.69885609112391\n",
            "Step=8700  Loss=0.6994679772643516\n",
            "Step=8800  Loss=0.7000873052892067\n",
            "Step=8900  Loss=0.7007139570095745\n",
            "Step=9000  Loss=0.7013478161991252\n",
            "Step=9100  Loss=0.70198876856262\n",
            "Step=9200  Loss=0.702636701704738\n",
            "Step=9300  Loss=0.703291505099226\n",
            "Step=9400  Loss=0.703953070058378\n",
            "Step=9500  Loss=0.7046212897028521\n",
            "Step=9600  Loss=0.7052960589318331\n",
            "Step=9700  Loss=0.7059772743935494\n",
            "Step=9800  Loss=0.7066648344561453\n",
            "Step=9900  Loss=0.7073586391789188\n",
            "Step=10000  Loss=0.708058590283926\n",
            "Step=10100  Loss=0.7087645911279565\n",
            "Step=10200  Loss=0.7094765466748829\n",
            "Step=10300  Loss=0.7101943634683866\n",
            "Step=10400  Loss=0.7109179496050627\n",
            "Step=10500  Loss=0.7116472147079032\n",
            "Step=10600  Loss=0.7123820699001611\n",
            "Step=10700  Loss=0.7131224277795949\n",
            "Step=10800  Loss=0.7138682023930936\n",
            "Step=10900  Loss=0.7146193092116813\n",
            "Step=11000  Loss=0.7153756651059012\n",
            "Step=11100  Loss=0.7161371883215776\n",
            "Step=11200  Loss=0.7169037984559531\n",
            "Step=11300  Loss=0.7176754164342014\n",
            "Step=11400  Loss=0.7184519644863105\n",
            "Step=11500  Loss=0.7192333661243385\n",
            "Step=11600  Loss=0.7200195461200336\n",
            "Step=11700  Loss=0.7208104304828203\n",
            "Step=11800  Loss=0.7216059464381462\n",
            "Step=11900  Loss=0.7224060224061878\n",
            "Step=12000  Loss=0.7232105879809088\n",
            "Step=12100  Loss=0.7240195739094722\n",
            "Step=12200  Loss=0.7248329120719988\n",
            "Step=12300  Loss=0.7256505354616695\n",
            "Step=12400  Loss=0.7264723781651663\n",
            "Step=12500  Loss=0.7272983753434522\n",
            "Step=12600  Loss=0.7281284632128797\n",
            "Step=12700  Loss=0.7289625790266291\n",
            "Step=12800  Loss=0.7298006610564695\n",
            "Step=12900  Loss=0.7306426485748392\n",
            "Step=13000  Loss=0.7314884818372415\n",
            "Step=13100  Loss=0.7323381020649501\n",
            "Step=13200  Loss=0.733191451428022\n",
            "Step=13300  Loss=0.7340484730286118\n",
            "Step=13400  Loss=0.7349091108845832\n",
            "Step=13500  Loss=0.7357733099134143\n",
            "Step=13600  Loss=0.7366410159163906\n",
            "Step=13700  Loss=0.7375121755630839\n",
            "Step=13800  Loss=0.7383867363761091\n",
            "Step=13900  Loss=0.7392646467161579\n",
            "Step=14000  Loss=0.740145855767304\n",
            "Step=14100  Loss=0.7410303135225731\n",
            "Step=14200  Loss=0.7419179707697773\n",
            "Step=14300  Loss=0.7428087790776065\n",
            "Step=14400  Loss=0.7437026907819742\n",
            "Step=14500  Loss=0.7445996589726136\n",
            "Step=14600  Loss=0.7454996374799168\n",
            "Step=14700  Loss=0.7464025808620174\n",
            "Step=14800  Loss=0.7473084443921099\n",
            "Step=14900  Loss=0.748217184046001\n",
            "Step=15000  Loss=0.749128756489891\n",
            "Step=15100  Loss=0.7500431190683788\n",
            "Step=15200  Loss=0.7509602297926905\n",
            "Step=15300  Loss=0.7518800473291214\n",
            "Step=15400  Loss=0.7528025309876945\n",
            "Step=15500  Loss=0.753727640711026\n",
            "Step=15600  Loss=0.7546553370633979\n",
            "Step=15700  Loss=0.7555855812200318\n",
            "Step=15800  Loss=0.7565183349565615\n",
            "Step=15900  Loss=0.7574535606386995\n",
            "Step=16000  Loss=0.7583912212120942\n",
            "Step=16100  Loss=0.7593312801923762\n",
            "Step=16200  Loss=0.7602737016553873\n",
            "Step=16300  Loss=0.7612184502275914\n",
            "Step=16400  Loss=0.7621654910766601\n",
            "Step=16500  Loss=0.7631147899022356\n",
            "Step=16600  Loss=0.7640663129268624\n",
            "Step=16700  Loss=0.7650200268870867\n",
            "Step=16800  Loss=0.765975899024721\n",
            "Step=16900  Loss=0.7669338970782688\n",
            "Step=17000  Loss=0.767893989274509\n",
            "Step=17100  Loss=0.768856144320234\n",
            "Step=17200  Loss=0.7698203313941413\n",
            "Step=17300  Loss=0.7707865201388748\n",
            "Step=17400  Loss=0.7717546806532104\n",
            "Step=17500  Loss=0.772724783484389\n",
            "Step=17600  Loss=0.7736967996205868\n",
            "Step=17700  Loss=0.7746707004835275\n",
            "Step=17800  Loss=0.7756464579212269\n",
            "Step=17900  Loss=0.7766240442008737\n",
            "Step=18000  Loss=0.777603432001838\n",
            "Step=18100  Loss=0.77858459440881\n",
            "Step=18200  Loss=0.7795675049050638\n",
            "Step=18300  Loss=0.7805521373658436\n",
            "Step=18400  Loss=0.7815384660518713\n",
            "Step=18500  Loss=0.7825264656029732\n",
            "Step=18600  Loss=0.7835161110318214\n",
            "Step=18700  Loss=0.784507377717791\n",
            "Step=18800  Loss=0.7855002414009269\n",
            "Step=18900  Loss=0.7864946781760237\n",
            "Step=19000  Loss=0.7874906644868078\n",
            "Step=19100  Loss=0.7884881771202309\n",
            "Step=19200  Loss=0.7894871932008629\n",
            "Step=19300  Loss=0.7904876901853871\n",
            "Step=19400  Loss=0.7914896458571953\n",
            "Step=19500  Loss=0.7924930383210806\n",
            "Step=19600  Loss=0.7934978459980261\n",
            "Step=19700  Loss=0.7945040476200849\n",
            "Step=19800  Loss=0.7955116222253561\n",
            "Step=19900  Loss=0.7965205491530479\n",
            "Step=20000  Loss=0.7975308080386314\n",
            "Step=20100  Loss=0.7985423788090777\n",
            "Step=20200  Loss=0.7995552416781859\n",
            "Step=20300  Loss=0.8005693771419882\n",
            "Step=20400  Loss=0.801584765974242\n",
            "Step=20500  Loss=0.8026013892219981\n",
            "Step=20600  Loss=0.8036192282012516\n",
            "Step=20700  Loss=0.8046382644926678\n",
            "Step=20800  Loss=0.8056584799373826\n",
            "Step=20900  Loss=0.8066798566328817\n",
            "Step=21000  Loss=0.8077023769289472\n",
            "Step=21100  Loss=0.808726023423679\n",
            "Step=21200  Loss=0.8097507789595876\n",
            "Step=21300  Loss=0.8107766266197505\n",
            "Step=21400  Loss=0.8118035497240416\n",
            "Step=21500  Loss=0.8128315318254233\n",
            "Step=21600  Loss=0.8138605567063064\n",
            "Step=21700  Loss=0.8148906083749681\n",
            "Step=21800  Loss=0.8159216710620412\n",
            "Step=21900  Loss=0.816953729217057\n",
            "Step=22000  Loss=0.8179867675050504\n",
            "Step=22100  Loss=0.8190207708032252\n",
            "Step=22200  Loss=0.8200557241976787\n",
            "Step=22300  Loss=0.8210916129801774\n",
            "Step=22400  Loss=0.8221284226449944\n",
            "Step=22500  Loss=0.8231661388857983\n",
            "Step=22600  Loss=0.824204747592596\n",
            "Step=22700  Loss=0.8252442348487289\n",
            "Step=22800  Loss=0.8262845869279205\n",
            "Step=22900  Loss=0.8273257902913731\n",
            "Step=23000  Loss=0.8283678315849167\n",
            "Step=23100  Loss=0.8294106976362053\n",
            "Step=23200  Loss=0.8304543754519609\n",
            "Step=23300  Loss=0.8314988522152628\n",
            "Step=23400  Loss=0.8325441152828883\n",
            "Step=23500  Loss=0.8335901521826918\n",
            "Step=23600  Loss=0.8346369506110339\n",
            "Step=23700  Loss=0.83568449843025\n",
            "Step=23800  Loss=0.8367327836661635\n",
            "Step=23900  Loss=0.8377817945056406\n",
            "Step=24000  Loss=0.838831519294188\n",
            "Step=24100  Loss=0.8398819465335864\n",
            "Step=24200  Loss=0.840933064879568\n",
            "Step=24300  Loss=0.841984863139532\n",
            "Step=24400  Loss=0.8430373302702955\n",
            "Step=24500  Loss=0.8440904553758861\n",
            "Step=24600  Loss=0.8451442277053688\n",
            "Step=24700  Loss=0.8461986366507077\n",
            "Step=24800  Loss=0.8472536717446668\n",
            "Step=24900  Loss=0.8483093226587436\n",
            "Step=25000  Loss=0.8493655792011346\n",
            "Step=25100  Loss=0.8504224313147392\n",
            "Step=25200  Loss=0.8514798690751926\n",
            "Step=25300  Loss=0.8525378826889326\n",
            "Step=25400  Loss=0.8535964624912975\n",
            "Step=25500  Loss=0.854655598944656\n",
            "Step=25600  Loss=0.8557152826365675\n",
            "Step=25700  Loss=0.8567755042779708\n",
            "Step=25800  Loss=0.8578362547014057\n",
            "Step=25900  Loss=0.8588975248592587\n",
            "Step=26000  Loss=0.859959305822043\n",
            "Step=26100  Loss=0.8610215887767009\n",
            "Step=26200  Loss=0.8620843650249377\n",
            "Step=26300  Loss=0.863147625981579\n",
            "Step=26400  Loss=0.8642113631729588\n",
            "Step=26500  Loss=0.8652755682353293\n",
            "Step=26600  Loss=0.8663402329132976\n",
            "Step=26700  Loss=0.8674053490582901\n",
            "Step=26800  Loss=0.8684709086270376\n",
            "Step=26900  Loss=0.8695369036800861\n",
            "Step=27000  Loss=0.8706033263803324\n",
            "Step=27100  Loss=0.8716701689915813\n",
            "Step=27200  Loss=0.8727374238771272\n",
            "Step=27300  Loss=0.8738050834983572\n",
            "Step=27400  Loss=0.8748731404133759\n",
            "Step=27500  Loss=0.8759415872756542\n",
            "Step=27600  Loss=0.8770104168326968\n",
            "Step=27700  Loss=0.8780796219247312\n",
            "Step=27800  Loss=0.8791491954834192\n",
            "Step=27900  Loss=0.8802191305305865\n",
            "Step=28000  Loss=0.8812894201769723\n",
            "Step=28100  Loss=0.8823600576210006\n",
            "Step=28200  Loss=0.8834310361475675\n",
            "Step=28300  Loss=0.8845023491268502\n",
            "Step=28400  Loss=0.8855739900131323\n",
            "Step=28500  Loss=0.8866459523436497\n",
            "Step=28600  Loss=0.8877182297374522\n",
            "Step=28700  Loss=0.8887908158942827\n",
            "Step=28800  Loss=0.8898637045934761\n",
            "Step=28900  Loss=0.8909368896928729\n",
            "Step=29000  Loss=0.8920103651277487\n",
            "Step=29100  Loss=0.8930841249097647\n",
            "Step=29200  Loss=0.8941581631259294\n",
            "Step=29300  Loss=0.8952324739375774\n",
            "Step=29400  Loss=0.8963070515793651\n",
            "Step=29500  Loss=0.8973818903582808\n",
            "Step=29600  Loss=0.8984569846526695\n",
            "Step=29700  Loss=0.8995323289112743\n",
            "Step=29800  Loss=0.9006079176522909\n",
            "Step=29900  Loss=0.9016837454624339\n",
            "Step=30000  Loss=0.9027598069960235\n",
            "Step=30100  Loss=0.90383609697408\n",
            "Step=30200  Loss=0.9049126101834347\n",
            "Step=30300  Loss=0.9059893414758539\n",
            "Step=30400  Loss=0.907066285767176\n",
            "Step=30500  Loss=0.9081434380364606\n",
            "Step=30600  Loss=0.9092207933251514\n",
            "Step=30700  Loss=0.9102983467362514\n",
            "Step=30800  Loss=0.91137609343351\n",
            "Step=30900  Loss=0.9124540286406226\n",
            "Step=31000  Loss=0.9135321476404417\n",
            "Step=31100  Loss=0.9146104457741983\n",
            "Step=31200  Loss=0.9156889184407401\n",
            "Step=31300  Loss=0.9167675610957731\n",
            "Step=31400  Loss=0.9178463692511204\n",
            "Step=31500  Loss=0.9189253384739895\n",
            "Step=31600  Loss=0.9200044643862514\n",
            "Step=31700  Loss=0.9210837426637292\n",
            "Step=31800  Loss=0.9221631690354963\n",
            "Step=31900  Loss=0.9232427392831897\n",
            "Step=32000  Loss=0.9243224492403238\n",
            "Step=32100  Loss=0.9254022947916256\n",
            "Step=32200  Loss=0.9264822718723715\n",
            "Step=32300  Loss=0.9275623764677364\n",
            "Step=32400  Loss=0.9286426046121524\n",
            "Step=32500  Loss=0.9297229523886761\n",
            "Step=32600  Loss=0.9308034159283654\n",
            "Step=32700  Loss=0.9318839914096648\n",
            "Step=32800  Loss=0.9329646750578016\n",
            "Step=32900  Loss=0.9340454631441863\n",
            "Step=33000  Loss=0.9351263519858273\n",
            "Step=33100  Loss=0.9362073379447474\n",
            "Step=33200  Loss=0.9372884174274169\n",
            "Step=33300  Loss=0.938369586884186\n",
            "Step=33400  Loss=0.9394508428087325\n",
            "Step=33500  Loss=0.9405321817375115\n",
            "Step=33600  Loss=0.9416136002492173\n",
            "Step=33700  Loss=0.9426950949642507\n",
            "Step=33800  Loss=0.9437766625441957\n",
            "Step=33900  Loss=0.9448582996912991\n",
            "Step=34000  Loss=0.9459400031479646\n",
            "Step=34100  Loss=0.9470217696962459\n",
            "Step=34200  Loss=0.9481035961573542\n",
            "Step=34300  Loss=0.9491854793911678\n",
            "Step=34400  Loss=0.9502674162957507\n",
            "Step=34500  Loss=0.9513494038068765\n",
            "Step=34600  Loss=0.9524314388975604\n",
            "Step=34700  Loss=0.9535135185775978\n",
            "Step=34800  Loss=0.9545956398931067\n",
            "Step=34900  Loss=0.9556777999260802\n",
            "Step=35000  Loss=0.9567599957939419\n",
            "Step=35100  Loss=0.9578422246491094\n",
            "Step=35200  Loss=0.9589244836785642\n",
            "Step=35300  Loss=0.9600067701034243\n",
            "Step=35400  Loss=0.9610890811785273\n",
            "Step=35500  Loss=0.9621714141920157\n",
            "Step=35600  Loss=0.9632537664649293\n",
            "Step=35700  Loss=0.9643361353508026\n",
            "Step=35800  Loss=0.9654185182352684\n",
            "Step=35900  Loss=0.9665009125356662\n",
            "Step=36000  Loss=0.9675833157006561\n",
            "Step=36100  Loss=0.9686657252098386\n",
            "Step=36200  Loss=0.9697481385733779\n",
            "Step=36300  Loss=0.9708305533316328\n",
            "Step=36400  Loss=0.9719129670547886\n",
            "Step=36500  Loss=0.9729953773424997\n",
            "Step=36600  Loss=0.9740777818235303\n",
            "Step=36700  Loss=0.9751601781554077\n",
            "Step=36800  Loss=0.9762425640240717\n",
            "Step=36900  Loss=0.9773249371435362\n",
            "Step=37000  Loss=0.9784072952555493\n",
            "Step=37100  Loss=0.9794896361292638\n",
            "Step=37200  Loss=0.9805719575609063\n",
            "Step=37300  Loss=0.9816542573734554\n",
            "Step=37400  Loss=0.9827365334163208\n",
            "Step=37500  Loss=0.9838187835650296\n",
            "Step=37600  Loss=0.9849010057209135\n",
            "Step=37700  Loss=0.9859831978108035\n",
            "Step=37800  Loss=0.9870653577867254\n",
            "Step=37900  Loss=0.9881474836256023\n",
            "Step=38000  Loss=0.9892295733289594\n",
            "Step=38100  Loss=0.9903116249226321\n",
            "Step=38200  Loss=0.9913936364564802\n",
            "Step=38300  Loss=0.9924756060041019\n",
            "Step=38400  Loss=0.9935575316625564\n",
            "Step=38500  Loss=0.994639411552088\n",
            "Step=38600  Loss=0.9957212438158516\n",
            "Step=38700  Loss=0.9968030266196439\n",
            "Step=38800  Loss=0.9978847581516401\n",
            "Step=38900  Loss=0.9989664366221296\n",
            "Step=39000  Loss=1.0000480602632584\n",
            "Step=39100  Loss=1.0011296273287735\n",
            "Step=39200  Loss=1.0022111360937709\n",
            "Step=39300  Loss=1.0032925848544467\n",
            "Step=39400  Loss=1.0043739719278522\n",
            "Step=39500  Loss=1.0054552956516503\n",
            "Step=39600  Loss=1.0065365543838776\n",
            "Step=39700  Loss=1.0076177465027074\n",
            "Step=39800  Loss=1.0086988704062163\n",
            "Step=39900  Loss=1.0097799245121548\n",
            "Step=40000  Loss=1.0108609072577193\n",
            "Step=40100  Loss=1.0119418170993266\n",
            "Step=40200  Loss=1.0130226525123966\n",
            "Step=40300  Loss=1.0141034119911279\n",
            "Step=40400  Loss=1.0151840940482855\n",
            "Step=40500  Loss=1.0162646972149867\n",
            "Step=40600  Loss=1.0173452200404909\n",
            "Step=40700  Loss=1.0184256610919908\n",
            "Step=40800  Loss=1.0195060189544074\n",
            "Step=40900  Loss=1.0205862922301892\n",
            "Step=41000  Loss=1.0216664795391088\n",
            "Step=41100  Loss=1.0227465795180681\n",
            "Step=41200  Loss=1.023826590820901\n",
            "Step=41300  Loss=1.0249065121181828\n",
            "Step=41400  Loss=1.0259863420970388\n",
            "Step=41500  Loss=1.0270660794609556\n",
            "Step=41600  Loss=1.028145722929597\n",
            "Step=41700  Loss=1.0292252712386196\n",
            "Step=41800  Loss=1.0303047231394937\n",
            "Step=41900  Loss=1.031384077399321\n",
            "Step=42000  Loss=1.0324633328006616\n",
            "Step=42100  Loss=1.0335424881413582\n",
            "Step=42200  Loss=1.0346215422343625\n",
            "Step=42300  Loss=1.035700493907569\n",
            "Step=42400  Loss=1.036779342003642\n",
            "Step=42500  Loss=1.0378580853798538\n",
            "Step=42600  Loss=1.038936722907918\n",
            "Step=42700  Loss=1.040015253473831\n",
            "Step=42800  Loss=1.0410936759777056\n",
            "Step=42900  Loss=1.0421719893336212\n",
            "Step=43000  Loss=1.043250192469463\n",
            "Step=43100  Loss=1.0443282843267676\n",
            "Step=43200  Loss=1.0454062638605728\n",
            "Step=43300  Loss=1.0464841300392658\n",
            "Step=43400  Loss=1.0475618818444354\n",
            "Step=43500  Loss=1.0486395182707247\n",
            "Step=43600  Loss=1.0497170383256862\n",
            "Step=43700  Loss=1.0507944410296381\n",
            "Step=43800  Loss=1.0518717254155243\n",
            "Step=43900  Loss=1.052948890528772\n",
            "Step=44000  Loss=1.0540259354271575\n",
            "Step=44100  Loss=1.0551028591806644\n",
            "Step=44200  Loss=1.0561796608713532\n",
            "Step=44300  Loss=1.0572563395932266\n",
            "Step=44400  Loss=1.0583328944520962\n",
            "Step=44500  Loss=1.0594093245654566\n",
            "Step=44600  Loss=1.0604856290623514\n",
            "Step=44700  Loss=1.0615618070832504\n",
            "Step=44800  Loss=1.062637857779922\n",
            "Step=44900  Loss=1.0637137803153103\n",
            "Step=45000  Loss=1.0647895738634128\n",
            "Step=45100  Loss=1.0658652376091573\n",
            "Step=45200  Loss=1.0669407707482843\n",
            "Step=45300  Loss=1.0680161724872281\n",
            "Step=45400  Loss=1.0690914420429998\n",
            "Step=45500  Loss=1.070166578643072\n",
            "Step=45600  Loss=1.0712415815252643\n",
            "Step=45700  Loss=1.0723164499376316\n",
            "Step=45800  Loss=1.0733911831383516\n",
            "Step=45900  Loss=1.0744657803956141\n",
            "Step=46000  Loss=1.075540240987515\n",
            "Step=46100  Loss=1.076614564201945\n",
            "Step=46200  Loss=1.0776887493364853\n",
            "Step=46300  Loss=1.0787627956983024\n",
            "Step=46400  Loss=1.0798367026040447\n",
            "Step=46500  Loss=1.0809104693797378\n",
            "Step=46600  Loss=1.0819840953606858\n",
            "Step=46700  Loss=1.0830575798913682\n",
            "Step=46800  Loss=1.084130922325343\n",
            "Step=46900  Loss=1.0852041220251483\n",
            "Step=47000  Loss=1.086277178362204\n",
            "Step=47100  Loss=1.0873500907167173\n",
            "Step=47200  Loss=1.0884228584775888\n",
            "Step=47300  Loss=1.0894954810423165\n",
            "Step=47400  Loss=1.0905679578169054\n",
            "Step=47500  Loss=1.0916402882157752\n",
            "Step=47600  Loss=1.0927124716616707\n",
            "Step=47700  Loss=1.0937845075855717\n",
            "Step=47800  Loss=1.0948563954266048\n",
            "Step=47900  Loss=1.0959281346319567\n",
            "Step=48000  Loss=1.0969997246567862\n",
            "Step=48100  Loss=1.098071164964141\n",
            "Step=48200  Loss=1.0991424550248716\n",
            "Step=48300  Loss=1.1002135943175493\n",
            "Step=48400  Loss=1.1012845823283839\n",
            "Step=48500  Loss=1.1023554185511386\n",
            "Step=48600  Loss=1.1034261024870544\n",
            "Step=48700  Loss=1.1044966336447684\n",
            "Step=48800  Loss=1.1055670115402338\n",
            "Step=48900  Loss=1.1066372356966414\n",
            "Step=49000  Loss=1.107707305644347\n",
            "Step=49100  Loss=1.1087772209207913\n",
            "Step=49200  Loss=1.1098469810704246\n",
            "Step=49300  Loss=1.1109165856446344\n",
            "Step=49400  Loss=1.1119860342016687\n",
            "Step=49500  Loss=1.1130553263065683\n",
            "Step=49600  Loss=1.1141244615310892\n",
            "Step=49700  Loss=1.115193439453635\n",
            "Step=49800  Loss=1.1162622596591842\n",
            "Step=49900  Loss=1.1173309217392229\n",
            "Step=50000  Loss=1.118399425291674\n",
            "Total training time: 1.471945 seconds\n",
            "Predictions [[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1]]\n",
            "Ground truth [[-1 -1  1  1  1 -1  1 -1  1  1 -1 -1  1 -1 -1  1  1 -1 -1 -1 -1 -1  1 -1\n",
            "   1  1  1  1 -1  1 -1 -1  1 -1  1  1  1 -1  1  1 -1  1  1  1  1 -1 -1  1\n",
            "   1  1 -1  1  1  1 -1  1  1 -1  1 -1 -1  1  1  1 -1  1  1  1  1 -1  1  1\n",
            "   1  1  1  1 -1  1 -1 -1 -1  1 -1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1 -1\n",
            "  -1  1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1 -1\n",
            "  -1  1  1  1  1 -1  1  1  1 -1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1  1\n",
            "   1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1 -1 -1  1  1 -1  1  1 -1  1\n",
            "  -1  1 -1 -1  1  1  1  1 -1 -1 -1  1  1  1  1  1 -1  1  1  1 -1  1  1  1\n",
            "   1  1 -1  1  1 -1 -1  1 -1  1 -1 -1  1  1  1 -1  1  1 -1 -1 -1 -1  1  1\n",
            "   1  1  1  1 -1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
            "   1  1  1  1  1  1 -1  1 -1 -1 -1  1  1  1 -1 -1 -1 -1  1  1  1 -1 -1 -1\n",
            "  -1 -1  1  1  1  1 -1  1  1  1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1  1  1  1\n",
            "   1  1 -1  1  1  1  1  1 -1 -1 -1  1  1  1  1 -1 -1  1  1  1  1  1 -1 -1\n",
            "   1  1  1 -1 -1  1 -1  1  1 -1  1  1 -1 -1  1 -1  1 -1 -1  1 -1 -1  1  1\n",
            "   1 -1  1  1  1]]\n",
            "Training set mean accuracy: 0.3783\n",
            "Validation set mean accuracy: 0.3509\n",
            "Testing set mean accuracy: 0.3772\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=10000\n",
            "Step=100  Loss=inf\n",
            "Step=200  Loss=inf\n",
            "Step=300  Loss=inf\n",
            "Step=400  Loss=inf\n",
            "Step=500  Loss=inf\n",
            "Step=600  Loss=inf\n",
            "Step=700  Loss=inf\n",
            "Step=800  Loss=inf\n",
            "Step=900  Loss=inf\n",
            "Step=1000  Loss=inf\n",
            "Step=1100  Loss=inf\n",
            "Step=1200  Loss=inf\n",
            "Step=1300  Loss=inf\n",
            "Step=1400  Loss=inf\n",
            "Step=1500  Loss=inf\n",
            "Step=1600  Loss=inf\n",
            "Step=1700  Loss=inf\n",
            "Step=1800  Loss=inf\n",
            "Step=1900  Loss=inf\n",
            "Step=2000  Loss=inf\n",
            "Step=2100  Loss=inf\n",
            "Step=2200  Loss=inf\n",
            "Step=2300  Loss=inf\n",
            "Step=2400  Loss=inf\n",
            "Step=2500  Loss=inf\n",
            "Step=2600  Loss=inf\n",
            "Step=2700  Loss=inf\n",
            "Step=2800  Loss=inf\n",
            "Step=2900  Loss=inf\n",
            "Step=3000  Loss=inf\n",
            "Step=3100  Loss=inf\n",
            "Step=3200  Loss=inf\n",
            "Step=3300  Loss=inf\n",
            "Step=3400  Loss=inf\n",
            "Step=3500  Loss=inf\n",
            "Step=3600  Loss=inf\n",
            "Step=3700  Loss=inf\n",
            "Step=3800  Loss=inf\n",
            "Step=3900  Loss=inf\n",
            "Step=4000  Loss=inf\n",
            "Step=4100  Loss=inf\n",
            "Step=4200  Loss=inf\n",
            "Step=4300  Loss=inf\n",
            "Step=4400  Loss=inf\n",
            "Step=4500  Loss=inf\n",
            "Step=4600  Loss=inf\n",
            "Step=4700  Loss=inf\n",
            "Step=4800  Loss=inf\n",
            "Step=4900  Loss=inf\n",
            "Step=5000  Loss=inf\n",
            "Step=5100  Loss=inf\n",
            "Step=5200  Loss=inf\n",
            "Step=5300  Loss=inf\n",
            "Step=5400  Loss=inf\n",
            "Step=5500  Loss=inf\n",
            "Step=5600  Loss=inf\n",
            "Step=5700  Loss=inf\n",
            "Step=5800  Loss=inf\n",
            "Step=5900  Loss=inf\n",
            "Step=6000  Loss=inf\n",
            "Step=6100  Loss=inf\n",
            "Step=6200  Loss=inf\n",
            "Step=6300  Loss=inf\n",
            "Step=6400  Loss=inf\n",
            "Step=6500  Loss=inf\n",
            "Step=6600  Loss=inf\n",
            "Step=6700  Loss=inf\n",
            "Step=6800  Loss=inf\n",
            "Step=6900  Loss=inf\n",
            "Step=7000  Loss=inf\n",
            "Step=7100  Loss=inf\n",
            "Step=7200  Loss=inf\n",
            "Step=7300  Loss=inf\n",
            "Step=7400  Loss=inf\n",
            "Step=7500  Loss=inf\n",
            "Step=7600  Loss=inf\n",
            "Step=7700  Loss=inf\n",
            "Step=7800  Loss=inf\n",
            "Step=7900  Loss=inf\n",
            "Step=8000  Loss=inf\n",
            "Step=8100  Loss=inf\n",
            "Step=8200  Loss=inf\n",
            "Step=8300  Loss=inf\n",
            "Step=8400  Loss=inf\n",
            "Step=8500  Loss=inf\n",
            "Step=8600  Loss=inf\n",
            "Step=8700  Loss=inf\n",
            "Step=8800  Loss=inf\n",
            "Step=8900  Loss=inf\n",
            "Step=9000  Loss=inf\n",
            "Step=9100  Loss=inf\n",
            "Step=9200  Loss=inf\n",
            "Step=9300  Loss=inf\n",
            "Step=9400  Loss=inf\n",
            "Step=9500  Loss=inf\n",
            "Step=9600  Loss=inf\n",
            "Step=9700  Loss=inf\n",
            "Step=9800  Loss=inf\n",
            "Step=9900  Loss=inf\n",
            "Step=10000  Loss=inf\n",
            "Total training time: 0.331702 seconds\n",
            "Predictions [[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1]]\n",
            "Ground truth [[-1 -1  1  1  1 -1  1 -1  1  1 -1 -1  1 -1 -1  1  1 -1 -1 -1 -1 -1  1 -1\n",
            "   1  1  1  1 -1  1 -1 -1  1 -1  1  1  1 -1  1  1 -1  1  1  1  1 -1 -1  1\n",
            "   1  1 -1  1  1  1 -1  1  1 -1  1 -1 -1  1  1  1 -1  1  1  1  1 -1  1  1\n",
            "   1  1  1  1 -1  1 -1 -1 -1  1 -1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1 -1\n",
            "  -1  1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1 -1\n",
            "  -1  1  1  1  1 -1  1  1  1 -1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1  1\n",
            "   1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1 -1 -1  1  1 -1  1  1 -1  1\n",
            "  -1  1 -1 -1  1  1  1  1 -1 -1 -1  1  1  1  1  1 -1  1  1  1 -1  1  1  1\n",
            "   1  1 -1  1  1 -1 -1  1 -1  1 -1 -1  1  1  1 -1  1  1 -1 -1 -1 -1  1  1\n",
            "   1  1  1  1 -1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
            "   1  1  1  1  1  1 -1  1 -1 -1 -1  1  1  1 -1 -1 -1 -1  1  1  1 -1 -1 -1\n",
            "  -1 -1  1  1  1  1 -1  1  1  1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1  1  1  1\n",
            "   1  1 -1  1  1  1  1  1 -1 -1 -1  1  1  1  1 -1 -1  1  1  1  1  1 -1 -1\n",
            "   1  1  1 -1 -1  1 -1  1  1 -1  1  1 -1 -1  1 -1  1 -1 -1  1 -1 -1  1  1\n",
            "   1 -1  1  1  1]]\n",
            "Training set mean accuracy: 0.3783\n",
            "Validation set mean accuracy: 0.3509\n",
            "Testing set mean accuracy: 0.3772\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=None \n",
            "\t batch_size=56 \n",
            "\t T=10000\n",
            "Step=100  Loss=78.03924061623107\n",
            "Step=200  Loss=85.25945824097201\n",
            "Step=300  Loss=89.01176329805435\n",
            "Step=400  Loss=91.38323278873676\n",
            "Step=500  Loss=93.51912775035339\n",
            "Step=600  Loss=95.38630396163535\n",
            "Step=700  Loss=96.92324057269111\n",
            "Step=800  Loss=98.10276981239284\n",
            "Step=900  Loss=99.24840443610212\n",
            "Step=1000  Loss=100.33146950031819\n",
            "Step=1100  Loss=101.16471670239945\n",
            "Step=1200  Loss=101.89805218353834\n",
            "Step=1300  Loss=102.67424232858036\n",
            "Step=1400  Loss=103.41461782244389\n",
            "Step=1500  Loss=104.03486216506882\n",
            "Step=1600  Loss=104.62993484368602\n",
            "Step=1700  Loss=105.29707532119177\n",
            "Step=1800  Loss=105.87147937525909\n",
            "Step=1900  Loss=106.3696541583269\n",
            "Step=2000  Loss=106.86024238610123\n",
            "Step=2100  Loss=107.31183921125049\n",
            "Step=2200  Loss=107.75663890615172\n",
            "Step=2300  Loss=108.21880063883278\n",
            "Step=2400  Loss=108.65338478583215\n",
            "Step=2500  Loss=109.10134493865934\n",
            "Step=2600  Loss=109.48586621674727\n",
            "Step=2700  Loss=109.87181845577415\n",
            "Step=2800  Loss=110.21211329526908\n",
            "Step=2900  Loss=110.55052344314007\n",
            "Step=3000  Loss=110.86689140704709\n",
            "Step=3100  Loss=111.18417626400183\n",
            "Step=3200  Loss=111.44804769638336\n",
            "Step=3300  Loss=111.75469697553375\n",
            "Step=3400  Loss=112.05933278740926\n",
            "Step=3500  Loss=112.35865342529834\n",
            "Step=3600  Loss=112.61505495856805\n",
            "Step=3700  Loss=112.88330563204141\n",
            "Step=3800  Loss=113.13513727264433\n",
            "Step=3900  Loss=113.4059941009024\n",
            "Step=4000  Loss=113.67940586473767\n",
            "Step=4100  Loss=113.96255869211954\n",
            "Step=4200  Loss=114.19646037115015\n",
            "Step=4300  Loss=114.43468227544122\n",
            "Step=4400  Loss=114.64599188440415\n",
            "Step=4500  Loss=114.84963737448476\n",
            "Step=4600  Loss=115.09308353980067\n",
            "Step=4700  Loss=115.3170353814408\n",
            "Step=4800  Loss=115.53249834603304\n",
            "Step=4900  Loss=115.73105071752167\n",
            "Step=5000  Loss=115.95334115597586\n",
            "Step=5100  Loss=116.14570687337685\n",
            "Step=5200  Loss=116.3329438060029\n",
            "Step=5300  Loss=116.52510303916527\n",
            "Step=5400  Loss=116.71815288010647\n",
            "Step=5500  Loss=116.90494241358795\n",
            "Step=5600  Loss=117.08942475168324\n",
            "Step=5700  Loss=117.27147170982585\n",
            "Step=5800  Loss=117.45727424454773\n",
            "Step=5900  Loss=117.60563680831011\n",
            "Step=6000  Loss=117.76152841603356\n",
            "Step=6100  Loss=117.93390366933892\n",
            "Step=6200  Loss=118.07459247263358\n",
            "Step=6300  Loss=118.24373462581923\n",
            "Step=6400  Loss=118.38414166848\n",
            "Step=6500  Loss=118.53903544928463\n",
            "Step=6600  Loss=118.70159957213386\n",
            "Step=6700  Loss=118.8473986573576\n",
            "Step=6800  Loss=118.9916108566691\n",
            "Step=6900  Loss=119.1267909752459\n",
            "Step=7000  Loss=119.26725942947192\n",
            "Step=7100  Loss=119.39617179734107\n",
            "Step=7200  Loss=119.5281137728908\n",
            "Step=7300  Loss=119.66955077424144\n",
            "Step=7400  Loss=119.79288180693284\n",
            "Step=7500  Loss=119.92313110255309\n",
            "Step=7600  Loss=120.063505117729\n",
            "Step=7700  Loss=120.18902730838356\n",
            "Step=7800  Loss=120.33718391103032\n",
            "Step=7900  Loss=120.47745893391998\n",
            "Step=8000  Loss=120.56111549150448\n",
            "Step=8100  Loss=120.67770858656588\n",
            "Step=8200  Loss=120.78399710496474\n",
            "Step=8300  Loss=120.9045457559325\n",
            "Step=8400  Loss=121.02104377021139\n",
            "Step=8500  Loss=121.14857880345275\n",
            "Step=8600  Loss=121.24285025110767\n",
            "Step=8700  Loss=121.35968963179327\n",
            "Step=8800  Loss=121.47551044167257\n",
            "Step=8900  Loss=121.58260390788546\n",
            "Step=9000  Loss=121.67647417979623\n",
            "Step=9100  Loss=121.78784452804443\n",
            "Step=9200  Loss=121.891218034142\n",
            "Step=9300  Loss=121.9929847932397\n",
            "Step=9400  Loss=122.0987654873175\n",
            "Step=9500  Loss=122.19861505084197\n",
            "Step=9600  Loss=122.3054378302225\n",
            "Step=9700  Loss=122.4146178749278\n",
            "Step=9800  Loss=122.51287742819126\n",
            "Step=9900  Loss=122.60317628271336\n",
            "Step=10000  Loss=122.70334622431754\n",
            "Total training time: 0.578967 seconds\n",
            "Predictions [[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1]]\n",
            "Ground truth [[-1 -1  1  1  1 -1  1 -1  1  1 -1 -1  1 -1 -1  1  1 -1 -1 -1 -1 -1  1 -1\n",
            "   1  1  1  1 -1  1 -1 -1  1 -1  1  1  1 -1  1  1 -1  1  1  1  1 -1 -1  1\n",
            "   1  1 -1  1  1  1 -1  1  1 -1  1 -1 -1  1  1  1 -1  1  1  1  1 -1  1  1\n",
            "   1  1  1  1 -1  1 -1 -1 -1  1 -1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1 -1\n",
            "  -1  1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1 -1\n",
            "  -1  1  1  1  1 -1  1  1  1 -1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1  1\n",
            "   1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1 -1 -1  1  1 -1  1  1 -1  1\n",
            "  -1  1 -1 -1  1  1  1  1 -1 -1 -1  1  1  1  1  1 -1  1  1  1 -1  1  1  1\n",
            "   1  1 -1  1  1 -1 -1  1 -1  1 -1 -1  1  1  1 -1  1  1 -1 -1 -1 -1  1  1\n",
            "   1  1  1  1 -1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
            "   1  1  1  1  1  1 -1  1 -1 -1 -1  1  1  1 -1 -1 -1 -1  1  1  1 -1 -1 -1\n",
            "  -1 -1  1  1  1  1 -1  1  1  1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1  1  1  1\n",
            "   1  1 -1  1  1  1  1  1 -1 -1 -1  1  1  1  1 -1 -1  1  1  1  1  1 -1 -1\n",
            "   1  1  1 -1 -1  1 -1  1  1 -1  1  1 -1 -1  1 -1  1 -1 -1  1 -1 -1  1  1\n",
            "   1 -1  1  1  1]]\n",
            "Training set mean accuracy: 0.3783\n",
            "Validation set mean accuracy: 0.3509\n",
            "Testing set mean accuracy: 0.3772\n",
            "***** Results of our logistic regression model trained on breast cancer dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=0.9 \n",
            "\t batch_size=56 \n",
            "\t T=10000\n",
            "Step=100  Loss=inf\n",
            "Step=200  Loss=inf\n",
            "Step=300  Loss=inf\n",
            "Step=400  Loss=inf\n",
            "Step=500  Loss=inf\n",
            "Step=600  Loss=inf\n",
            "Step=700  Loss=inf\n",
            "Step=800  Loss=inf\n",
            "Step=900  Loss=inf\n",
            "Step=1000  Loss=inf\n",
            "Step=1100  Loss=inf\n",
            "Step=1200  Loss=inf\n",
            "Step=1300  Loss=inf\n",
            "Step=1400  Loss=inf\n",
            "Step=1500  Loss=inf\n",
            "Step=1600  Loss=inf\n",
            "Step=1700  Loss=inf\n",
            "Step=1800  Loss=inf\n",
            "Step=1900  Loss=inf\n",
            "Step=2000  Loss=inf\n",
            "Step=2100  Loss=inf\n",
            "Step=2200  Loss=inf\n",
            "Step=2300  Loss=inf\n",
            "Step=2400  Loss=inf\n",
            "Step=2500  Loss=inf\n",
            "Step=2600  Loss=inf\n",
            "Step=2700  Loss=inf\n",
            "Step=2800  Loss=inf\n",
            "Step=2900  Loss=inf\n",
            "Step=3000  Loss=inf\n",
            "Step=3100  Loss=inf\n",
            "Step=3200  Loss=inf\n",
            "Step=3300  Loss=inf\n",
            "Step=3400  Loss=inf\n",
            "Step=3500  Loss=inf\n",
            "Step=3600  Loss=inf\n",
            "Step=3700  Loss=inf\n",
            "Step=3800  Loss=inf\n",
            "Step=3900  Loss=inf\n",
            "Step=4000  Loss=inf\n",
            "Step=4100  Loss=inf\n",
            "Step=4200  Loss=inf\n",
            "Step=4300  Loss=inf\n",
            "Step=4400  Loss=inf\n",
            "Step=4500  Loss=inf\n",
            "Step=4600  Loss=inf\n",
            "Step=4700  Loss=inf\n",
            "Step=4800  Loss=inf\n",
            "Step=4900  Loss=inf\n",
            "Step=5000  Loss=inf\n",
            "Step=5100  Loss=inf\n",
            "Step=5200  Loss=inf\n",
            "Step=5300  Loss=inf\n",
            "Step=5400  Loss=inf\n",
            "Step=5500  Loss=inf\n",
            "Step=5600  Loss=inf\n",
            "Step=5700  Loss=inf\n",
            "Step=5800  Loss=inf\n",
            "Step=5900  Loss=inf\n",
            "Step=6000  Loss=inf\n",
            "Step=6100  Loss=inf\n",
            "Step=6200  Loss=inf\n",
            "Step=6300  Loss=inf\n",
            "Step=6400  Loss=inf\n",
            "Step=6500  Loss=inf\n",
            "Step=6600  Loss=inf\n",
            "Step=6700  Loss=inf\n",
            "Step=6800  Loss=inf\n",
            "Step=6900  Loss=inf\n",
            "Step=7000  Loss=inf\n",
            "Step=7100  Loss=inf\n",
            "Step=7200  Loss=inf\n",
            "Step=7300  Loss=inf\n",
            "Step=7400  Loss=inf\n",
            "Step=7500  Loss=inf\n",
            "Step=7600  Loss=inf\n",
            "Step=7700  Loss=inf\n",
            "Step=7800  Loss=inf\n",
            "Step=7900  Loss=inf\n",
            "Step=8000  Loss=inf\n",
            "Step=8100  Loss=inf\n",
            "Step=8200  Loss=inf\n",
            "Step=8300  Loss=inf\n",
            "Step=8400  Loss=inf\n",
            "Step=8500  Loss=inf\n",
            "Step=8600  Loss=inf\n",
            "Step=8700  Loss=inf\n",
            "Step=8800  Loss=inf\n",
            "Step=8900  Loss=inf\n",
            "Step=9000  Loss=inf\n",
            "Step=9100  Loss=inf\n",
            "Step=9200  Loss=inf\n",
            "Step=9300  Loss=inf\n",
            "Step=9400  Loss=inf\n",
            "Step=9500  Loss=inf\n",
            "Step=9600  Loss=inf\n",
            "Step=9700  Loss=inf\n",
            "Step=9800  Loss=inf\n",
            "Step=9900  Loss=inf\n",
            "Step=10000  Loss=inf\n",
            "Total training time: 0.588608 seconds\n",
            "Predictions [[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  -1 -1 -1 -1 -1]]\n",
            "Ground truth [[-1 -1  1  1  1 -1  1 -1  1  1 -1 -1  1 -1 -1  1  1 -1 -1 -1 -1 -1  1 -1\n",
            "   1  1  1  1 -1  1 -1 -1  1 -1  1  1  1 -1  1  1 -1  1  1  1  1 -1 -1  1\n",
            "   1  1 -1  1  1  1 -1  1  1 -1  1 -1 -1  1  1  1 -1  1  1  1  1 -1  1  1\n",
            "   1  1  1  1 -1  1 -1 -1 -1  1 -1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1 -1\n",
            "  -1  1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1 -1\n",
            "  -1  1  1  1  1 -1  1  1  1 -1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1  1\n",
            "   1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1 -1 -1  1  1 -1  1  1 -1  1\n",
            "  -1  1 -1 -1  1  1  1  1 -1 -1 -1  1  1  1  1  1 -1  1  1  1 -1  1  1  1\n",
            "   1  1 -1  1  1 -1 -1  1 -1  1 -1 -1  1  1  1 -1  1  1 -1 -1 -1 -1  1  1\n",
            "   1  1  1  1 -1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
            "   1  1  1  1  1  1 -1  1 -1 -1 -1  1  1  1 -1 -1 -1 -1  1  1  1 -1 -1 -1\n",
            "  -1 -1  1  1  1  1 -1  1  1  1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1  1  1  1\n",
            "   1  1 -1  1  1  1  1  1 -1 -1 -1  1  1  1  1 -1 -1  1  1  1  1  1 -1 -1\n",
            "   1  1  1 -1 -1  1 -1  1  1 -1  1  1 -1 -1  1 -1  1 -1 -1  1 -1 -1  1  1\n",
            "   1 -1  1  1  1]]\n",
            "Training set mean accuracy: 0.3783\n",
            "Validation set mean accuracy: 0.3509\n",
            "Testing set mean accuracy: 0.3772\n",
            "\n",
            "Preprocessing the digits greater or less than 5 dataset (1797 samples, 64 feature dimensions)\n",
            "***** Results on the digits greater or less than 5 dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.9147\n",
            "Validation set mean accuracy: 0.9025\n",
            "Testing set mean accuracy: 0.8611\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=gradient_descent \n",
            "\t alpha=0.0001 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=None \n",
            "\t batch_size=None \n",
            "\t T=1\n",
            "Total training time: 0.000087 seconds\n",
            "Predictions [[-1 -1 -1 ... -1 -1 -1]]\n",
            "Ground truth [[-1 -1 -1 ... -1 -1  1]]\n",
            "Training set mean accuracy: 0.5195\n",
            "Validation set mean accuracy: 0.4568\n",
            "Testing set mean accuracy: 0.4778\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=None \n",
            "\t beta=0.9 \n",
            "\t batch_size=None \n",
            "\t T=1\n",
            "Total training time: 0.000075 seconds\n",
            "Predictions [[-1 -1 -1 ... -1 -1 -1]]\n",
            "Ground truth [[-1 -1 -1 ... -1 -1  1]]\n",
            "Training set mean accuracy: 0.5195\n",
            "Validation set mean accuracy: 0.4568\n",
            "Testing set mean accuracy: 0.4778\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=None \n",
            "\t batch_size=150 \n",
            "\t T=1\n",
            "Total training time: 0.000130 seconds\n",
            "Predictions [[-1 -1 -1 ... -1 -1 -1]]\n",
            "Ground truth [[-1 -1 -1 ... -1 -1  1]]\n",
            "Training set mean accuracy: 0.5195\n",
            "Validation set mean accuracy: 0.4568\n",
            "Testing set mean accuracy: 0.4778\n",
            "***** Results of our logistic regression model trained on digits greater or less than 5 dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.1 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=0.9 \n",
            "\t batch_size=150 \n",
            "\t T=1\n",
            "Total training time: 0.000118 seconds\n",
            "Predictions [[-1 -1 -1 ... -1 -1 -1]]\n",
            "Ground truth [[-1 -1 -1 ... -1 -1  1]]\n",
            "Training set mean accuracy: 0.5195\n",
            "Validation set mean accuracy: 0.4568\n",
            "Testing set mean accuracy: 0.4778\n",
            "\n",
            "Preprocessing the fir and pine coverage dataset (495141 samples, 54 feature dimensions)\n",
            "***** Results on the fir and pine coverage dataset using scikit-learn logistic regression model *****\n",
            "Training set mean accuracy: 0.7463\n",
            "Validation set mean accuracy: 0.7450\n",
            "Testing set mean accuracy: 0.7465\n",
            "***** Results of our logistic regression model trained on fir and pine coverage dataset *****\n",
            "\t optimizer_type=stochastic_gradient_descent \n",
            "\t alpha=0.01 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=None \n",
            "\t batch_size=49000 \n",
            "\t T=1\n",
            "Total training time: 0.062680 seconds\n",
            "Predictions [[-1 -1 -1 ... -1 -1 -1]]\n",
            "Ground truth [[-1  1  1 ... -1 -1 -1]]\n",
            "Training set mean accuracy: 0.5727\n",
            "Validation set mean accuracy: 0.5700\n",
            "Testing set mean accuracy: 0.5727\n",
            "***** Results of our logistic regression model trained on fir and pine coverage dataset *****\n",
            "\t optimizer_type=momentum_stochastic_gradient_descent \n",
            "\t alpha=0.1 \n",
            "\t eta_decay_factor=0.99 \n",
            "\t beta=0.9 \n",
            "\t batch_size=49000 \n",
            "\t T=1\n",
            "Total training time: 0.069863 seconds\n",
            "Predictions [[-1 -1 -1 ... -1 -1 -1]]\n",
            "Ground truth [[-1  1  1 ... -1 -1 -1]]\n",
            "Training set mean accuracy: 0.5727\n",
            "Validation set mean accuracy: 0.5700\n",
            "Testing set mean accuracy: 0.5727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load breast cancer, digits, and tree coverage datasets\n",
        "datasets = [\n",
        "    skdata.load_breast_cancer(),\n",
        "    skdata.load_digits(),\n",
        "    skdata.fetch_covtype()\n",
        "]\n",
        "dataset_names = [\n",
        "    'breast cancer',\n",
        "    'digits greater or less than 5',\n",
        "    'fir and pine coverage',\n",
        "]\n",
        "\n",
        "# Loss functions to minimize\n",
        "dataset_optimizer_types = [\n",
        "    # For breast cancer dataset\n",
        "    [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    ], [\n",
        "        'gradient_descent',\n",
        "        'momentum_gradient_descent',\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    # For fir and pine coverage dataset\n",
        "    ], [\n",
        "        'stochastic_gradient_descent',\n",
        "        'momentum_stochastic_gradient_descent'\n",
        "    ]\n",
        "]\n",
        "\n",
        "# DONE?: Select hyperparameters\n",
        "\n",
        "# Step size (always used)\n",
        "dataset_alphas = [\n",
        "    # For breast cancer dataset\n",
        "    [1e-10, 1e-4, 1e-4, 0.01],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [1e-4, 0.01, 0.01, 0.1],\n",
        "    # For fir and pine coverage dataset\n",
        "    [0.01, 0.1]\n",
        "]\n",
        "\n",
        "# How much the learning rate should decrease over time (only for SGD)\n",
        "dataset_eta_decay_factors = [\n",
        "    # For breast cancer dataset\n",
        "    [None, None, 0.99, 0.99],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, None, 0.99, 0.99],\n",
        "    # For fir and pine coverage dataset\n",
        "    [0.99, 0.99]\n",
        "]\n",
        "\n",
        "# Momentum (only for momentum-based duh)\n",
        "# None for no momentum\n",
        "dataset_betas = [\n",
        "    # For breast cancer dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [None, 0.9, None, 0.9],\n",
        "    # For fir and pine coverage dataset\n",
        "    [None, 0.9]\n",
        "]\n",
        "\n",
        "# Samples (only for SGD)\n",
        "dataset_batch_sizes = [\n",
        "    # For breast cancer dataset\n",
        "    # N = 569\n",
        "    [None, None, 56, 56],\n",
        "    # For digits greater than or less than 5 dataset \n",
        "    # N = 1797\n",
        "    [None, None, 150, 150],\n",
        "    # For fir and pine coverage dataset \n",
        "    # # N = 495141\n",
        "    [49000, 49000]\n",
        "]\n",
        "\n",
        "\n",
        "# Iterations\n",
        "dataset_Ts = [\n",
        "    # For breast cancer dataset\n",
        "    [50000, 10000, 10000, 10000],\n",
        "    # For digits greater than or less than 5 dataset\n",
        "    [1, 1, 1, 1],\n",
        "    # For fir and pine coverage dataset\n",
        "    [1, 1]\n",
        "]\n",
        "\n",
        "# Zip up all dataset options\n",
        "dataset_options = zip(\n",
        "    datasets,\n",
        "    dataset_names,\n",
        "    dataset_optimizer_types,\n",
        "    dataset_alphas,\n",
        "    dataset_eta_decay_factors,\n",
        "    dataset_betas,\n",
        "    dataset_batch_sizes,\n",
        "    dataset_Ts)\n",
        "\n",
        "\n",
        "for options in dataset_options:\n",
        "\n",
        "    # Unpack dataset options\n",
        "    dataset, \\\n",
        "        dataset_name, \\\n",
        "        optimizer_types, \\\n",
        "        alphas, \\\n",
        "        eta_decay_factors, \\\n",
        "        betas, \\\n",
        "        batch_sizes, \\\n",
        "        Ts = options\n",
        "\n",
        "    '''\n",
        "    Create the training, validation and testing splits\n",
        "    '''\n",
        "    x = dataset.data\n",
        "    y = dataset.target\n",
        "\n",
        "    if dataset_name == 'digits greater or less than 5':\n",
        "        y[y < 5] = 1\n",
        "        y[y >= 5] = 0\n",
        "    elif dataset_name == 'fir and pine coverage':\n",
        "\n",
        "        idx_fir_or_pine = np.where(np.logical_or(y == 1, y == 2))[0]\n",
        "\n",
        "        x = x[idx_fir_or_pine, :]\n",
        "        y = y[idx_fir_or_pine]\n",
        "\n",
        "        # Pine class: 0; Fir class: 1\n",
        "        y[y == 2] = 0\n",
        "\n",
        "    print('Preprocessing the {} dataset ({} samples, {} feature dimensions)'.format(dataset_name, x.shape[0], x.shape[1]))\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(x.shape[0])\n",
        "\n",
        "    # Choose the first 60% as training set, next 20% as validation and the rest as testing\n",
        "    train_split_idx = int(0.60 * x.shape[0])\n",
        "    val_split_idx = int(0.80 * x.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:val_split_idx]\n",
        "    test_indices = shuffled_indices[val_split_idx:]\n",
        "\n",
        "    # Select the examples from x and y to construct our training, validation, testing sets\n",
        "    x_train, y_train = x[train_indices, :], y[train_indices]\n",
        "    x_val, y_val = x[val_indices, :], y[val_indices]\n",
        "    x_test, y_test = x[test_indices, :], y[test_indices]\n",
        "\n",
        "    '''\n",
        "    Trains and tests logistic regression model from scikit-learn\n",
        "    '''\n",
        "    model_scikit = LogisticRegressionSciKit(penalty=None, fit_intercept=False)\n",
        "\n",
        "    # DONE?: Train scikit-learn logistic regression model\n",
        "    model_scikit.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "    print('***** Results on the {} dataset using scikit-learn logistic regression model *****'.format(dataset_name))\n",
        "\n",
        "    # DONE: Score model using mean accuracy on training set\n",
        "    predictions_train = model_scikit.predict(x_train)\n",
        "    score_train = skmetrics.accuracy_score(y_train, predictions_train)\n",
        "    print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "    # DONE: Score model using mean accuracy on validation set\n",
        "    predictions_val = model_scikit.predict(x_val)\n",
        "    score_val = skmetrics.accuracy_score(y_val, predictions_val)\n",
        "    print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "    # DONE: Score model using mean accuracy on testing set\n",
        "    predictions_test = model_scikit.predict(x_test)\n",
        "    score_test = skmetrics.accuracy_score(y_test, predictions_test)\n",
        "    print('Testing set mean accuracy: {:.4f}'.format(score_test))\n",
        "\n",
        "    '''\n",
        "    Trains, validates, and tests our logistic regression model for binary classification\n",
        "    '''\n",
        "    # Take the transpose of the dataset to match the dimensions discussed in lecture\n",
        "    # i.e., (N x d) to (d x N)\n",
        "    x_train = np.transpose(x_train, axes=(1, 0))\n",
        "    x_val = np.transpose(x_val, axes=(1, 0))\n",
        "    x_test = np.transpose(x_test, axes=(1, 0))\n",
        "    y_train = np.expand_dims(y_train, axis=0)\n",
        "    y_val = np.expand_dims(y_val, axis=0)\n",
        "    y_test = np.expand_dims(y_test, axis=0)\n",
        "\n",
        "    # DONE: Set the ground truth to the appropriate classes (integers) according to lecture\n",
        "    # -1 and +1\n",
        "    y_train = np.where(y_train == 0, -1, 1)\n",
        "    y_val = np.where(y_val == 0, -1, 1)\n",
        "    y_test = np.where(y_test == 0, -1, 1)\n",
        "\n",
        "    model_options = zip(optimizer_types, alphas, eta_decay_factors, betas, batch_sizes, Ts)\n",
        "\n",
        "    for optimizer_type, alpha, eta_decay_factor, beta, batch_size, T in model_options:\n",
        "\n",
        "        # DONE: Initialize our logistic regression model\n",
        "        model_ours = LogisticRegression()\n",
        "\n",
        "        print('***** Results of our logistic regression model trained on {} dataset *****'.format(dataset_name))\n",
        "        print('\\t optimizer_type={} \\n\\t alpha={} \\n\\t eta_decay_factor={} \\n\\t beta={} \\n\\t batch_size={} \\n\\t T={}'.format(\n",
        "            optimizer_type, alpha, eta_decay_factor, beta, batch_size, T))\n",
        "\n",
        "        time_start = time.time()\n",
        "\n",
        "        # DONE: Train model on training set\n",
        "        model_ours.fit(\n",
        "            x_train,\n",
        "            y_train,\n",
        "            T,\n",
        "            alpha,\n",
        "            eta_decay_factor,\n",
        "            beta,\n",
        "            batch_size,\n",
        "            optimizer_type\n",
        "        )\n",
        "\n",
        "        time_elapsed = time.time() - time_start\n",
        "        print('Total training time: {:3f} seconds'.format(time_elapsed))\n",
        "\n",
        "        # DONE?: Score model using mean accuracy on training set\n",
        "        predictions_train = model_ours.predict(x_train)\n",
        "        score_train = np.mean(predictions_train == y_train)\n",
        "        print(\"Predictions\", predictions_train)\n",
        "        print(\"Ground truth\", y_train)\n",
        "        print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "        # DONE: Score model using mean accuracy on training set\n",
        "        predictions_val = model_ours.predict(x_val)\n",
        "        score_val = np.mean(predictions_val == y_val)\n",
        "        print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "        # DONE: Score model using mean accuracy on training set\n",
        "        predictions_test = model_ours.predict(x_test)\n",
        "        score_test = np.mean(predictions_test == y_test)\n",
        "        print('Testing set mean accuracy: {:.4f}'.format(score_test))\n",
        "\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
