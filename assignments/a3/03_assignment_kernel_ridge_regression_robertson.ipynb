{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r37l-FPc9o0h"
      },
      "source": [
        "**Assignment 3: Kernel Ridge Regression**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "*Student: Hailey Robertson*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "\n",
        "4. Move the 03_assignment_kernel_regression.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Assignments/03_assignment_kernel_ridge_regression.ipynb\n",
        "```\n",
        "\n",
        "In this assignment, we will optimize a kernelized linear function in nonlinear space. We will implement several kernels (linear, polynomial, radial basis function) and train a kernel ridge regression model. We will benchmark our implementation against the one from sci-kit learn, where we should be comparable. Additionally, we will test the speed up for using kernels when the nonlinear mapping function expands feature to high dimensions.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training and validation/testing scores.\n",
        "\n",
        "```\n",
        "Report training and validation/testing scores here.\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: Doe, Jane (Please write names in <Last Name, First Name> format)\n",
        "\n",
        "Collaboration details: Discussed ... implementation details with Jane Doe.\n",
        "```\n",
        "\n",
        "\n",
        "**IMPORTANT**:\n",
        "\n",
        "- For full credit, your mean squared error for all trained models across all datasets should be same as the scores achieved by sci-kit learn's kernel ridge regression model across training, validation and testing splits. Your kernel ridge regression must be faster than sci-kit linear regression with polynomial expansion at 4th order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "koDraeo69YZH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "import sklearn.preprocessing as skpreprocess\n",
        "from sklearn.linear_model import Ridge as RidgeRegressionSciKit\n",
        "from sklearn.kernel_ridge import KernelRidge as KernelRidgeRegressionSciKit\n",
        "from matplotlib import pyplot as plt\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-QOpegXZON9"
      },
      "source": [
        "Helper function for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A8LjhjQuZK6P"
      },
      "outputs": [],
      "source": [
        "def plot_results(axis,\n",
        "                 x_values,\n",
        "                 y_values,\n",
        "                 labels,\n",
        "                 colors,\n",
        "                 x_limits,\n",
        "                 y_limits,\n",
        "                 x_label,\n",
        "                 y_label):\n",
        "    '''\n",
        "    Plots x and y values using line plot with labels and colors\n",
        "\n",
        "    Args:\n",
        "        axis :  pyplot.ax\n",
        "            matplotlib subplot axis\n",
        "        x_values : list[numpy[float32]]\n",
        "            list of numpy array of x values\n",
        "        y_values : list[numpy[float32]]\n",
        "            list of numpy array of y values\n",
        "        labels : str\n",
        "            list of names for legend\n",
        "        colors : str\n",
        "            colors for each line\n",
        "        x_limits : list[float32]\n",
        "            min and max values of x axis\n",
        "        y_limits : list[float32]\n",
        "            min and max values of y axis\n",
        "        x_label : list[float32]\n",
        "            name of x axis\n",
        "        y_label : list[float32]\n",
        "            name of y axis\n",
        "    '''\n",
        "\n",
        "    # Iterate through x_values, y_values, labels, and colors and plot them\n",
        "    # with associated legend\n",
        "    for x, y, label, color in zip(x_values, y_values, labels, colors):\n",
        "        axis.plot(x, y, marker='o', color=color, label=label)\n",
        "        axis.legend(loc='best')\n",
        "\n",
        "    # Set x and y limits\n",
        "    axis.set_xlim(x_limits)\n",
        "    axis.set_ylim(y_limits)\n",
        "\n",
        "    # Set x and y labels\n",
        "    axis.set_xlabel(x_label)\n",
        "    axis.set_ylabel(y_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAR4bm26XZiJ"
      },
      "source": [
        "Implementation of kernel ridge regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI62IQ3d9SpW"
      },
      "outputs": [],
      "source": [
        "class KernelRidgeRegression(object):\n",
        "\n",
        "    def __init__(self, kernel_func, degree=None, gamma=None):\n",
        "        '''\n",
        "        Class for kernel ridge regression\n",
        "\n",
        "        Arg(s):\n",
        "            kernel_func : str\n",
        "                name of kernel function to use: linear, polynomial, rbf (gaussian)\n",
        "            degree : int\n",
        "                p-order for polynomial\n",
        "            gamma : float\n",
        "                standard deviation of the Gaussian\n",
        "        '''\n",
        "\n",
        "        # Define private variables\n",
        "        self.__weights = None\n",
        "        self.__X = None\n",
        "        self.__kernel_func = kernel_func\n",
        "        self.__degree = degree\n",
        "        self.__gamma = gamma\n",
        "\n",
        "    def __linear_kernel(self, X1, X2):\n",
        "        '''\n",
        "        Computes the linear kernel function on X1 and X2\n",
        "\n",
        "        Arg(s):\n",
        "            X1 : numpy[float32]\n",
        "                N x d feature vector\n",
        "            X2 : numpy[float32]\n",
        "                N x d feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : N x N kernel matrix\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement linear kernel\n",
        "        return np.dot(X1, X2.T)\n",
        "\n",
        "\n",
        "    def __polynomial_kernel(self, X1, X2, degree):\n",
        "        '''\n",
        "        Computes the p-order polynomial kernel function on X1 and X2 with c = 1\n",
        "\n",
        "        Arg(s):\n",
        "            X1 : numpy[float32]\n",
        "                N x d feature vector\n",
        "            X2 : numpy[float32]\n",
        "                N x d feature vector\n",
        "            degree : int\n",
        "                p-order for polynomial\n",
        "        Returns:\n",
        "            numpy[float32] : N x N kernel matrix\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement polynomial kernel with c = 1\n",
        "\n",
        "        return (np.dot(X1, X2.T) + 1) ** degree\n",
        "\n",
        "    def __rbf_kernel(self, X1, X2, gamma):\n",
        "        '''\n",
        "        Computes the RBF (Gaussian) kernel function on X1 and X2\n",
        "\n",
        "        Arg(s):\n",
        "            X1 : numpy[float32]\n",
        "                N x d feature vector\n",
        "            X2 : numpy[float32]\n",
        "                N x d feature vector\n",
        "            gamma : float\n",
        "                standard deviation of the Gaussian\n",
        "        Returns:\n",
        "            numpy[float32] : N x N kernel matrix\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement RBF kernel using Gaussian form\n",
        "        X1_squared = np.sum(X1 ** 2, axis=1)\n",
        "        X2_squared = np.sum(X2 ** 2, axis=1)\n",
        "        X1_X2 = np.dot(X1, X2.T) \n",
        "        return np.exp(-gamma * (X1_squared[:, None] + X2_squared[None, :] - 2 * X1_X2))\n",
        "\n",
        "    def fit(self, X, y, weight_decay=0):\n",
        "        '''\n",
        "        Fits the model to X and y via normal equation in kernelized form\n",
        "\n",
        "        Arg(s):\n",
        "            X : numpy[float32]\n",
        "                N x d feature vector\n",
        "            y : numpy[float32]\n",
        "                N x 1 ground-truth label\n",
        "            weight_decay : float\n",
        "                weight of weight decay term\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement the fit function\n",
        "        self.__X = X\n",
        "        \n",
        "        if self.__kernel_func == 'linear':\n",
        "            K = self.__linear_kernel(X, X)\n",
        "\n",
        "        elif self.__kernel_func == 'polynomial':\n",
        "            K = self.__polynomial_kernel(X, X, self.__degree)\n",
        "\n",
        "        elif self.__kernel_func == 'rbf':\n",
        "            K = self.__rbf_kernel(X, X, self.__gamma)\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported kernel function: {}'.format(self.__kernel_func))\n",
        "\n",
        "        self.__weights = np.linalg.solve(K + weight_decay * np.eye(K.shape[0]), y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Predicts the real value for each feature vector X\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                N x d feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : N x 1 real value vector (\\hat{y})\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement the predict function\n",
        "\n",
        "        if self.__kernel_func == 'linear':\n",
        "            K = self.__linear_kernel(X, self.__X)\n",
        "\n",
        "        elif self.__kernel_func == 'polynomial':\n",
        "            K = self.__polynomial_kernel(X, self.__X, self.__degree)\n",
        "\n",
        "        elif self.__kernel_func == 'rbf':\n",
        "            K = self.__rbf_kernel(X, self.__X, self.__gamma)\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Unsupported kernel function: {}'.format(self.__kernel_func))\n",
        "\n",
        "        return np.dot(K, self.__weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj9L0VG_YxUQ"
      },
      "source": [
        "Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WZgWz7PxhEl9"
      },
      "outputs": [],
      "source": [
        "# Load diabetes and Friedman #1 datasets\n",
        "datasets = [\n",
        "    skdata.load_diabetes(),\n",
        "    skdata.make_friedman1(n_samples=5000, n_features=20, noise=0.0, random_state=1)\n",
        "]\n",
        "\n",
        "dataset_names = [\n",
        "    'Diabetes',\n",
        "    'Friedman #1'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zusvzb2xJJzi"
      },
      "source": [
        "Training, validating and testing kernel ridge regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EI7TMva6JIh8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing the Diabetes dataset (442 samples, 10 feature dimensions)\n",
            "***** Experiments on the Diabetes dataset using linear kernel ridge regression model with weight decay of 0.01 *****\n",
            "Results for Scikit-learn model\n",
            "Training set mean squared error: 26074.5179\n",
            "Validation set mean squared error: 27863.0141\n",
            "Testing set mean squared error: 26389.7230\n",
            "Results for our model\n",
            "Training set mean squared error: 26074.5179\n",
            "Validation set mean squared error: 27863.0141\n",
            "Testing set mean squared error: 26389.7230\n",
            "***** Experiments on the Diabetes dataset using polynomial (degree=3) kernel ridge regression model with weight decay of 0.01 *****\n",
            "Results for Scikit-learn model\n",
            "Training set mean squared error: 2394.3930\n",
            "Validation set mean squared error: 3136.2133\n",
            "Testing set mean squared error: 3735.4574\n",
            "Results for our model\n",
            "Training set mean squared error: 2394.3930\n",
            "Validation set mean squared error: 3136.2133\n",
            "Testing set mean squared error: 3735.4574\n",
            "***** Experiments on the Diabetes dataset using rbf (gamma=1) kernel ridge regression model with weight decay of 0.01 *****\n",
            "Results for Scikit-learn model\n",
            "Training set mean squared error: 2576.2058\n",
            "Validation set mean squared error: 3046.3601\n",
            "Testing set mean squared error: 3518.3835\n",
            "Results for our model\n",
            "Training set mean squared error: 2443.6828\n",
            "Validation set mean squared error: 3085.7602\n",
            "Testing set mean squared error: 3623.8841\n",
            "\n",
            "Preprocessing the Friedman #1 dataset (5000 samples, 20 feature dimensions)\n",
            "***** Experiments on the Friedman #1 dataset using linear kernel ridge regression model with weight decay of 0.0001 *****\n",
            "Results for Scikit-learn model\n",
            "Training set mean squared error: 5.7926\n",
            "Validation set mean squared error: 5.8019\n",
            "Testing set mean squared error: 6.7293\n",
            "Results for our model\n",
            "Training set mean squared error: 5.7926\n",
            "Validation set mean squared error: 5.8019\n",
            "Testing set mean squared error: 6.7293\n",
            "***** Experiments on the Friedman #1 dataset using polynomial (degree=3) kernel ridge regression model with weight decay of 0.0001 *****\n",
            "Results for Scikit-learn model\n",
            "Training set mean squared error: 0.0352\n",
            "Validation set mean squared error: 0.2626\n",
            "Testing set mean squared error: 0.2804\n",
            "Results for our model\n",
            "Training set mean squared error: 0.0352\n",
            "Validation set mean squared error: 0.2626\n",
            "Testing set mean squared error: 0.2804\n",
            "***** Experiments on the Friedman #1 dataset using rbf (gamma=1) kernel ridge regression model with weight decay of 0.0001 *****\n",
            "Results for Scikit-learn model\n",
            "Training set mean squared error: 0.0000\n",
            "Validation set mean squared error: 1.0327\n",
            "Testing set mean squared error: 1.2685\n",
            "Results for our model\n",
            "Training set mean squared error: 0.0000\n",
            "Validation set mean squared error: 3.4797\n",
            "Testing set mean squared error: 4.5286\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "diabetes_weight_decay = 1e-2\n",
        "diabetes_degree = 3\n",
        "diabetes_gamma = 1\n",
        "\n",
        "friedman1_weight_decay = 1e-4\n",
        "friedman1_degree = 3\n",
        "friedman1_gamma = 1\n",
        "\n",
        "dataset_hyperparameters = [\n",
        "    # For diabetes dataset\n",
        "    [\n",
        "        diabetes_weight_decay,\n",
        "        diabetes_degree,\n",
        "        diabetes_gamma\n",
        "    ],\n",
        "    # For Friedman #1 dataset\n",
        "    [\n",
        "        friedman1_weight_decay,\n",
        "        friedman1_degree,\n",
        "        friedman1_gamma\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Zip up all dataset options\n",
        "dataset_options = zip(\n",
        "    datasets,\n",
        "    dataset_names,\n",
        "    dataset_hyperparameters)\n",
        "\n",
        "\n",
        "for options in dataset_options:\n",
        "\n",
        "    # Unpack dataset options\n",
        "    dataset, \\\n",
        "        dataset_name, \\\n",
        "        dataset_hyperparameters = options\n",
        "\n",
        "    weight_decay, degree, gamma = dataset_hyperparameters\n",
        "\n",
        "    '''\n",
        "    Create the training, validation and testing splits\n",
        "    '''\n",
        "    if dataset_name == 'Friedman #1':\n",
        "        X, y = dataset\n",
        "    else:\n",
        "        X = dataset.data\n",
        "        y = dataset.target\n",
        "\n",
        "\n",
        "    print('Preprocessing the {} dataset ({} samples, {} feature dimensions)'.format(dataset_name, X.shape[0], X.shape[1]))\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(X.shape[0])\n",
        "\n",
        "    # Choose the first 60% as training set, next 20% as validation and the rest as testing\n",
        "    train_split_idx = int(0.60 * X.shape[0])\n",
        "    val_split_idx = int(0.80 * X.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:val_split_idx]\n",
        "    test_indices = shuffled_indices[val_split_idx:]\n",
        "\n",
        "    # Select the examples from X and y to construct our training, validation, testing sets\n",
        "    X_train, y_train = X[train_indices, :], y[train_indices]\n",
        "    X_val, y_val = X[val_indices, :], y[val_indices]\n",
        "    X_test, y_test = X[test_indices, :], y[test_indices]\n",
        "\n",
        "    for kernel in ['linear', 'polynomial', 'rbf']:\n",
        "\n",
        "        '''\n",
        "        Trains and tests kernel ridge regression model for different kernels\n",
        "        '''\n",
        "\n",
        "        if kernel == 'linear':\n",
        "            print('***** Experiments on the {} dataset using {} kernel ridge regression model with weight decay of {} *****'.format(\n",
        "                dataset_name,\n",
        "                kernel,\n",
        "                weight_decay))\n",
        "\n",
        "            # DONE: Instantiate KernelRidgeRegressionSciKit with linear kernel\n",
        "            model_scikit = KernelRidgeRegressionSciKit(kernel=kernel, alpha=weight_decay)\n",
        "\n",
        "            # DONE: Instantiate our kernel ridge regression model with linear kernel\n",
        "            model_ours = KernelRidgeRegression(kernel)\n",
        "\n",
        "        elif kernel == 'polynomial':\n",
        "            print('***** Experiments on the {} dataset using {} (degree={}) kernel ridge regression model with weight decay of {} *****'.format(\n",
        "                dataset_name,\n",
        "                kernel,\n",
        "                degree,\n",
        "                weight_decay))\n",
        "\n",
        "            # DONE: Instantiate KernelRidgeRegressionSciKit with a polynomial kernel with specified degree and gamma of 1\n",
        "            model_scikit = KernelRidgeRegressionSciKit(kernel=kernel, degree=degree, gamma=1, alpha=weight_decay)\n",
        "\n",
        "            # DONE: Instantiate our kernel ridge regression model with polynomial kernel with specified degree\n",
        "            model_ours = KernelRidgeRegression(kernel, degree=degree)\n",
        "\n",
        "        elif kernel == 'rbf':\n",
        "            print('***** Experiments on the {} dataset using {} (gamma={}) kernel ridge regression model with weight decay of {} *****'.format(\n",
        "                dataset_name,\n",
        "                kernel,\n",
        "                gamma,\n",
        "                weight_decay))\n",
        "\n",
        "            # DONE: Instantiate KernelRidgeRegressionSciKit with an rbf kernel. Please choose gamma for Scikit. Note: Scikit implementation using gamma is as follows: 1 / (2 * specified gamma ** 2)\n",
        "            model_scikit = KernelRidgeRegressionSciKit(kernel=kernel, gamma=1 / (2 * gamma ** 2), alpha=weight_decay)   \n",
        "\n",
        "            # DONE: Instantiate our kernel ridge regression model with an rbf kernel with specified gamma\n",
        "            model_ours = KernelRidgeRegression(kernel, gamma=gamma)\n",
        "        else:\n",
        "            raise ValueError('Unsupported kernel function: {}'.format(kernel))\n",
        "\n",
        "        print('Results for Scikit-learn model')\n",
        "\n",
        "        # DONE: Train scikit-learn model\n",
        "        model_scikit.fit(X_train, y_train)\n",
        "\n",
        "        # DONE: Score model using mean squared error on training set\n",
        "        mse_scikit_train = skmetrics.mean_squared_error(y_train, model_scikit.predict(X_train))\n",
        "        print('Training set mean squared error: {:.4f}'.format(mse_scikit_train))\n",
        "\n",
        "        # DONE: Score model using mean squared error validation set\n",
        "        mse_scikit_val = skmetrics.mean_squared_error(y_val, model_scikit.predict(X_val))\n",
        "        print('Validation set mean squared error: {:.4f}'.format(mse_scikit_val))\n",
        "\n",
        "        # DONE: Score model using mean squared error testing set\n",
        "        mse_scikit_test = skmetrics.mean_squared_error(y_test, model_scikit.predict(X_test))\n",
        "        print('Testing set mean squared error: {:.4f}'.format(mse_scikit_test))\n",
        "\n",
        "        print('Results for our model')\n",
        "\n",
        "        # DONE: Train our model\n",
        "        model_ours.fit(X_train, y_train, weight_decay)\n",
        "\n",
        "        # DONE: Score model using mean squared error on training set\n",
        "        mse_ours_train = skmetrics.mean_squared_error(y_train, model_ours.predict(X_train)) \n",
        "        print('Training set mean squared error: {:.4f}'.format(mse_ours_train))\n",
        "\n",
        "        # DONE: Score model using mean squared error validation set\n",
        "        mse_ours_val = skmetrics.mean_squared_error(y_val, model_ours.predict(X_val))\n",
        "        print('Validation set mean squared error: {:.4f}'.format(mse_ours_val))\n",
        "\n",
        "        # DONE: Score model using mean squared error testing set\n",
        "        mse_ours_test = skmetrics.mean_squared_error(y_test, model_ours.predict(X_test))\n",
        "        print('Testing set mean squared error: {:.4f}'.format(mse_ours_test))\n",
        "\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTdMEJQ1ZbVh"
      },
      "source": [
        "Comparing run time for polynomial kernel and polynomial feature expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9Vy7mK-FYl1A"
      },
      "outputs": [],
      "source": [
        "# Define weight decay and polynomial degrees\n",
        "weight_decay = 1\n",
        "\n",
        "degrees = [\n",
        "    2, 3, 4, 5\n",
        "]\n",
        "\n",
        "# Lists to hold time elapsed for\n",
        "times_elapsed_poly_expand = []\n",
        "times_elapsed_poly_kernel = []\n",
        "\n",
        "# Select Friedman #1 dataset\n",
        "dataset = skdata.make_friedman1(n_samples=5000, n_features=20, noise=1.0, random_state=1)\n",
        "\n",
        "X, y = dataset\n",
        "\n",
        "for degree in degrees:\n",
        "\n",
        "    # DONE: Initialize polynomial expansion\n",
        "    poly_transform = skpreprocess.PolynomialFeatures(degree=degree, include_bias=False)\n",
        "\n",
        "    # DONE: Compute the polynomial terms needed for the data\n",
        "    poly_transform.fit(X)\n",
        "\n",
        "\n",
        "    # DONE: Transform the data by nonlinear mapping\n",
        "    X_poly = poly_transform.transform(X)\n",
        "\n",
        "    # DONE: Initialize sci-kit ridge regression model\n",
        "    model_poly_expand = RidgeRegressionSciKit(alpha=weight_decay)\n",
        "\n",
        "    print('Training ridge regression model with degree {} polynomial expansion with {} samples, {} feature dimensions'.format(\n",
        "        degree,\n",
        "        X_poly.shape[0],\n",
        "        X_poly.shape[1]))\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    # DONE: Train sci-kit ridge regression model on polynomial expanded X\n",
        "    model_poly_expand.fit(X_poly, y)\n",
        "    time_elapsed_poly_expand = 1000 * (time.time() - time_start)\n",
        "\n",
        "    print('Training time: {:.2f}ms'.format(time_elapsed_poly_expand))\n",
        "\n",
        "    # DONE: Append training time to list of time elapsed for polynomial feature expansion\n",
        "    times_elapsed_poly_expand.append(time_elapsed_poly_expand)\n",
        "\n",
        "    # DONE: Initialize our polynomial kernel ridge regression model\n",
        "    model_poly_kernel = KernelRidgeRegression(kernel_func='polynomial', degree=degree)\n",
        "\n",
        "    print('Training kernel ridge regression model with degree {} polynomial with {} samples, {} feature dimensions'.format(\n",
        "        degree,\n",
        "        X.shape[0],\n",
        "        X.shape[1]))\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    # DONE: Train our polynomial kernel ridge regression model on X\n",
        "    model_poly_kernel.fit(X, y, weight_decay)\n",
        "    time_elapsed_poly_kernel = 1000 * (time.time() - time_start)\n",
        "\n",
        "    print('Training time: {:.2f}ms'.format(time_elapsed_poly_kernel))\n",
        "\n",
        "    # DONE: Append training time to list of time elapsed for polynomial kernel\n",
        "    times_elapsed_poly_kernel.append(time_elapsed_poly_kernel)\n",
        "\n",
        "    print('')\n",
        "\n",
        "\n",
        "# Create figure for training, validation and testing scores for different features\n",
        "labels = ['Polynomial Expansion', 'Polynomial Kernel']\n",
        "colors = ['blue', 'red']\n",
        "\n",
        "# DONE: Create a subplot of a 1 by 1 figure to plot MSE for training and testing\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "# DONE: Set x values (polynomial degree) and y values (time in ms in log scale)\n",
        "x_values = [degrees]\n",
        "y_values = [times_elapsed_poly_expand, times_elapsed_poly_kernel]\n",
        "\n",
        "# DONE: Plot MSE scores for training and testing sets\n",
        "# Set x limits between 1 to 1 + maximum value of all degrees and y limits between 1 and 1 + maximum value of all log time in ms\n",
        "# Set x label to degree and y label to milliseconds (log scale)' and y label to 'MSE',\n",
        "ax.set_xlim([1, 1 + max(degrees)])\n",
        "ax.set_ylim([1, 1 + max(np.log(times_elapsed_poly_expand))])\n",
        "ax.set_xlabel('degree')\n",
        "ax.set_ylabel('milliseconds (log scale)')\n",
        "\n",
        "# DONE: Create plot title of 'Comparing Polynomial Feature Expansion and Polynomial Kernel Run-times'\n",
        "ax.set_title('Comparing Polynomial Feature Expansion and Polynomial Kernel Run-times')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
