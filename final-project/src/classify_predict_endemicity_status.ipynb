{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a91bc",
   "metadata": {},
   "source": [
    "# Beyond sporadic outbreaks: Classifying and explaining dengue endemicity over scenarios of global change.\n",
    "\n",
    "*CPSC 581: Machine Learning*\n",
    "\n",
    "*Yale University*\n",
    "\n",
    "*Instructor: Alex Wong*\n",
    "\n",
    "*Student: Hailey Robertson*\n",
    "\n",
    "## Setup:\n",
    "\n",
    "1. Enable Google Colaboratory as an app on your Google Drive account\n",
    "\n",
    "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
    "```\n",
    "/content/drive/MyDrive/Colab Notebooks\n",
    "```\n",
    "\n",
    "3. Create the following directory structure in your Google Drive\n",
    "```\n",
    "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Final project\n",
    "```\n",
    "\n",
    "4. Move the classify_predict_endemicity.ipynb and requirements.txt into\n",
    "```\n",
    "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Final project/src\n",
    "```\n",
    "so that its absolute path is\n",
    "```\n",
    "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Final project/src/classify_predict_endemicity.ipynb\n",
    "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Final project/src/requirements.txt\n",
    "\n",
    "```\n",
    "\n",
    "5. Prior to starting this project, please create a directory called 'data' within your 'Final project' directory with all files from the unzipped 'data' folder\n",
    "```\n",
    "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/data\n",
    "```\n",
    "\n",
    "6. Prior to starting this project, ensure there is a directory called 'output' within your 'Final project' directory with 'data' and 'figures' subfolders (unzipping the 'output' folder works to create this structure)\n",
    "```\n",
    "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/output\n",
    "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/output/data\n",
    "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/output/figures\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d705de",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04284350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import auth\n",
    "from google.auth import default\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive/', force_remount=True)\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Final project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b61e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If running outside Colab (e.g., in terminal, VSCode) - path should not be a string\n",
    "# Only uncomment if needed - takes a long time to run and doesn't really work with Colab\n",
    "# !pip install -r '/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Final project/src/requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7b335",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1065aeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: country_converter in /Users/haileyrobertson/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages (1.3)\n",
      "Requirement already satisfied: pandas>=1.0 in /Users/haileyrobertson/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages (from country_converter) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/haileyrobertson/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages (from pandas>=1.0->country_converter) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/haileyrobertson/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/haileyrobertson/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages (from pandas>=1.0->country_converter) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/haileyrobertson/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages (from pandas>=1.0->country_converter) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/haileyrobertson/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skmetrics\n",
    "import sklearn.preprocessing as skpreprocessing\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Ridge\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import shap\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, TwoSlopeNorm\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon \n",
    "!pip install country_converter --upgrade # Not a default for Colab\n",
    "import country_converter as coco\n",
    "\n",
    "# Other\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*findfont.*Open Sans.*\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d043a2",
   "metadata": {},
   "source": [
    "#### Nice defaults for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283e851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAABlCAYAAAB5uH+EAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABIdJREFUeJzt3MFOY2UcxuHTCm0HBBJijKntChPdGXdzCbMxXIhr3XIvXoyrWXgByoJmuAA6ZZAWjimTMa7G8x2GfL4zz7Pppgn/vEDDLwUGbdu2DQAAAIQa1j4AAAAAHkPYAgAAEE3YAgAAEE3YAgAAEE3YAgAAEE3YAgAAEE3YAgAAEG2ny5Pu7++by8vL5uDgoBkMBk9/FQAAAJ+0tm2b5XLZTKfTZjgcPj5st1E7n88/1H0AAADQyWKxaGaz2ePDdvtO7daXP/7cDHfH3T46zey7Se0TIr344ZvaJ8Q5//PX2idEevF9p5dA/uX56PPaJ0S6e/mq9glxXv/+Re0TIr189XXtE+L8tt7UPiHSt1/9UfuEOM9/+qX2CXFWq1Vzenr6T4++T6ef6t79+vE2aoe7Yq2rnYmt+pjs7dU+Ic5oslv7hEh7+8K21MF4VPuESHfPfK0VG3ld6+PZjjcgSo3az2qfEGmy63u01P7+fu0TYnX5c1j/PAoAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBoO12e1Lbtw+P9+q+nvuejsrmpfUGmm+vr2ifEub1Z1z4h0vXq7Wsb3S3Xt7VPiHT3ZlP7hDivb72u9fFm42e1Urcb35993Kx9j5ZarVa1T4jd7F2Pvs+g7fCs8/Pz5uTk5MNcBwAAAB0tFotmNps9/h3b4+Pjh8eLi4vm6Oio68f/5F1dXTXz+fzhE3F4eFj7nAg268du5WzWj93K2awfu5WzWT92K2ezfuxWbvse7HK5bKbT6X8+t1PYDodv/xR3G7U+CeW2m9mtjM36sVs5m/Vjt3I268du5WzWj93K2awfu5Xp+saqfx4FAABANGELAADAxx+24/G4OTs7e3ikO7uVs1k/ditns37sVs5m/ditnM36sVs5m/Vjt6fV6b8iAwAAwP+VX0UGAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAACgSfY3CMHKKCI1i9QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAABlCAYAAAArpKpSAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAqBJREFUeJzt2r1q22AYhuHPdpossQPZ6siTSelSCF17Op0KPYacaYYYcgCWp5RERe4PdEgeOXZRRK5r0WAZXl7BjWR51DRNUwB40vjpjwBoCSVAIJQAgVACBEIJEAglQCCUAMFR6eDx8bHc3d2V6XRaRqNRl68AvGrtX8jrui7z+byMx+P9Q9lGcrFYHGo+gFdjtVqVqqr2D2V7J9n6cHlZJpPJYaZ7Az5efel7hEH6/u1r3yMMTnXxvu8RBmdT1+Xz1ae/fds7lH8et9tICmV3746P+x5hkE5PT/seYXCm01nfIwxWl58TvcwBCIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECI5KB03TbI8PDw9dTue3H/f3fY8wSJvNpu8RBqeu132PMDibuv6nb88ZNR3Ourm5Kcvl8jDTAbwiq9WqVFW1/x3l+fn59nh7e1vOzs4OM90bsF6vy2Kx2F6I2WzW9ziDYGcvY2+7a+8R67ou8/k8ntsplOPxr58y20i6CLtrd2Zvu7Gzl7G33XS98fMyByAQSoBDhPLk5KRcX19vj3Rnb7uzs5ext/+r01tvgLfMozdAIJQAgVACBEIJEAglQCCUAIFQAgRCCVCe9xO822NQ1qjWWgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['#1A5784', '#38828C', '#84B5B2', '#60904F', '#ACC253', '#E7C960', '#E49243', '#D75F58', '#A04275', '#633D71', '#8c564b', '#c7c7c7']\n",
    "sns.palplot(sns.color_palette(colors))\n",
    "\n",
    "# Define chart color palette\n",
    "chart = ['#2C2B2B','#565E69','#CACED3','#E7EAEE']\n",
    "sns.palplot(sns.color_palette(chart))\n",
    "\n",
    "# Define constants\n",
    "figure_size = (20,6)\n",
    "\n",
    "# Set background\n",
    "sns.set_context('talk') #change the size from small to medium\n",
    "sns.set_style('white') #change bg to white\n",
    "    \n",
    "# Set font family globally\n",
    "plt.rcParams['font.family'] = 'Open Sans'\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "\n",
    "# Set margins\n",
    "plt.rcParams['axes.xmargin'] = 0.01\n",
    "plt.rcParams['axes.ymargin'] = 0.01\n",
    "\n",
    "# Define list of date formats\n",
    "zfmts = ['', '%Y','%b\\n%Y', '%b', '%b-%d', '%H:%M', '%H:%M']\n",
    "\n",
    "# Format axes \n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.bottom'] = True\n",
    "plt.rcParams['axes.spines.left'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.titlepad'] = 10\n",
    "\n",
    "# Format ticks\n",
    "plt.rcParams[\"xtick.direction\"] = \"out\"\n",
    "plt.rcParams['xtick.major.size'] = 10\n",
    "plt.rcParams['xtick.minor.size'] = 10\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['xtick.color'] = chart[2]\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['xtick.labelcolor'] = chart[1]\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.major.pad\"] = 0\n",
    "plt.rcParams[\"ytick.minor.pad\"] = 0\n",
    "plt.rcParams[\"ytick.major.size\"] = 10\n",
    "plt.rcParams[\"ytick.minor.size\"] = 10\n",
    "plt.rcParams[\"ytick.color\"] = chart[2]\n",
    "plt.rcParams[\"ytick.major.width\"] = 0.1\n",
    "plt.rcParams[\"ytick.minor.width\"] = 0.1\n",
    "plt.rcParams[\"ytick.labelcolor\"] = chart[1]\n",
    "plt.rcParams[\"ytick.labelsize\"] = 8\n",
    "\n",
    "\n",
    "# Adjust fontdict for title\n",
    "titlefont = {'family': 'Open Sans',\n",
    "             'color':  chart[0], \n",
    "             'weight': 400,\n",
    "             'size': 20}\n",
    "\n",
    "# Set grid style\n",
    "plt.rcParams['grid.color'] = chart[2]\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth']=0.5\n",
    "\n",
    "# Set legend style\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['legend.handlelength'] = 1\n",
    "plt.rcParams['legend.handleheight'] = 1.125\n",
    "\n",
    "\n",
    "\n",
    "# Set axis labels\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.labelcolor'] = chart[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d2104",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be54bd8",
   "metadata": {},
   "source": [
    "#### Load Open Dengue and align country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load dengue data ---\n",
    "url = 'https://raw.githubusercontent.com/OpenDengue/master-repo/refs/heads/main/data/raw_data/masterDB_V1.2.csv'\n",
    "open_dengue = pd.read_csv(url, index_col=0, encoding='latin-1').reset_index()\n",
    "\n",
    "open_dengue[\"adm_0_iso3\"] = coco.convert(\n",
    "    names=open_dengue[\"adm_0_name\"],\n",
    "    to='ISO3',\n",
    "    not_found=\"missing\"\n",
    ")\n",
    "\n",
    "date_cols = ['calendar_start_date', 'calendar_end_date']\n",
    "open_dengue[date_cols] = open_dengue[date_cols].apply(pd.to_datetime)\n",
    "open_dengue['year'] = open_dengue['calendar_start_date'].dt.year\n",
    "open_dengue['month'] = open_dengue['calendar_start_date'].dt.month\n",
    "\n",
    "#  Not all periods are the same length – some places report every year, some every month, some every week\n",
    "open_dengue[\"date_diff\"] = (open_dengue[\"calendar_end_date\"] - open_dengue[\"calendar_start_date\"]).dt.days\n",
    "\n",
    "# --- Load world geometry ---\n",
    "world = gpd.read_file(\"data/ne_110m_admin_0_countries\")\n",
    "\n",
    "world = world.rename(columns={\n",
    "    \"ADM0_A3\": \"adm_0_iso3\",\n",
    "    \"ADMIN\": \"adm_0_name\",\n",
    "    \"REGION_UN\": \"region_un\",\n",
    "    \"geometry\": \"adm_0_geometry\"\n",
    "})[[\"adm_0_iso3\", \"adm_0_name\", \"region_un\", \"adm_0_geometry\"]]\n",
    "\n",
    "world = world.sort_values(by=\"adm_0_name\")\n",
    "\n",
    "# --- Fix known issues ---\n",
    "# Split out French Guiana from France\n",
    "france_idx = world['adm_0_name'] == 'France'\n",
    "france_geom = world.loc[france_idx, 'adm_0_geometry'].values[0]\n",
    "\n",
    "if isinstance(france_geom, MultiPolygon):\n",
    "    polygons = list(france_geom.geoms)\n",
    "    french_guiana_polygon = next((poly for poly in polygons if poly.bounds[0] < -50 and poly.bounds[2] > -54), None)\n",
    "\n",
    "    if french_guiana_polygon:\n",
    "        # Remove French Guiana from France\n",
    "        remaining_polygons = [poly for poly in polygons if poly != french_guiana_polygon]\n",
    "        world.loc[france_idx, 'adm_0_geometry'] = MultiPolygon(remaining_polygons)\n",
    "\n",
    "        # Add French Guiana as separate entry\n",
    "        french_guiana_row = {\n",
    "            'adm_0_iso3': 'GUF',\n",
    "            'adm_0_name': 'French Guiana',\n",
    "            'region_un': 'Americas',\n",
    "            'adm_0_geometry': french_guiana_polygon\n",
    "        }\n",
    "        world = pd.concat([world, gpd.GeoDataFrame([french_guiana_row], geometry='adm_0_geometry')], ignore_index=True)\n",
    "\n",
    "# Patch ISO3 codes for special cases\n",
    "world.loc[world['adm_0_name'] == 'Norway', 'adm_0_iso3'] = 'NOR'\n",
    "world.loc[world['adm_0_name'] == 'Somaliland', 'adm_0_iso3'] = 'SOM'\n",
    "world.loc[world['adm_0_name'] == 'Kosovo', 'adm_0_iso3'] = 'RKS'\n",
    "world.loc[world['adm_0_name'] == 'South Sudan', 'adm_0_iso3'] = 'SSD'\n",
    "\n",
    "# --- Merge dengue data with geometry ---\n",
    "dengue = pd.merge(open_dengue, world, on='adm_0_iso3', how='outer', suffixes=('', '_world'))\n",
    "\n",
    "# Fill UN regions with mapping\n",
    "with open('data/un_regions.json') as f:\n",
    "    countries = json.load(f)\n",
    "dengue['region_un'] = dengue['adm_0_iso3'].map(countries).fillna(\"Other\")\n",
    "\n",
    "# Aesthetics\n",
    "dengue = dengue.drop(columns=['adm_0_name_world'])\n",
    "front_cols = ['adm_0_name', 'adm_0_iso3']\n",
    "geometry_col = ['adm_0_geometry']\n",
    "other_cols = [col for col in dengue.columns if col not in front_cols + geometry_col]\n",
    "dengue = dengue[front_cols + other_cols + geometry_col]\n",
    "\n",
    "# Fill the years so that ISO3s stay as a time series\n",
    "years = pd.Series(range(int(1950), int(dengue['year'].max()) + 1))\n",
    "unique_iso3 = dengue['adm_0_iso3'].unique()\n",
    "\n",
    "all_combinations = pd.MultiIndex.from_product([unique_iso3, years], names=['adm_0_iso3', 'year']).to_frame(index=False)\n",
    "\n",
    "dengue_ts = all_combinations.merge(dengue, on=['adm_0_iso3', 'year'], how='outer')\n",
    "\n",
    "# List of columns to fill\n",
    "columns_to_fill = ['adm_0_name', 'region_un', 'adm_0_geometry']\n",
    "\n",
    "# Fill NaN values for multiple columns\n",
    "dengue_ts[columns_to_fill] = (\n",
    "    dengue_ts\n",
    "    .groupby('adm_0_iso3')[columns_to_fill]\n",
    "    .transform(lambda x: x.ffill().bfill())\n",
    ")\n",
    "\n",
    "dengue_ts = dengue_ts.dropna(subset=['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcf0ee",
   "metadata": {},
   "source": [
    "#### Load population first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean population data ---\n",
    "pop = pd.read_csv(\"data/WPP2022_Demographic_Indicators_Medium.csv\", dtype={'Time': int,'TPopulation1July':float}, low_memory=False)\n",
    "columns = ['ISO3_code','Time','TPopulation1July']\n",
    "pop = pop[columns]\n",
    "pop.rename(columns={\"ISO3_code\": \"adm_0_iso3\", \"Time\": \"year\", \"TPopulation1July\": \"TPopulation1July_per_1000\"}, inplace=True)\n",
    "pop['year'] = pd.to_numeric(pop['year'], errors='coerce').astype('Int64')\n",
    "dengue_ts = dengue_ts.merge(pop, how='outer', on=['adm_0_iso3', 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831a441",
   "metadata": {},
   "source": [
    "#### Calculate incidence rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e85374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = dengue_ts.groupby(['adm_0_iso3', \n",
    "                                'year',\n",
    "                                'region_un',\n",
    "                                'TPopulation1July_per_1000'\n",
    "                                ]).agg({\n",
    "    'dengue_total': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "grouped_df['TPopulation1July'] = grouped_df['TPopulation1July_per_1000'] * 1000\n",
    "grouped_df['incidence'] = grouped_df['dengue_total'] / grouped_df['TPopulation1July']\n",
    "grouped_df['incidence_per_100k'] = grouped_df['incidence'] * 100000\n",
    "grouped_df['log_incidence_per_100k'] = np.log1p(grouped_df['incidence_per_100k'])  # log(x + 1) to handle zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab822c",
   "metadata": {},
   "source": [
    "#### Load other predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e874ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean population density ---\n",
    "pop_dns = pd.read_csv(\"data/pop_density_wb.csv\")\n",
    "year_cols = [col for col in pop_dns.columns if col.isdigit()]\n",
    "pop_dns = pop_dns.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"pop_density_per_km2\"\n",
    ")\n",
    "pop_dns.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "\n",
    "# --- Clean climate data ---\n",
    "clim = ['era5_tnn_annual_min_temp.csv', 'era5_tas_annual_mean_temp.csv', 'era5_pr_annual_mean_precipitation.csv', 'era5_hur_annual_mean_relative_humidity.csv']\n",
    "\n",
    "climate_df = {}\n",
    "\n",
    "for file in clim:\n",
    "    data = pd.read_csv(f\"data/{file}\")\n",
    "    \n",
    "    year_cols = [col for col in data.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "    melted = data.melt(\n",
    "        id_vars=[\"code\"],\n",
    "        value_vars=year_cols,\n",
    "        var_name=\"year\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    melted[\"year\"] = melted[\"year\"].str.extract(r\"(\\d{4})\")\n",
    "    melted.rename(columns={\"code\": \"adm_0_iso3\"}, inplace=True)\n",
    "    indicator = file.replace('.csv', '')  \n",
    "    melted.rename(columns={\"value\": indicator}, inplace=True)\n",
    "    climate_df[indicator] = melted\n",
    "\n",
    "# --- Clean urbanization data ---\n",
    "urban = pd.read_csv('data/urban_growth_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in urban.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "urban = urban.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"urban_growth\"\n",
    ")\n",
    "urban.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "urban['year'] = urban['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean urban pop data ---\n",
    "u_pop = pd.read_csv(\"data/urban_pop_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in u_pop.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "u_pop = u_pop.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"urban_pop\"\n",
    ")\n",
    "u_pop.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "u_pop['year'] = urban['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean GDP data ---\n",
    "gdp = pd.read_csv(\"data/gdp_ppp_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in gdp.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "gdp = gdp.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"gdp_ppp\"\n",
    ")\n",
    "gdp.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "gdp['year'] = gdp['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean arrivals data ---\n",
    "arr = pd.read_csv(\"data/tourism_arrivals_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in arr.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "arr = arr.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"tourism_arrivals\"\n",
    ")\n",
    "arr.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "arr['year'] = arr['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean departures data ---\n",
    "dep = pd.read_csv(\"data/tourism_departures_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in dep.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "dep = dep.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"tourism_departures\"\n",
    ")\n",
    "dep.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "dep['year'] = dep['year'].str.extract(r\"(\\d{4})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['adm_0_iso3', 'year', 'region_un', 'TPopulation1July_per_1000',\n",
      "       'dengue_total', 'TPopulation1July', 'incidence', 'incidence_per_100k',\n",
      "       'log_incidence_per_100k', 'pop_density_per_km2',\n",
      "       'era5_hur_annual_mean_relative_humidity',\n",
      "       'era5_pr_annual_mean_precipitation', 'era5_tas_annual_mean_temp',\n",
      "       'era5_tnn_annual_min_temp', 'urban_pop', 'gdp_ppp', 'tourism_arrivals',\n",
      "       'tourism_departures'],\n",
      "      dtype='object')\n",
      "(15549, 18)\n"
     ]
    }
   ],
   "source": [
    "predictor_dfs = [pop_dns, \n",
    "                 climate_df['era5_hur_annual_mean_relative_humidity'],\n",
    "                 climate_df['era5_pr_annual_mean_precipitation'],\n",
    "                 climate_df['era5_tas_annual_mean_temp'],\n",
    "                 climate_df['era5_tnn_annual_min_temp'],\n",
    "                 u_pop,\n",
    "                 gdp,\n",
    "                 arr,\n",
    "                 dep]\n",
    "\n",
    "merged_df = grouped_df.copy()\n",
    "\n",
    "for i, predictor in enumerate(predictor_dfs):\n",
    "    predictor['year'] = pd.to_numeric(predictor['year'], errors='coerce').astype('Int64')\n",
    "    predictor = predictor.dropna(subset=['adm_0_iso3', 'year'])\n",
    "    merged_df = merged_df.merge(predictor, how='outer', on=['adm_0_iso3', 'year'])\n",
    "\n",
    "df = merged_df.replace('..', np.nan)\n",
    "df = df[(df['year'] >= 1950) & (df['year'] <= 2022)]\n",
    "df = df.dropna(subset=['region_un', 'adm_0_iso3', 'year'])\n",
    "df.drop_duplicates(subset=['adm_0_iso3', 'year'], inplace=True)\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n",
    "df.to_csv('output/data/cleaned_master_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2681f",
   "metadata": {},
   "source": [
    "##### View data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3443e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "heatmap_data = df.pivot(index='adm_0_iso3', columns='year', values='incidence_per_100k').fillna(0)\n",
    "print(len(heatmap_data))\n",
    "rocket_r_cmap = sns.color_palette(\"rocket_r\", as_cmap=True)\n",
    "custom_cmap = ListedColormap([\"white\"] + list(rocket_r_cmap(np.linspace(0.01, 1, 1000))))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 16))  \n",
    "\n",
    "custom_colors = [\"white\", colors[1]]\n",
    "\n",
    "im = sns.heatmap(data=heatmap_data, \n",
    "                 cmap=custom_cmap, \n",
    "                 annot=False, \n",
    "                 fmt='g', \n",
    "                 linewidths=0.1, \n",
    "                 ax=ax, \n",
    "                 vmin=0, \n",
    "                 vmax=1000, \n",
    "                 cbar=True, \n",
    "                 cbar_kws={'orientation': 'horizontal', 'pad': 0.04, 'label':'Incidence rate per 100,000'})\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Country', fontsize=12)\n",
    "ax.set_title(\"Incidence rate per 100,000 over time\",loc='left', fontsize=14)\n",
    "\n",
    "ax.tick_params(axis='x', pad=5, length=0, labelsize=6, width=10)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "ax.set_yticks(np.arange(len(heatmap_data.index)))\n",
    "ax.set_yticklabels(heatmap_data.index, fontsize=6)\n",
    "\n",
    "ax.tick_params(top=False, bottom=True, labeltop=False, labelbottom=True)\n",
    "\n",
    "ax.spines.bottom.set_visible(False)\n",
    "ax.spines.left.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = f\"output/figures/open_dengue_incidence_1950-2023_full_covg.png\"\n",
    "plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17857d12",
   "metadata": {},
   "source": [
    "## Aim 1: Classify countries by endemicity status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f5559",
   "metadata": {},
   "source": [
    "### Check multicollinearity with VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables have VIF < 5\n"
     ]
    }
   ],
   "source": [
    "# --- Load cleaned data ---\n",
    "df = pd.read_csv('output/data/cleaned_master_df.csv')\n",
    "\n",
    "# --- VIF filtering ---\n",
    "features = {\n",
    "    'era5_tas_annual_mean_temp': 'Mean temperature (°C)',\n",
    "    'era5_pr_annual_mean_precipitation': 'Mean precipitation (mm)',\n",
    "    'era5_hur_annual_mean_relative_humidity': 'Mean relative humidity (%)',\n",
    "    'urban_pop': 'Urban population (%)',\n",
    "    'gdp_ppp': 'GDP per capita (PPP)',\n",
    "    'tourism_arrivals': 'International tourism arrivals',\n",
    "    'tourism_departures': 'International tourism departures',\n",
    "    'pop_density_per_km2': 'Population density (people/km²)',\n",
    "    'TPopulation1July': 'Population size',\n",
    "    'log_incidence_per_100k':'Log incidence rate'\n",
    "}\n",
    "\n",
    "df_features = df[list(features.keys())].dropna().rename(columns=features)\n",
    "\n",
    "# --- VIF Filtering Function ---\n",
    "def calculate_vif(df, thresh=5):\n",
    "    variables = df.copy()\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"variable\"] = variables.columns\n",
    "        vif[\"VIF\"] = [variance_inflation_factor(add_constant(variables).values, i + 1)\n",
    "                      for i in range(variables.shape[1])]\n",
    "        max_vif = vif[\"VIF\"].max()\n",
    "        if max_vif > thresh:\n",
    "            drop_var = vif.sort_values(\"VIF\", ascending=False)[\"variable\"].iloc[0]\n",
    "            print(f\"Dropping '{drop_var}' with VIF = {max_vif:.2f}\")\n",
    "            variables = variables.drop(columns=[drop_var])\n",
    "            dropped = True\n",
    "        else:\n",
    "            print(f\"All variables have VIF < {thresh}\")\n",
    "    return variables\n",
    "\n",
    "df_features_cleaned = calculate_vif(df_features)\n",
    "cleaned_columns = df_features_cleaned.columns.tolist()\n",
    "\n",
    "corr_matrix = df_features_cleaned.corr()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=False,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "\n",
    "plt.title(\"Correlation Matrix\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/figures/correlation_matrix.png\", dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633f327",
   "metadata": {},
   "source": [
    "### Cluster and plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_cluster(df, method='kmeans', n_clusters=3, random_state=42):\n",
    "    scaler = skpreprocessing.StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    if method == 'kmeans':\n",
    "        model = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    elif method == 'meanshift':\n",
    "        model = MeanShift()\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'kmeans' or 'meanshift'\")\n",
    "\n",
    "    clusters = model.fit_predict(X_scaled)\n",
    "    sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "    print(f\"{method.capitalize()} Silhouette Score: {sil_score:.3f}\")\n",
    "    \n",
    "    return scaler, model, clusters, X_scaled\n",
    "\n",
    "def plot_pca_clusters(X_scaled, cluster_labels, title, save_path, palette):\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=cluster_labels, palette=palette, s=50, legend='full')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.legend(title='Cluster', loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def apply_model_to_full_data(df_full, cleaned_columns, scaler, model, cluster_col_name):\n",
    "    df_imputed = df_full[cleaned_columns].copy()\n",
    "    df_imputed = df_imputed.fillna(df_imputed.mean())\n",
    "    X_scaled = scaler.transform(df_imputed)\n",
    "    cluster_labels = model.predict(X_scaled)\n",
    "    df_imputed[cluster_col_name] = cluster_labels\n",
    "    return df_imputed\n",
    "\n",
    "def save_cluster_labels(df, path):\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def plot_cluster_timeline(\n",
    "    df,\n",
    "    cluster_col='kmeans_cluster',\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    size='log_incidence_per_100k',\n",
    "    title=\"Cluster membership over time by country\",\n",
    "    legend_labels=None,\n",
    "    palette=None,\n",
    "    save_path=\"timeline_cluster_plot.png\",\n",
    "    figsize=(20, 30),\n",
    "    dpi=300,\n",
    "    edgecolor='k'\n",
    "):\n",
    "    if palette is None:\n",
    "        unique_clusters = sorted(df[cluster_col].dropna().unique())\n",
    "        default_colors = sns.color_palette(colors, len(unique_clusters))\n",
    "        palette = {k: default_colors[i] for i, k in enumerate(unique_clusters)}\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    scatter = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=cluster_col,\n",
    "        size=size,\n",
    "        sizes=(1, 200),\n",
    "        palette=palette,\n",
    "        alpha=0.7,\n",
    "        edgecolor=edgecolor,\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "    plt.title(title, loc='left')\n",
    "    plt.xlabel(x.capitalize())\n",
    "    plt.ylabel(y.replace(\"_\", \" \").capitalize(), labelpad=10)\n",
    "\n",
    "    # Extract legend handles\n",
    "    handles, labels = scatter.get_legend_handles_labels()\n",
    "\n",
    "    # Find where hue and size legends start\n",
    "    hue_title_idx = labels.index(cluster_col)\n",
    "    size_title_idx = labels.index(size)\n",
    "\n",
    "    hue_handles = handles[hue_title_idx + 1:size_title_idx]\n",
    "    size_handles = handles[size_title_idx + 1:]\n",
    "\n",
    "    if legend_labels:\n",
    "        hue_labels = legend_labels\n",
    "    else:\n",
    "        hue_labels = [f\"Cluster {i}\" for i in sorted(df[cluster_col].unique())]\n",
    "\n",
    "    legend1 = plt.legend(\n",
    "        handles=hue_handles,\n",
    "        labels=hue_labels,\n",
    "        title=\"Cluster\",\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        handletextpad=0.4,\n",
    "        borderaxespad=0.1,\n",
    "        alignment='left'\n",
    "    )\n",
    "\n",
    "    legend2 = plt.legend(\n",
    "        handles=size_handles,\n",
    "        labels=labels[size_title_idx + 1:],\n",
    "        title=\"Log incidence per 100k\",\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(1.02, 0.90),\n",
    "        handletextpad=0.4,\n",
    "        borderaxespad=0.1,\n",
    "        alignment='left'\n",
    "    )\n",
    "\n",
    "    plt.gca().add_artist(legend1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043814c3",
   "metadata": {},
   "source": [
    "#### K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans Silhouette Score: 0.304\n",
      "Kmeans Silhouette Score: 0.503\n",
      "Kmeans Silhouette Score: 0.917\n"
     ]
    }
   ],
   "source": [
    "# === 1. All Features ===\n",
    "df_cleaned = df_features_cleaned.copy()\n",
    "scaler, kmeans, clusters, X_scaled = scale_and_cluster(df_cleaned, method='kmeans', n_clusters=3)\n",
    "df_cleaned['kmeans_cluster'] = clusters\n",
    "\n",
    "plot_pca_clusters(\n",
    "    X_scaled, \n",
    "    clusters, \n",
    "    \"K-Means clusters in PCA space (All features)\",\n",
    "    \"output/figures/kmeans_pca_clusters_all_features.png\", \n",
    "    colors[4:7]\n",
    ")\n",
    "\n",
    "df_full_imputed = apply_model_to_full_data(\n",
    "    df_full=df[list(features.keys())].rename(columns=features), \n",
    "    cleaned_columns=features.values(), \n",
    "    scaler=scaler, \n",
    "    model=kmeans, \n",
    "    cluster_col_name='kmeans_cluster'\n",
    ")\n",
    "save_cluster_labels(df_full_imputed, \"output/data/all_features_kmeans_cluster_labels_by_country_year.csv\")\n",
    "\n",
    "\n",
    "# === 2. Incidence-Correlated Features ===\n",
    "corr_features = {\n",
    "    'era5_tas_annual_mean_temp': 'Mean temperature (°C)',\n",
    "    'era5_pr_annual_mean_precipitation': 'Mean precipitation (mm)'\n",
    "}\n",
    "df_corr = df[list(corr_features.keys())].dropna().rename(columns=corr_features)\n",
    "\n",
    "scaler, kmeans, clusters, X_scaled = scale_and_cluster(df_corr, method='kmeans', n_clusters=3)\n",
    "df_corr['kmeans_cluster'] = clusters\n",
    "\n",
    "plot_pca_clusters(\n",
    "    X_scaled, \n",
    "    clusters, \n",
    "    \"K-Means clusters in PCA space (Incidence-correlated features)\",\n",
    "    \"output/figures/kmeans_pca_clusters_incidence_correlated_features.png\", \n",
    "    colors[4:7]\n",
    ")\n",
    "\n",
    "df_with_clusters = df.loc[df_corr.index, ['adm_0_iso3', 'year']].copy()\n",
    "df_with_clusters['kmeans_cluster'] = clusters\n",
    "df_with_clusters = df.merge(df_with_clusters, on=['adm_0_iso3', 'year'], how='left')\n",
    "\n",
    "save_cluster_labels(df_with_clusters, \"output/data/incidence_correlated_kmeans_cluster_labels_by_country_year.csv\")\n",
    "\n",
    "cluster_palette = {0: colors[6], 1: colors[5], 2: colors[4]}\n",
    "legend_labels = [\"Endemic\", \"Hypo-endemic\", \"Non-endemic\"]\n",
    "\n",
    "plot_cluster_timeline(\n",
    "    df=df_with_clusters,\n",
    "    cluster_col='kmeans_cluster',\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    size='log_incidence_per_100k',\n",
    "    title=\"Cluster membership over time by country (incidence correlated features)\",\n",
    "    legend_labels=legend_labels,\n",
    "    palette=cluster_palette,\n",
    "    save_path=\"output/figures/incidence_correlated_timeline_kmeans_cluster_plot.png\",\n",
    "    dpi=1000\n",
    ")\n",
    "\n",
    "\n",
    "# === 3. Incidence Only ===\n",
    "target_col = \"log_incidence_per_100k\"\n",
    "df_inc = df[[target_col]].dropna()\n",
    "\n",
    "scaler, kmeans, clusters, X_scaled = scale_and_cluster(df_inc, method='kmeans', n_clusters=3)\n",
    "df_inc['kmeans_cluster'] = clusters\n",
    "\n",
    "df_inc_imputed = df.copy()\n",
    "df_inc_imputed[target_col] = df_inc_imputed[target_col].fillna(df_inc[target_col].mean())\n",
    "X_full_scaled = scaler.transform(df_inc_imputed[[target_col]])\n",
    "df_inc_imputed['kmeans_cluster'] = kmeans.predict(X_full_scaled)\n",
    "\n",
    "save_cluster_labels(df_inc_imputed, \"output/data/incidence_kmeans_cluster_labels_by_country_year.csv\")\n",
    "\n",
    "cluster_palette = {0: colors[6], 1: colors[4], 2: colors[5]}\n",
    "legend_labels = [\"Endemic\", \"Non-endemic\", \"Hypo-endemic\"]\n",
    "\n",
    "plot_cluster_timeline(\n",
    "    df=df_inc_imputed,\n",
    "    cluster_col='kmeans_cluster',\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    size='log_incidence_per_100k',\n",
    "    title=\"Cluster membership over time by country (incidence only)\",\n",
    "    legend_labels=legend_labels,\n",
    "    palette=cluster_palette,\n",
    "    save_path=\"output/figures/incidence_timeline_kmeans_cluster_plot.png\",\n",
    "    dpi=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0bc80e",
   "metadata": {},
   "source": [
    "#### Mean shift clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8b99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meanshift Silhouette Score: 0.408\n",
      "Meanshift Silhouette Score: 0.439\n",
      "Meanshift Silhouette Score: 0.911\n"
     ]
    }
   ],
   "source": [
    "# === 1. All Features ===\n",
    "df_cleaned = df_features_cleaned.copy()\n",
    "scaler, meanshift, clusters, X_scaled = scale_and_cluster(df_cleaned, method='meanshift')\n",
    "df_cleaned['meanshift_cluster'] = clusters\n",
    "\n",
    "plot_pca_clusters(\n",
    "    X_scaled, \n",
    "    clusters, \n",
    "    \"Meanshift clusters in PCA space (All features)\",\n",
    "    \"output/figures/meanshift_pca_clusters_all_features.png\", \n",
    "    colors[::2]\n",
    ")\n",
    "\n",
    "df_full_imputed = apply_model_to_full_data(\n",
    "    df_full=df[list(features.keys())].rename(columns=features), \n",
    "    cleaned_columns=features.values(), \n",
    "    scaler=scaler, \n",
    "    model=meanshift, \n",
    "    cluster_col_name='meanshift_cluster'\n",
    ")\n",
    "save_cluster_labels(df_full_imputed, \"output/data/all_features_meanshift_cluster_labels_by_country_year.csv\")\n",
    "\n",
    "# === 2. Incidence-Correlated Features ===\n",
    "corr_features = {\n",
    "    'era5_tas_annual_mean_temp': 'Mean temperature (°C)',\n",
    "    'era5_pr_annual_mean_precipitation': 'Mean precipitation (mm)'\n",
    "}\n",
    "df_corr = df[list(corr_features.keys())].dropna().rename(columns=corr_features)\n",
    "\n",
    "scaler, meanshift, clusters, X_scaled = scale_and_cluster(df_corr, method='meanshift')\n",
    "df_corr['meanshift_cluster'] = clusters\n",
    "\n",
    "plot_pca_clusters(\n",
    "    X_scaled, \n",
    "    clusters, \n",
    "    \"Meanshift clusters in PCA space (Incidence-correlated features)\",\n",
    "    \"output/figures/meanshift_pca_clusters_incidence_correlated_features.png\", \n",
    "    colors[4:7]\n",
    ")\n",
    "\n",
    "df_with_clusters = df.loc[df_corr.index, ['adm_0_iso3', 'year']].copy()\n",
    "df_with_clusters['meanshift_cluster'] = clusters\n",
    "df_with_clusters = df.merge(df_with_clusters, on=['adm_0_iso3', 'year'], how='left')\n",
    "\n",
    "save_cluster_labels(df_with_clusters, \"output/data/incidence_correlated_meanshift_cluster_labels_by_country_year.csv\")\n",
    "\n",
    "cluster_palette = {0: colors[6], 1: colors[5], 2: colors[4]}\n",
    "legend_labels = [\"Endemic\", \"Hypo-endemic\", \"Non-endemic\"]\n",
    "\n",
    "plot_cluster_timeline(\n",
    "    df=df_with_clusters,\n",
    "    cluster_col='meanshift_cluster',\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    size='log_incidence_per_100k',\n",
    "    title=\"Cluster membership over time by country (incidence correlated features)\",\n",
    "    legend_labels=legend_labels,\n",
    "    palette=cluster_palette,\n",
    "    save_path=\"output/figures/incidence_correlated_timeline_meanshift_cluster_plot.png\",\n",
    "    dpi=1000\n",
    ")\n",
    "\n",
    "# === 3. Incidence Only ===\n",
    "target_col = \"log_incidence_per_100k\"\n",
    "df_inc = df[[target_col]].dropna()\n",
    "\n",
    "scaler, meanshift, clusters, X_scaled = scale_and_cluster(df_inc, method='meanshift')\n",
    "df_inc['meanshift_cluster'] = clusters\n",
    "\n",
    "df_inc_imputed = df.copy()\n",
    "df_inc_imputed[target_col] = df_inc_imputed[target_col].fillna(df_inc[target_col].mean())\n",
    "X_full_scaled = scaler.transform(df_inc_imputed[[target_col]])\n",
    "df_inc_imputed['meanshift_cluster'] = meanshift.predict(X_full_scaled)\n",
    "\n",
    "save_cluster_labels(df_inc_imputed, \"output/data/incidence_meanshift_cluster_labels_by_country_year.csv\")\n",
    "\n",
    "plot_cluster_timeline(\n",
    "    df=df_inc_imputed,\n",
    "    cluster_col='meanshift_cluster',\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    size='log_incidence_per_100k',\n",
    "    title=\"Cluster membership over time by country (incidence only)\",\n",
    "    legend_labels=None,\n",
    "    palette=None,\n",
    "    save_path=\"output/figures/incidence_timeline_meanshift_cluster_plot.png\",\n",
    "    dpi=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e530b3b",
   "metadata": {},
   "source": [
    "## Aim 2: Predict endemicity status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a56b0",
   "metadata": {},
   "source": [
    "### Predict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630cd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use incidence only version to prevent data leakage\n",
    "df_with_labels = pd.read_csv('output/data/incidence_kmeans_cluster_labels_by_country_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91798c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data (15549 samples, 10 feature dimensions)\n",
      "Training set mean accuracy: 0.6620\n",
      "Validation set mean accuracy: 0.6521\n",
      "SGD logistic regression validation accuracy: 0.7907\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocess data ---\n",
    "target = 'kmeans_cluster'\n",
    "class_features = [\n",
    "    'TPopulation1July',\n",
    "    'pop_density_per_km2',\n",
    "    'era5_hur_annual_mean_relative_humidity',\n",
    "    'era5_pr_annual_mean_precipitation', \n",
    "    'era5_tas_annual_mean_temp',\n",
    "    'era5_tnn_annual_min_temp', \n",
    "    'urban_pop', \n",
    "    'gdp_ppp', \n",
    "    'tourism_arrivals',\n",
    "    'tourism_departures'\n",
    "]\n",
    "\n",
    "df_model = df_with_labels[df_with_labels[target].notna()].copy()\n",
    "X = df_model[class_features]\n",
    "y = df_model[target].astype(int)\n",
    "\n",
    "y_unique = np.unique(y)\n",
    "y_names = ['Endemic', 'Non-endemic', 'Hypo-endemic'] \n",
    "\n",
    "print(\"Preprocessing data ({} samples, {} feature dimensions)\".format(X.shape[0], X.shape[1]))\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X))\n",
    "\n",
    "train_split_idx = int(0.8 * len(X))\n",
    "\n",
    "train_indices = shuffled_indices[0:train_split_idx]\n",
    "val_indices = shuffled_indices[train_split_idx:]\n",
    "\n",
    "# Select the examples from X and y to construct our training, validation, testing sets\n",
    "X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "X_val, y_val = X.iloc[val_indices], y.iloc[val_indices]\n",
    "\n",
    "# --- Impute missing values in training and validation data ---\n",
    "imputer = SimpleImputer(fill_value=-999, strategy='constant') \n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "\n",
    "# --- Apply Standard Scaling ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_train_class_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_val_class_scaled = scaler.transform(X_val_imputed)\n",
    "\n",
    "# --- Train and validate model ---\n",
    "class_model = LogisticRegression(\n",
    "    class_weight='balanced',  \n",
    "    penalty='l2',             \n",
    "    C=1.0,                    \n",
    "    solver='lbfgs'      \n",
    ")\n",
    "class_model.fit(X_train_class_scaled, y_train)\n",
    "\n",
    "predictions_train = class_model.predict(X_train_class_scaled)\n",
    "score_train = skmetrics.accuracy_score(y_train, predictions_train)\n",
    "print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
    "\n",
    "predictions_val = class_model.predict(X_val_class_scaled)\n",
    "score_val = skmetrics.accuracy_score(y_val, predictions_val)\n",
    "print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "confusion_matrix = skmetrics.confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "confusion_matrix_plot = skmetrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=y_names)\n",
    "\n",
    "confusion_matrix_plot.plot()\n",
    "\n",
    "plt.savefig(\"output/figures/confusion_matrix_all_features_kmeans.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# --- ROC curve ---\n",
    "probabilities_val = class_model.predict_proba(X_val_class_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "one_hot_val = skpreprocessing.label_binarize(y_val, classes=y_unique)\n",
    "\n",
    "for class_id, color, label in zip(range(len(y_unique)), colors[4:7][::-1], y_names):\n",
    "    skmetrics.RocCurveDisplay.from_predictions(\n",
    "        one_hot_val[:, class_id],\n",
    "        probabilities_val[:, class_id],\n",
    "        name=f\"ROC curve for {label}\",\n",
    "        color=color,\n",
    "        ax=ax\n",
    "    )\n",
    "plt.savefig(\"output/figures/roc_curve_all_features_kmeans.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# --- SGD comparison ---\n",
    "sgd_model = SGDClassifier(\n",
    "    loss='log_loss',              \n",
    "    eta0=1e-3,\n",
    "    penalty='l2',                 \n",
    "    class_weight='balanced',      \n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "sgd_model.fit(X_train_class_scaled, y_train)\n",
    "\n",
    "sgd_val_predictions = sgd_model.predict(X_val_class_scaled)\n",
    "sgd_val_accuracy = skmetrics.accuracy_score(y_val, sgd_val_predictions)\n",
    "print(\"SGD logistic regression validation accuracy: {:.4f}\".format(sgd_val_accuracy))\n",
    "\n",
    "# --- Compare accuracies ---\n",
    "models = ['LogisticRegression', 'SGDClassifier']\n",
    "accuracies = [score_val, sgd_val_accuracy]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(models, accuracies, color=[colors[1], colors[6]])\n",
    "plt.ylim(0, 1.1)\n",
    "plt.axhline(0.70, color=chart[0], linestyle='--', linewidth=1, label='Target accuracy (0.70)')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.title('Validation accuracy comparison')\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/figures/validation_accuracy_comparison_all_features_kmeans.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434ddb0",
   "metadata": {},
   "source": [
    "### BONUS: Predict incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.5907\n",
      "Train R^2: 0.3736\n",
      "Validation MSE: 1.5202\n",
      "Validation R^2: 0.3816\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocess data ---\n",
    "target = 'log_incidence_per_100k'  \n",
    "incidence_features = [\n",
    "    'TPopulation1July',\n",
    "    'pop_density_per_km2',\n",
    "    'era5_hur_annual_mean_relative_humidity',\n",
    "    'era5_pr_annual_mean_precipitation', \n",
    "    'era5_tas_annual_mean_temp',\n",
    "    'era5_tnn_annual_min_temp', \n",
    "    'urban_pop', \n",
    "    'gdp_ppp', \n",
    "    'tourism_arrivals',\n",
    "    'tourism_departures'\n",
    "]\n",
    "\n",
    "df_model = df_with_labels[df_with_labels[target].notna()].copy()\n",
    "X = df_model[incidence_features]\n",
    "y = df_model[target]\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X))\n",
    "train_split_idx = int(0.8 * len(X))\n",
    "train_indices = shuffled_indices[:train_split_idx]\n",
    "val_indices = shuffled_indices[train_split_idx:]\n",
    "\n",
    "X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "X_val, y_val = X.iloc[val_indices], y.iloc[val_indices]\n",
    "\n",
    "imputer = SimpleImputer(fill_value=-999, strategy='constant')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "\n",
    "# --- Generate polynomial features ---\n",
    "poly = skpreprocessing.PolynomialFeatures(degree=3, include_bias=True)  \n",
    "X_train_poly = poly.fit_transform(X_train_imputed)\n",
    "poly_feature_names = poly.get_feature_names_out().tolist()\n",
    "X_val_poly = poly.transform(X_val_imputed)\n",
    "\n",
    "# --- Scale ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_train_incidence_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_val_incidence_scaled = scaler.transform(X_val_poly)\n",
    "\n",
    "# --- Fit polynomial regression ---\n",
    "incidence_model = Ridge(alpha=1.0)\n",
    "incidence_model.fit(X_train_incidence_scaled, y_train)\n",
    "\n",
    "train_predictions = incidence_model.predict(X_train_incidence_scaled)\n",
    "\n",
    "train_mse = skmetrics.mean_squared_error(y_train, train_predictions)\n",
    "train_r2 = skmetrics.r2_score(y_train, train_predictions)\n",
    "\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Train R^2: {train_r2:.4f}\")\n",
    "\n",
    "# --- Predictions ---\n",
    "val_predictions = incidence_model.predict(X_val_incidence_scaled)\n",
    "\n",
    "mse = skmetrics.mean_squared_error(y_val, val_predictions)\n",
    "r2 = skmetrics.r2_score(y_val, val_predictions)\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation R^2: {r2:.4f}\")\n",
    "\n",
    "# --- Residuals ---\n",
    "residuals = y_val - val_predictions\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(val_predictions, residuals, alpha=0.6, edgecolor=chart[1], c=colors[0])\n",
    "plt.axhline(0, color=colors[8], linestyle='--', linewidth=1)\n",
    "plt.xlabel('Predicted Log Incidence per 100k')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Polynomial Regression)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/figures/residuals_poly_regression_all_features.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(residuals, kde=True, bins=30, color=colors[0])\n",
    "plt.axvline(0, color=colors[8], linestyle='--')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/figures/residual_distribution_poly_regression_all_features.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e96292",
   "metadata": {},
   "source": [
    "#### Predict incidence for 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_years = merged_df['year'].isin([2023])\n",
    "df_future = merged_df[future_years].copy()\n",
    "X_future = df_future[incidence_features].replace('..', np.nan).astype(float)\n",
    "\n",
    "X_future_imputed = imputer.transform(X_future)\n",
    "X_future_poly = poly.transform(X_future_imputed)\n",
    "X_future_scaled = scaler.transform(X_future_poly)\n",
    "\n",
    "future_predictions = incidence_model.predict(X_future_scaled)\n",
    "df_future['predicted_log_incidence_per_100k'] = future_predictions\n",
    "\n",
    "\n",
    "residual_std = np.std(residuals)\n",
    "\n",
    "z = stats.norm.ppf(0.975)\n",
    "ci_margin = z * residual_std\n",
    "\n",
    "df_future['ci_lower'] = future_predictions - ci_margin\n",
    "df_future['ci_upper'] = future_predictions + ci_margin\n",
    "\n",
    "df_future['predicted_incidence_per_100k'] = np.exp(df_future['predicted_log_incidence_per_100k'])\n",
    "df_future['ci_lower_incidence'] = np.exp(df_future['ci_lower'])\n",
    "df_future['ci_upper_incidence'] = np.exp(df_future['ci_upper'])\n",
    "\n",
    "map_2023 = world.merge(df_future, left_on='adm_0_iso3', right_on='adm_0_iso3')\n",
    "map_2023 = gpd.GeoDataFrame(map_2023, geometry='adm_0_geometry')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "vmin = map_2023['predicted_incidence_per_100k'].min()\n",
    "vmax = map_2023['predicted_incidence_per_100k'].max()\n",
    "\n",
    "map_2023.plot(\n",
    "    column='predicted_incidence_per_100k',\n",
    "    cmap='viridis',\n",
    "    linewidth=0.8,\n",
    "    ax=ax,\n",
    "    edgecolor='0.8',\n",
    "    legend=True,\n",
    "    missing_kwds={\n",
    "        'color': 'lightgrey',\n",
    "        'label': 'No data'\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "ax.set_title('Predicted Incidence per 100k (2023)', fontsize=14)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/figures/predicted_incidence_map_2023.png\", dpi=300)\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf012a",
   "metadata": {},
   "source": [
    "## Aim 3: Use explainable AI to interpret predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = class_model\n",
    "explainer = shap.Explainer(model, X_train_class_scaled)\n",
    "shap_values = explainer(X_val_class_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 16))\n",
    "shap.summary_plot(shap_values, X_val_class_scaled, feature_names=class_features, show=False)\n",
    "\n",
    "# Customize axis labels\n",
    "plt.xlabel(\"SHAP value (impact on model output)\", fontsize=12)\n",
    "plt.ylabel(\"Features\", fontsize=12)\n",
    "\n",
    "# Save the figure\n",
    "plt.legend(loc='lower right')  # only needed if your plot includes color legend\n",
    "plt.savefig('output/figures/classification_shap_values_kmeans.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66851b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
