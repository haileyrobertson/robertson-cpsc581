{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a91bc",
   "metadata": {},
   "source": [
    "**Beyond sporadic outbreaks: Classifying and explaining dengue endemicity over scenarios of global change.**\n",
    "\n",
    "*CPSC 581: Machine Learning*\n",
    "\n",
    "*Yale University*\n",
    "\n",
    "*Instructor: Alex Wong*\n",
    "\n",
    "*Student: Hailey Robertson*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7b335",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b61e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1065aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skmetrics\n",
    "import sklearn.preprocessing as skpreprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import font_manager\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon \n",
    "import country_converter as coco\n",
    "\n",
    "# Other\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "np.random.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d043a2",
   "metadata": {},
   "source": [
    "#### Nice defaults for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e283e851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAABlCAYAAAB5uH+EAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABIdJREFUeJzt3MFOY2UcxuHTCm0HBBJijKntChPdGXdzCbMxXIhr3XIvXoyrWXgByoJmuAA6ZZAWjimTMa7G8x2GfL4zz7Pppgn/vEDDLwUGbdu2DQAAAIQa1j4AAAAAHkPYAgAAEE3YAgAAEE3YAgAAEE3YAgAAEE3YAgAAEE3YAgAAEG2ny5Pu7++by8vL5uDgoBkMBk9/FQAAAJ+0tm2b5XLZTKfTZjgcPj5st1E7n88/1H0AAADQyWKxaGaz2ePDdvtO7daXP/7cDHfH3T46zey7Se0TIr344ZvaJ8Q5//PX2idEevF9p5dA/uX56PPaJ0S6e/mq9glxXv/+Re0TIr189XXtE+L8tt7UPiHSt1/9UfuEOM9/+qX2CXFWq1Vzenr6T4++T6ef6t79+vE2aoe7Yq2rnYmt+pjs7dU+Ic5oslv7hEh7+8K21MF4VPuESHfPfK0VG3ld6+PZjjcgSo3az2qfEGmy63u01P7+fu0TYnX5c1j/PAoAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBoO12e1Lbtw+P9+q+nvuejsrmpfUGmm+vr2ifEub1Z1z4h0vXq7Wsb3S3Xt7VPiHT3ZlP7hDivb72u9fFm42e1Urcb35993Kx9j5ZarVa1T4jd7F2Pvs+g7fCs8/Pz5uTk5MNcBwAAAB0tFotmNps9/h3b4+Pjh8eLi4vm6Oio68f/5F1dXTXz+fzhE3F4eFj7nAg268du5WzWj93K2awfu5WzWT92K2ezfuxWbvse7HK5bKbT6X8+t1PYDodv/xR3G7U+CeW2m9mtjM36sVs5m/Vjt3I268du5WzWj93K2awfu5Xp+saqfx4FAABANGELAADAxx+24/G4OTs7e3ikO7uVs1k/ditns37sVs5m/ditnM36sVs5m/Vjt6fV6b8iAwAAwP+VX0UGAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAACgSfY3CMHKKCI1i9QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAABlCAYAAAArpKpSAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAqBJREFUeJzt2r1q22AYhuHPdpossQPZ6siTSelSCF17Op0KPYacaYYYcgCWp5RERe4PdEgeOXZRRK5r0WAZXl7BjWR51DRNUwB40vjpjwBoCSVAIJQAgVACBEIJEAglQCCUAMFR6eDx8bHc3d2V6XRaRqNRl68AvGrtX8jrui7z+byMx+P9Q9lGcrFYHGo+gFdjtVqVqqr2D2V7J9n6cHlZJpPJYaZ7Az5efel7hEH6/u1r3yMMTnXxvu8RBmdT1+Xz1ae/fds7lH8et9tICmV3746P+x5hkE5PT/seYXCm01nfIwxWl58TvcwBCIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECI5KB03TbI8PDw9dTue3H/f3fY8wSJvNpu8RBqeu132PMDibuv6nb88ZNR3Ourm5Kcvl8jDTAbwiq9WqVFW1/x3l+fn59nh7e1vOzs4OM90bsF6vy2Kx2F6I2WzW9ziDYGcvY2+7a+8R67ou8/k8ntsplOPxr58y20i6CLtrd2Zvu7Gzl7G33XS98fMyByAQSoBDhPLk5KRcX19vj3Rnb7uzs5ext/+r01tvgLfMozdAIJQAgVACBEIJEAglQCCUAIFQAgRCCVCe9xO822NQ1qjWWgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['#1A5784', '#38828C', '#84B5B2', '#60904F', '#ACC253', '#E7C960', '#E49243', '#D75F58', '#A04275', '#633D71', '#8c564b', '#c7c7c7']\n",
    "sns.palplot(sns.color_palette(colors))\n",
    "\n",
    "# Define chart color palette\n",
    "chart = ['#2C2B2B','#565E69','#CACED3','#E7EAEE']\n",
    "sns.palplot(sns.color_palette(chart))\n",
    "\n",
    "# Define constants\n",
    "figure_size = (20,6)\n",
    "\n",
    "# Set background\n",
    "sns.set_context('talk') #change the size from small to medium\n",
    "sns.set_style('white') #change bg to white\n",
    "\n",
    "# # Add every font at the specified location\n",
    "# font_dir = ['/Users/haileyrobertson/Library/Fonts']\n",
    "# for font in font_manager.findSystemFonts(font_dir):\n",
    "#     font_manager.fontManager.addfont(font)\n",
    "    \n",
    "# Set font family globally\n",
    "plt.rcParams['font.family'] = 'Open Sans'\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "\n",
    "# Set margins\n",
    "plt.rcParams['axes.xmargin'] = 0.01\n",
    "plt.rcParams['axes.ymargin'] = 0.01\n",
    "\n",
    "# Define list of date formats\n",
    "zfmts = ['', '%Y','%b\\n%Y', '%b', '%b-%d', '%H:%M', '%H:%M']\n",
    "\n",
    "# Format axes \n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.bottom'] = True\n",
    "plt.rcParams['axes.spines.left'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.titlepad'] = 10\n",
    "\n",
    "# Format ticks\n",
    "plt.rcParams[\"xtick.direction\"] = \"out\"\n",
    "plt.rcParams['xtick.major.size'] = 10\n",
    "plt.rcParams['xtick.minor.size'] = 10\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['xtick.color'] = chart[2]\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['xtick.labelcolor'] = chart[1]\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.major.pad\"] = 0\n",
    "plt.rcParams[\"ytick.minor.pad\"] = 0\n",
    "plt.rcParams[\"ytick.major.size\"] = 10\n",
    "plt.rcParams[\"ytick.minor.size\"] = 10\n",
    "plt.rcParams[\"ytick.color\"] = chart[2]\n",
    "plt.rcParams[\"ytick.major.width\"] = 0.1\n",
    "plt.rcParams[\"ytick.minor.width\"] = 0.1\n",
    "plt.rcParams[\"ytick.labelcolor\"] = chart[1]\n",
    "plt.rcParams[\"ytick.labelsize\"] = 8\n",
    "\n",
    "\n",
    "# Adjust fontdict for title\n",
    "titlefont = {'family': 'Open Sans',\n",
    "             'color':  chart[0], \n",
    "             'weight': 400,\n",
    "             'size': 20}\n",
    "\n",
    "# Set grid style\n",
    "plt.rcParams['grid.color'] = chart[2]\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth']=0.5\n",
    "\n",
    "# Set legend style\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['legend.handlelength'] = 1\n",
    "plt.rcParams['legend.handleheight'] = 1.125\n",
    "\n",
    "\n",
    "\n",
    "# Set axis labels\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.labelcolor'] = chart[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d2104",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be54bd8",
   "metadata": {},
   "source": [
    "#### Load Open Dengue and align country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e2d19",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Columns not found: 'combined_place', 'geo_resolution'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 88\u001b[0m\n\u001b[1;32m     84\u001b[0m columns_to_fill \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madm_0_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion_un\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madm_0_geometry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeo_resolution\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_place\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Fill NaN values for multiple columns\u001b[39;00m\n\u001b[1;32m     87\u001b[0m dengue_ts[columns_to_fill] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mdengue_ts\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madm_0_iso3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns_to_fill\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mffill()\u001b[38;5;241m.\u001b[39mbfill())\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     93\u001b[0m dengue_ts \u001b[38;5;241m=\u001b[39m dengue_ts\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py:1951\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1950\u001b[0m     )\n\u001b[0;32m-> 1951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages/pandas/core/base.py:239\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mintersection(key)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(key)):\n\u001b[1;32m    238\u001b[0m         bad_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(key)\u001b[38;5;241m.\u001b[39mdifference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m--> 239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(bad_keys)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(\u001b[38;5;28mlist\u001b[39m(key), ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Columns not found: 'combined_place', 'geo_resolution'\""
     ]
    }
   ],
   "source": [
    "# --- Load dengue data ---\n",
    "url = 'https://raw.githubusercontent.com/OpenDengue/master-repo/refs/heads/main/data/raw_data/masterDB_V1.2.csv'\n",
    "open_dengue = pd.read_csv(url, index_col=0, encoding='latin-1').reset_index()\n",
    "\n",
    "open_dengue[\"adm_0_iso3\"] = coco.convert(\n",
    "    names=open_dengue[\"adm_0_name\"],\n",
    "    to='ISO3',\n",
    "    not_found=\"missing\"\n",
    ")\n",
    "\n",
    "date_cols = ['calendar_start_date', 'calendar_end_date']\n",
    "open_dengue[date_cols] = open_dengue[date_cols].apply(pd.to_datetime)\n",
    "open_dengue['year'] = open_dengue['calendar_start_date'].dt.year\n",
    "open_dengue['month'] = open_dengue['calendar_start_date'].dt.month\n",
    "\n",
    "#  Not all periods are the same length – some places report every year, some every month, some every week\n",
    "open_dengue[\"date_diff\"] = (open_dengue[\"calendar_end_date\"] - open_dengue[\"calendar_start_date\"]).dt.days\n",
    "\n",
    "# --- Load world geometry ---\n",
    "world = gpd.read_file(\"../data/ne_110m_admin_0_countries\")\n",
    "\n",
    "world = world.rename(columns={\n",
    "    \"ADM0_A3\": \"adm_0_iso3\",\n",
    "    \"ADMIN\": \"adm_0_name\",\n",
    "    \"REGION_UN\": \"region_un\",\n",
    "    \"geometry\": \"adm_0_geometry\"\n",
    "})[[\"adm_0_iso3\", \"adm_0_name\", \"region_un\", \"adm_0_geometry\"]]\n",
    "\n",
    "world = world.sort_values(by=\"adm_0_name\")\n",
    "\n",
    "# --- Fix known issues ---\n",
    "# Split out French Guiana from France\n",
    "france_idx = world['adm_0_name'] == 'France'\n",
    "france_geom = world.loc[france_idx, 'adm_0_geometry'].values[0]\n",
    "\n",
    "if isinstance(france_geom, MultiPolygon):\n",
    "    polygons = list(france_geom.geoms)\n",
    "    french_guiana_polygon = next((poly for poly in polygons if poly.bounds[0] < -50 and poly.bounds[2] > -54), None)\n",
    "\n",
    "    if french_guiana_polygon:\n",
    "        # Remove French Guiana from France\n",
    "        remaining_polygons = [poly for poly in polygons if poly != french_guiana_polygon]\n",
    "        world.loc[france_idx, 'adm_0_geometry'] = MultiPolygon(remaining_polygons)\n",
    "\n",
    "        # Add French Guiana as separate entry\n",
    "        french_guiana_row = {\n",
    "            'adm_0_iso3': 'GUF',\n",
    "            'adm_0_name': 'French Guiana',\n",
    "            'region_un': 'Americas',\n",
    "            'adm_0_geometry': french_guiana_polygon\n",
    "        }\n",
    "        world = pd.concat([world, gpd.GeoDataFrame([french_guiana_row], geometry='adm_0_geometry')], ignore_index=True)\n",
    "\n",
    "# Patch ISO3 codes for special cases\n",
    "world.loc[world['adm_0_name'] == 'Norway', 'adm_0_iso3'] = 'NOR'\n",
    "world.loc[world['adm_0_name'] == 'Somaliland', 'adm_0_iso3'] = 'SOM'\n",
    "world.loc[world['adm_0_name'] == 'Kosovo', 'adm_0_iso3'] = 'RKS'\n",
    "world.loc[world['adm_0_name'] == 'South Sudan', 'adm_0_iso3'] = 'SSD'\n",
    "\n",
    "# --- Merge dengue data with geometry ---\n",
    "dengue = pd.merge(open_dengue, world, on='adm_0_iso3', how='outer', suffixes=('', '_world'))\n",
    "\n",
    "# Fill UN regions with mapping\n",
    "with open('../data/un_regions.json') as f:\n",
    "    countries = json.load(f)\n",
    "dengue['region_un'] = dengue['adm_0_iso3'].map(countries).fillna(\"Other\")\n",
    "\n",
    "# Aesthetics\n",
    "dengue = dengue.drop(columns=['adm_0_name_world'])\n",
    "front_cols = ['adm_0_name', 'adm_0_iso3']\n",
    "geometry_col = ['adm_0_geometry']\n",
    "other_cols = [col for col in dengue.columns if col not in front_cols + geometry_col]\n",
    "dengue = dengue[front_cols + other_cols + geometry_col]\n",
    "\n",
    "# Fill the years so that ISO3s stay as a time series\n",
    "years = pd.Series(range(int(1950), int(dengue['year'].max()) + 1))\n",
    "unique_iso3 = dengue['adm_0_iso3'].unique()\n",
    "\n",
    "all_combinations = pd.MultiIndex.from_product([unique_iso3, years], names=['adm_0_iso3', 'year']).to_frame(index=False)\n",
    "\n",
    "dengue_ts = all_combinations.merge(dengue, on=['adm_0_iso3', 'year'], how='outer')\n",
    "\n",
    "# List of columns to fill\n",
    "columns_to_fill = ['adm_0_name', 'region_un', 'adm_0_geometry']\n",
    "\n",
    "# Fill NaN values for multiple columns\n",
    "dengue_ts[columns_to_fill] = (\n",
    "    dengue_ts\n",
    "    .groupby('adm_0_iso3')[columns_to_fill]\n",
    "    .transform(lambda x: x.ffill().bfill())\n",
    ")\n",
    "\n",
    "dengue_ts = dengue_ts.dropna(subset=['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcf0ee",
   "metadata": {},
   "source": [
    "#### Load population first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean population data ---\n",
    "pop = pd.read_csv(\"../data/WPP2022_Demographic_Indicators_Medium.csv\", dtype={'Time': int,'TPopulation1July':float}, low_memory=False)\n",
    "columns = ['ISO3_code','Time','TPopulation1July']\n",
    "pop = pop[columns]\n",
    "pop.rename(columns={\"ISO3_code\": \"adm_0_iso3\", \"Time\": \"year\", \"TPopulation1July\": \"TPopulation1July_per_1000\"}, inplace=True)\n",
    "pop['year'] = pd.to_numeric(pop['year'], errors='coerce').astype('Int64')\n",
    "dengue_ts = dengue_ts.merge(pop, how='outer', on=['adm_0_iso3', 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831a441",
   "metadata": {},
   "source": [
    "#### Calculate incidence rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = dengue_ts.groupby(['adm_0_iso3', \n",
    "                                'year',\n",
    "                                'region_un',\n",
    "                                'TPopulation1July_per_1000'\n",
    "                                ]).agg({\n",
    "    'dengue_total': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "grouped_df['TPopulation1July'] = grouped_df['TPopulation1July_per_1000'] * 1000\n",
    "grouped_df['incidence'] = grouped_df['dengue_total'] / grouped_df['TPopulation1July']\n",
    "grouped_df['incidence_per_100k'] = grouped_df['incidence'] * 100000\n",
    "grouped_df['log_incidence_per_100k'] = np.log1p(grouped_df['incidence_per_100k'])  # log(x + 1) to handle zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab822c",
   "metadata": {},
   "source": [
    "#### Load other predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e874ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean population density ---\n",
    "pop_dns = pd.read_csv(\"../data/pop_density_wb.csv\")\n",
    "year_cols = [col for col in pop_dns.columns if col.isdigit()]\n",
    "pop_dns = pop_dns.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"pop_density_per_km2\"\n",
    ")\n",
    "pop_dns.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "\n",
    "# --- Clean climate data ---\n",
    "clim = ['era5_tnn_annual_min_temp.csv', 'era5_tas_annual_mean_temp.csv', 'era5_pr_annual_mean_precipitation.csv', 'era5_hur_annual_mean_relative_humidity.csv']\n",
    "\n",
    "climate_df = {}\n",
    "\n",
    "for file in clim:\n",
    "    data = pd.read_csv(f\"../data/{file}\")\n",
    "    \n",
    "    year_cols = [col for col in data.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "    melted = data.melt(\n",
    "        id_vars=[\"code\"],\n",
    "        value_vars=year_cols,\n",
    "        var_name=\"year\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    melted[\"year\"] = melted[\"year\"].str.extract(r\"(\\d{4})\")\n",
    "    melted.rename(columns={\"code\": \"adm_0_iso3\"}, inplace=True)\n",
    "    indicator = file.replace('.csv', '')  \n",
    "    melted.rename(columns={\"value\": indicator}, inplace=True)\n",
    "    climate_df[indicator] = melted\n",
    "\n",
    "# --- Clean urbanization data ---\n",
    "urban = pd.read_csv(\"../data/urban_growth_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in urban.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "urban = urban.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"urban_growth\"\n",
    ")\n",
    "urban.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "urban['year'] = urban['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean urban pop data ---\n",
    "u_pop = pd.read_csv(\"../data/urban_pop_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in u_pop.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "u_pop = u_pop.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"urban_pop\"\n",
    ")\n",
    "u_pop.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "u_pop['year'] = urban['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean GDP data ---\n",
    "gdp = pd.read_csv(\"../data/gdp_ppp_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in gdp.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "gdp = gdp.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"gdp_ppp\"\n",
    ")\n",
    "gdp.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "gdp['year'] = gdp['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean arrivals data ---\n",
    "arr = pd.read_csv(\"../data/tourism_arrivals_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in arr.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "arr = arr.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"tourism_arrivals\"\n",
    ")\n",
    "arr.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "arr['year'] = arr['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean departures data ---\n",
    "dep = pd.read_csv(\"../data/tourism_departures_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in dep.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "dep = dep.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"tourism_departures\"\n",
    ")\n",
    "dep.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "dep['year'] = dep['year'].str.extract(r\"(\\d{4})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['adm_0_iso3', 'year', 'region_un', 'TPopulation1July_per_1000',\n",
      "       'dengue_total', 'TPopulation1July', 'incidence', 'incidence_per_100k',\n",
      "       'log_incidence_per_100k', 'pop_density_per_km2',\n",
      "       'era5_hur_annual_mean_relative_humidity',\n",
      "       'era5_pr_annual_mean_precipitation', 'era5_tas_annual_mean_temp',\n",
      "       'era5_tnn_annual_min_temp', 'urban_pop', 'gdp_ppp', 'tourism_arrivals',\n",
      "       'tourism_departures'],\n",
      "      dtype='object')\n",
      "(15549, 18)\n"
     ]
    }
   ],
   "source": [
    "predictor_dfs = [pop_dns, \n",
    "                 climate_df['era5_hur_annual_mean_relative_humidity'],\n",
    "                 climate_df['era5_pr_annual_mean_precipitation'],\n",
    "                 climate_df['era5_tas_annual_mean_temp'],\n",
    "                 climate_df['era5_tnn_annual_min_temp'],\n",
    "                 u_pop,\n",
    "                 gdp,\n",
    "                 arr,\n",
    "                 dep]\n",
    "\n",
    "merged_df = grouped_df.copy()\n",
    "\n",
    "for i, predictor in enumerate(predictor_dfs):\n",
    "    predictor['year'] = pd.to_numeric(predictor['year'], errors='coerce').astype('Int64')\n",
    "    predictor = predictor.dropna(subset=['adm_0_iso3', 'year'])\n",
    "    merged_df = merged_df.merge(predictor, how='outer', on=['adm_0_iso3', 'year'])\n",
    "\n",
    "df = merged_df.replace('..', np.nan)\n",
    "df = df[(df['year'] >= 1950) & (df['year'] <= 2022)]\n",
    "df = df.dropna(subset=['region_un', 'adm_0_iso3', 'year'])\n",
    "df.drop_duplicates(subset=['adm_0_iso3', 'year'], inplace=True)\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n",
    "# df.to_csv('../output/data/cleaned_master_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2681f",
   "metadata": {},
   "source": [
    "##### View data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3443e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "heatmap_data = df.pivot(index='adm_0_iso3', columns='year', values='incidence_per_100k').fillna(0)\n",
    "print(len(heatmap_data))\n",
    "rocket_r_cmap = sns.color_palette(\"rocket_r\", as_cmap=True)\n",
    "custom_cmap = ListedColormap([\"white\"] + list(rocket_r_cmap(np.linspace(0.01, 1, 1000))))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 16))  \n",
    "\n",
    "custom_colors = [\"white\", colors[1]]\n",
    "\n",
    "# Plot the heatmap\n",
    "im = sns.heatmap(data=heatmap_data, \n",
    "                 cmap=custom_cmap, \n",
    "                 annot=False, \n",
    "                 fmt='g', \n",
    "                 linewidths=0.1, \n",
    "                 ax=ax, \n",
    "                 vmin=0, \n",
    "                 vmax=1000, \n",
    "                 cbar=True, \n",
    "                 cbar_kws={'orientation': 'horizontal', 'pad': 0.04, 'label':'Incidence rate per 100,000'})\n",
    "\n",
    "# Set tick labels and axis labels\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Country', fontsize=12)\n",
    "ax.set_title(\"Incidence rate per 100,000 over time\",loc='left', fontsize=14)\n",
    "\n",
    "# Set tick parameters\n",
    "ax.tick_params(axis='x', pad=5, length=0, labelsize=6, width=10)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "ax.set_yticks(np.arange(len(heatmap_data.index)))\n",
    "ax.set_yticklabels(heatmap_data.index, fontsize=6)\n",
    "\n",
    "ax.tick_params(top=False, bottom=True, labeltop=False, labelbottom=True)\n",
    "\n",
    "# Hide spines\n",
    "ax.spines.bottom.set_visible(False)\n",
    "ax.spines.left.set_visible(False)\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.tight_layout()\n",
    "filename = f\"../output/figures/open_dengue_incidence_1950-2023_full_covg.png\"\n",
    "plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17857d12",
   "metadata": {},
   "source": [
    "## Aim 1: Classify countries by endemicity status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633f327",
   "metadata": {},
   "source": [
    "#### Classify by endemicity status with k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f5559",
   "metadata": {},
   "source": [
    "##### Check multicollinearity with VIF and cluster with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd42d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables have VIF < 5\n",
      "Silhouette Score: 0.304\n"
     ]
    }
   ],
   "source": [
    "# --- Load cleaned data ---\n",
    "df = pd.read_csv('../output/data/cleaned_master_df.csv')\n",
    "\n",
    "# --- VIF filtering ---\n",
    "features = {\n",
    "    'era5_tas_annual_mean_temp': 'Mean temperature (°C)',\n",
    "    'era5_pr_annual_mean_precipitation': 'Mean precipitation (mm)',\n",
    "    'era5_hur_annual_mean_relative_humidity': 'Mean relative humidity (%)',\n",
    "    'urban_pop': 'Urban population (%)',\n",
    "    'gdp_ppp': 'GDP per capita (PPP)',\n",
    "    'tourism_arrivals': 'International tourism arrivals',\n",
    "    'tourism_departures': 'International tourism departures',\n",
    "    'pop_density_per_km2': 'Population density (people/km²)',\n",
    "    'TPopulation1July': 'Population size',\n",
    "    'log_incidence_per_100k': 'Log incidence per 100,000'\n",
    "}\n",
    "\n",
    "df_features = df[list(features.keys())].dropna().rename(columns=features)\n",
    "corr = df_features.corr()\n",
    "\n",
    "# --- VIF Filtering Function ---\n",
    "def calculate_vif(df, thresh=5):\n",
    "    \"\"\"\n",
    "    Removes features with VIF above the threshold.\n",
    "    \"\"\"\n",
    "    variables = df.copy()\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"variable\"] = variables.columns\n",
    "        vif[\"VIF\"] = [variance_inflation_factor(add_constant(variables).values, i + 1)\n",
    "                      for i in range(variables.shape[1])]\n",
    "        \n",
    "        max_vif = vif[\"VIF\"].max()\n",
    "        if max_vif > thresh:\n",
    "            drop_var = vif.sort_values(\"VIF\", ascending=False)[\"variable\"].iloc[0]\n",
    "            print(f\"Dropping '{drop_var}' with VIF = {max_vif:.2f}\")\n",
    "            variables = variables.drop(columns=[drop_var])\n",
    "            dropped = True\n",
    "        else:\n",
    "            print(f\"All variables have VIF < {thresh}\")\n",
    "    return variables\n",
    "\n",
    "# --- Apply VIF filtering and scale ---\n",
    "df_features_cleaned = calculate_vif(df_features)\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_features_cleaned)\n",
    "\n",
    "# --- K-Means clustering ---\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_cleaned = df_features_cleaned.copy()  \n",
    "df_cleaned['kmeans_cluster'] = clusters\n",
    "\n",
    "sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# --- PCA for visualization ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_2d[:, 0],\n",
    "    y=X_2d[:, 1],\n",
    "    hue=df_cleaned['kmeans_cluster'],\n",
    "    palette=colors[4:7],\n",
    "    ax=ax,\n",
    "    s=50,\n",
    "    legend='full'\n",
    ")\n",
    "\n",
    "ax.set_title(f'K-Means clusters in PCA space (All features)')    \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "legend = ax.legend(title='Cluster', loc='best', frameon=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "filename = f\"../output/figures/kmeans_pca_clusters_all_features.png\"\n",
    "plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98903a8d",
   "metadata": {},
   "source": [
    "##### Cluster with only incidence-correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.503\n"
     ]
    }
   ],
   "source": [
    "# Select only the two features that were highly correlated with incidence (primary measure of endemicity)\n",
    "corr_features = {\n",
    "    'era5_tas_annual_mean_temp': 'Mean temperature (°C)',\n",
    "    'era5_pr_annual_mean_precipitation': 'Mean precipitation (mm)',\n",
    "}\n",
    "\n",
    "df_corr_features = df[list(corr_features.keys())].dropna().rename(columns=corr_features)\n",
    "\n",
    "# --- K-Means clustering ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_corr_features)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_corr_cleaned = df_corr_features.copy()  \n",
    "df_corr_cleaned['kmeans_cluster'] = clusters\n",
    "\n",
    "sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# --- PCA for visualization ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_2d[:, 0],\n",
    "    y=X_2d[:, 1],\n",
    "    hue=df_corr_cleaned['kmeans_cluster'],\n",
    "    palette=colors[4:7],\n",
    "    ax=ax,\n",
    "    s=50,\n",
    "    legend='full'\n",
    ")\n",
    "\n",
    "ax.set_title(f'K-Means clusters in PCA space (Incidence-correlated features)')    \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "legend = ax.legend(title='Cluster', loc='best', frameon=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "filename = f\"../output/figures/kmeans_pca_clusters_incidence_correlated_features.png\"\n",
    "plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "df_with_clusters = df.loc[df_corr_cleaned.index, ['adm_0_iso3', 'year']].copy()\n",
    "\n",
    "df_with_clusters['kmeans_cluster'] = df_corr_cleaned['kmeans_cluster']\n",
    "\n",
    "df_with_clusters = pd.merge(df, df_with_clusters, on=['adm_0_iso3', 'year'], how='left')\n",
    "\n",
    "df_with_clusters.to_csv(f'../output/data/incidence_correlated_cluster_labels_by_country_year.csv', index=False)\n",
    "\n",
    "cluster_palette = {0: colors[6], 1: colors[5], 2: colors[4]}  \n",
    "\n",
    "legend_labels = [\"Endemic\", \"Hypo-endemic\", \"Non-endemic\"]\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    data=df_with_clusters,\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    hue='kmeans_cluster',\n",
    "    size='log_incidence_per_100k',\n",
    "    sizes=(1, 200),\n",
    "    palette=cluster_palette,\n",
    "    alpha=0.7,\n",
    "    edgecolor=chart[1],\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.title(\"Cluster membership over time by country (incidence correlated features)\", loc ='left')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Country\", labelpad=10)\n",
    "\n",
    "handles, labels = scatter.get_legend_handles_labels()\n",
    "\n",
    "hue_title_idx = labels.index(\"kmeans_cluster\")\n",
    "size_title_idx = labels.index(\"log_incidence_per_100k\")\n",
    "\n",
    "hue_handles = handles[hue_title_idx+1:size_title_idx]\n",
    "size_handles = handles[size_title_idx+1:]\n",
    "\n",
    "new_labels = legend_labels\n",
    "\n",
    "legend1 = plt.legend(\n",
    "    handles=hue_handles,\n",
    "    labels=new_labels,\n",
    "    title=\"Cluster\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    handletextpad=0.4,    \n",
    "    borderaxespad=0.1,    \n",
    "    alignment='left'\n",
    ")     \n",
    "\n",
    "legend2 = plt.legend(\n",
    "    handles=size_handles,\n",
    "    labels=labels[size_title_idx+1:],\n",
    "    title=\"Log incidence per 100k\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 0.92),\n",
    "    handletextpad=0.4,\n",
    "    borderaxespad=0.1,\n",
    "    alignment='left',\n",
    ")\n",
    "\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/incidence_correlated_timeline_cluster_plot.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514e4c4",
   "metadata": {},
   "source": [
    "##### Cluster with only incidence and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inc = df[[\"log_incidence_per_100k\"]].dropna()\n",
    "\n",
    "# --- K-Means clustering ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_inc)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_inc_cleaned = df_inc.copy()  \n",
    "df_inc_cleaned['kmeans_cluster'] = clusters\n",
    "\n",
    "sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# # --- PCA for visualization ---\n",
    "# pca_2d = PCA(n_components=2)\n",
    "# X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# sns.scatterplot(\n",
    "#     x=X_2d[:, 0],\n",
    "#     y=X_2d[:, 1],\n",
    "#     hue=df_corr_cleaned['kmeans_cluster'],\n",
    "#     palette=colors[4:7],\n",
    "#     ax=ax,\n",
    "#     s=50,\n",
    "#     legend='full'\n",
    "# )\n",
    "\n",
    "# ax.set_title(f'K-Means clusters in PCA space (Incidence and temperature)')    \n",
    "# ax.set_xlabel('PC1')\n",
    "# ax.set_ylabel('PC2')\n",
    "\n",
    "# legend = ax.legend(title='Cluster', loc='best', frameon=True)\n",
    "\n",
    "# fig.tight_layout()\n",
    "# filename = f\"../output/figures/kmeans_pca_clusters_temp_precipitation.png\"\n",
    "# plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "# plt.close()\n",
    "\n",
    "# --- Timeline plot ---\n",
    "df_with_inc_clusters = df.loc[df_inc_cleaned.index, ['adm_0_iso3', 'year']].copy()\n",
    "\n",
    "df_with_inc_clusters['kmeans_cluster'] = df_inc_cleaned['kmeans_cluster']\n",
    "\n",
    "\n",
    "df_with_inc_clusters = pd.merge(df, df_with_inc_clusters, on=['adm_0_iso3', 'year'], how='left')\n",
    "\n",
    "df_with_inc_clusters.to_csv(f'../output/data/incidence_cluster_labels_by_country_year.csv', index=False)\n",
    "\n",
    "\n",
    "cluster_palette = {0: colors[6], 1: colors[4], 2: colors[5]}  \n",
    "\n",
    "legend_labels = [\"Endemic\", \"Non-endemic\", \"Hypo-endemic\"]\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    data=df_with_inc_clusters,\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    hue='kmeans_cluster',\n",
    "    size='log_incidence_per_100k',\n",
    "    sizes=(1, 200),\n",
    "    palette=cluster_palette,\n",
    "    alpha=0.7,\n",
    "    edgecolor=chart[1],\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.title(\"Cluster membership over time by country (incidence)\", loc ='left')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Country\", labelpad=10)\n",
    "\n",
    "handles, labels = scatter.get_legend_handles_labels()\n",
    "\n",
    "hue_title_idx = labels.index(\"kmeans_cluster\")\n",
    "size_title_idx = labels.index(\"log_incidence_per_100k\")\n",
    "\n",
    "hue_handles = handles[hue_title_idx+1:size_title_idx]\n",
    "size_handles = handles[size_title_idx+1:]\n",
    "\n",
    "new_labels = legend_labels\n",
    "\n",
    "legend1 = plt.legend(\n",
    "    handles=hue_handles,\n",
    "    labels=new_labels,\n",
    "    title=\"Cluster\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    handletextpad=0.4,    \n",
    "    borderaxespad=0.1,    \n",
    "    alignment='left'\n",
    ")     \n",
    "\n",
    "legend2 = plt.legend(\n",
    "    handles=size_handles,\n",
    "    labels=labels[size_title_idx+1:],\n",
    "    title=\"Log incidence per 100k\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 0.92),\n",
    "    handletextpad=0.4,\n",
    "    borderaxespad=0.1,\n",
    "    alignment='left',\n",
    ")\n",
    "\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/incidence_timeline_cluster_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf012a",
   "metadata": {},
   "source": [
    "## Aim 2: Use explainable AI to interpret predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48d091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
