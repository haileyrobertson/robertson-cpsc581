{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a91bc",
   "metadata": {},
   "source": [
    "## Beyond sporadic outbreaks: Classifying and explaining dengue endemicity over scenarios of global change.\n",
    "\n",
    "*CPSC 581: Machine Learning*\n",
    "\n",
    "*Yale University*\n",
    "\n",
    "*Instructor: Alex Wong*\n",
    "\n",
    "*Student: Hailey Robertson*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d705de",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7b335",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b61e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1065aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skmetrics\n",
    "import sklearn.preprocessing as skpreprocessing\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Ridge\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import shap\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon \n",
    "import country_converter as coco\n",
    "\n",
    "# Other\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d043a2",
   "metadata": {},
   "source": [
    "#### Nice defaults for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e283e851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAABlCAYAAAB5uH+EAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABIdJREFUeJzt3MFOY2UcxuHTCm0HBBJijKntChPdGXdzCbMxXIhr3XIvXoyrWXgByoJmuAA6ZZAWjimTMa7G8x2GfL4zz7Pppgn/vEDDLwUGbdu2DQAAAIQa1j4AAAAAHkPYAgAAEE3YAgAAEE3YAgAAEE3YAgAAEE3YAgAAEE3YAgAAEG2ny5Pu7++by8vL5uDgoBkMBk9/FQAAAJ+0tm2b5XLZTKfTZjgcPj5st1E7n88/1H0AAADQyWKxaGaz2ePDdvtO7daXP/7cDHfH3T46zey7Se0TIr344ZvaJ8Q5//PX2idEevF9p5dA/uX56PPaJ0S6e/mq9glxXv/+Re0TIr189XXtE+L8tt7UPiHSt1/9UfuEOM9/+qX2CXFWq1Vzenr6T4++T6ef6t79+vE2aoe7Yq2rnYmt+pjs7dU+Ic5oslv7hEh7+8K21MF4VPuESHfPfK0VG3ld6+PZjjcgSo3az2qfEGmy63u01P7+fu0TYnX5c1j/PAoAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBoO12e1Lbtw+P9+q+nvuejsrmpfUGmm+vr2ifEub1Z1z4h0vXq7Wsb3S3Xt7VPiHT3ZlP7hDivb72u9fFm42e1Urcb35993Kx9j5ZarVa1T4jd7F2Pvs+g7fCs8/Pz5uTk5MNcBwAAAB0tFotmNps9/h3b4+Pjh8eLi4vm6Oio68f/5F1dXTXz+fzhE3F4eFj7nAg268du5WzWj93K2awfu5WzWT92K2ezfuxWbvse7HK5bKbT6X8+t1PYDodv/xR3G7U+CeW2m9mtjM36sVs5m/Vjt3I268du5WzWj93K2awfu5Xp+saqfx4FAABANGELAADAxx+24/G4OTs7e3ikO7uVs1k/ditns37sVs5m/ditnM36sVs5m/Vjt6fV6b8iAwAAwP+VX0UGAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAAAgmrAFAACgSfY3CMHKKCI1i9QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAABlCAYAAAArpKpSAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAqBJREFUeJzt2r1q22AYhuHPdpossQPZ6siTSelSCF17Op0KPYacaYYYcgCWp5RERe4PdEgeOXZRRK5r0WAZXl7BjWR51DRNUwB40vjpjwBoCSVAIJQAgVACBEIJEAglQCCUAMFR6eDx8bHc3d2V6XRaRqNRl68AvGrtX8jrui7z+byMx+P9Q9lGcrFYHGo+gFdjtVqVqqr2D2V7J9n6cHlZJpPJYaZ7Az5efel7hEH6/u1r3yMMTnXxvu8RBmdT1+Xz1ae/fds7lH8et9tICmV3746P+x5hkE5PT/seYXCm01nfIwxWl58TvcwBCIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECI5KB03TbI8PDw9dTue3H/f3fY8wSJvNpu8RBqeu132PMDibuv6nb88ZNR3Ourm5Kcvl8jDTAbwiq9WqVFW1/x3l+fn59nh7e1vOzs4OM90bsF6vy2Kx2F6I2WzW9ziDYGcvY2+7a+8R67ou8/k8ntsplOPxr58y20i6CLtrd2Zvu7Gzl7G33XS98fMyByAQSoBDhPLk5KRcX19vj3Rnb7uzs5ext/+r01tvgLfMozdAIJQAgVACBEIJEAglQCCUAIFQAgRCCVCe9xO822NQ1qjWWgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['#1A5784', '#38828C', '#84B5B2', '#60904F', '#ACC253', '#E7C960', '#E49243', '#D75F58', '#A04275', '#633D71', '#8c564b', '#c7c7c7']\n",
    "sns.palplot(sns.color_palette(colors))\n",
    "\n",
    "# Define chart color palette\n",
    "chart = ['#2C2B2B','#565E69','#CACED3','#E7EAEE']\n",
    "sns.palplot(sns.color_palette(chart))\n",
    "\n",
    "# Define constants\n",
    "figure_size = (20,6)\n",
    "\n",
    "# Set background\n",
    "sns.set_context('talk') #change the size from small to medium\n",
    "sns.set_style('white') #change bg to white\n",
    "\n",
    "# # Add every font at the specified location\n",
    "# font_dir = ['/Users/haileyrobertson/Library/Fonts']\n",
    "# for font in font_manager.findSystemFonts(font_dir):\n",
    "#     font_manager.fontManager.addfont(font)\n",
    "    \n",
    "# Set font family globally\n",
    "plt.rcParams['font.family'] = 'Open Sans'\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "\n",
    "# Set margins\n",
    "plt.rcParams['axes.xmargin'] = 0.01\n",
    "plt.rcParams['axes.ymargin'] = 0.01\n",
    "\n",
    "# Define list of date formats\n",
    "zfmts = ['', '%Y','%b\\n%Y', '%b', '%b-%d', '%H:%M', '%H:%M']\n",
    "\n",
    "# Format axes \n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.bottom'] = True\n",
    "plt.rcParams['axes.spines.left'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.titlepad'] = 10\n",
    "\n",
    "# Format ticks\n",
    "plt.rcParams[\"xtick.direction\"] = \"out\"\n",
    "plt.rcParams['xtick.major.size'] = 10\n",
    "plt.rcParams['xtick.minor.size'] = 10\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['xtick.color'] = chart[2]\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['xtick.labelcolor'] = chart[1]\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.major.pad\"] = 0\n",
    "plt.rcParams[\"ytick.minor.pad\"] = 0\n",
    "plt.rcParams[\"ytick.major.size\"] = 10\n",
    "plt.rcParams[\"ytick.minor.size\"] = 10\n",
    "plt.rcParams[\"ytick.color\"] = chart[2]\n",
    "plt.rcParams[\"ytick.major.width\"] = 0.1\n",
    "plt.rcParams[\"ytick.minor.width\"] = 0.1\n",
    "plt.rcParams[\"ytick.labelcolor\"] = chart[1]\n",
    "plt.rcParams[\"ytick.labelsize\"] = 8\n",
    "\n",
    "\n",
    "# Adjust fontdict for title\n",
    "titlefont = {'family': 'Open Sans',\n",
    "             'color':  chart[0], \n",
    "             'weight': 400,\n",
    "             'size': 20}\n",
    "\n",
    "# Set grid style\n",
    "plt.rcParams['grid.color'] = chart[2]\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth']=0.5\n",
    "\n",
    "# Set legend style\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['legend.handlelength'] = 1\n",
    "plt.rcParams['legend.handleheight'] = 1.125\n",
    "\n",
    "\n",
    "\n",
    "# Set axis labels\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.labelcolor'] = chart[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d2104",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be54bd8",
   "metadata": {},
   "source": [
    "#### Load Open Dengue and align country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb7e2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load dengue data ---\n",
    "url = 'https://raw.githubusercontent.com/OpenDengue/master-repo/refs/heads/main/data/raw_data/masterDB_V1.2.csv'\n",
    "open_dengue = pd.read_csv(url, index_col=0, encoding='latin-1').reset_index()\n",
    "\n",
    "open_dengue[\"adm_0_iso3\"] = coco.convert(\n",
    "    names=open_dengue[\"adm_0_name\"],\n",
    "    to='ISO3',\n",
    "    not_found=\"missing\"\n",
    ")\n",
    "\n",
    "date_cols = ['calendar_start_date', 'calendar_end_date']\n",
    "open_dengue[date_cols] = open_dengue[date_cols].apply(pd.to_datetime)\n",
    "open_dengue['year'] = open_dengue['calendar_start_date'].dt.year\n",
    "open_dengue['month'] = open_dengue['calendar_start_date'].dt.month\n",
    "\n",
    "#  Not all periods are the same length – some places report every year, some every month, some every week\n",
    "open_dengue[\"date_diff\"] = (open_dengue[\"calendar_end_date\"] - open_dengue[\"calendar_start_date\"]).dt.days\n",
    "\n",
    "# --- Load world geometry ---\n",
    "world = gpd.read_file(\"../data/ne_110m_admin_0_countries\")\n",
    "\n",
    "world = world.rename(columns={\n",
    "    \"ADM0_A3\": \"adm_0_iso3\",\n",
    "    \"ADMIN\": \"adm_0_name\",\n",
    "    \"REGION_UN\": \"region_un\",\n",
    "    \"geometry\": \"adm_0_geometry\"\n",
    "})[[\"adm_0_iso3\", \"adm_0_name\", \"region_un\", \"adm_0_geometry\"]]\n",
    "\n",
    "world = world.sort_values(by=\"adm_0_name\")\n",
    "\n",
    "# --- Fix known issues ---\n",
    "# Split out French Guiana from France\n",
    "france_idx = world['adm_0_name'] == 'France'\n",
    "france_geom = world.loc[france_idx, 'adm_0_geometry'].values[0]\n",
    "\n",
    "if isinstance(france_geom, MultiPolygon):\n",
    "    polygons = list(france_geom.geoms)\n",
    "    french_guiana_polygon = next((poly for poly in polygons if poly.bounds[0] < -50 and poly.bounds[2] > -54), None)\n",
    "\n",
    "    if french_guiana_polygon:\n",
    "        # Remove French Guiana from France\n",
    "        remaining_polygons = [poly for poly in polygons if poly != french_guiana_polygon]\n",
    "        world.loc[france_idx, 'adm_0_geometry'] = MultiPolygon(remaining_polygons)\n",
    "\n",
    "        # Add French Guiana as separate entry\n",
    "        french_guiana_row = {\n",
    "            'adm_0_iso3': 'GUF',\n",
    "            'adm_0_name': 'French Guiana',\n",
    "            'region_un': 'Americas',\n",
    "            'adm_0_geometry': french_guiana_polygon\n",
    "        }\n",
    "        world = pd.concat([world, gpd.GeoDataFrame([french_guiana_row], geometry='adm_0_geometry')], ignore_index=True)\n",
    "\n",
    "# Patch ISO3 codes for special cases\n",
    "world.loc[world['adm_0_name'] == 'Norway', 'adm_0_iso3'] = 'NOR'\n",
    "world.loc[world['adm_0_name'] == 'Somaliland', 'adm_0_iso3'] = 'SOM'\n",
    "world.loc[world['adm_0_name'] == 'Kosovo', 'adm_0_iso3'] = 'RKS'\n",
    "world.loc[world['adm_0_name'] == 'South Sudan', 'adm_0_iso3'] = 'SSD'\n",
    "\n",
    "# --- Merge dengue data with geometry ---\n",
    "dengue = pd.merge(open_dengue, world, on='adm_0_iso3', how='outer', suffixes=('', '_world'))\n",
    "\n",
    "# Fill UN regions with mapping\n",
    "with open('../data/un_regions.json') as f:\n",
    "    countries = json.load(f)\n",
    "dengue['region_un'] = dengue['adm_0_iso3'].map(countries).fillna(\"Other\")\n",
    "\n",
    "# Aesthetics\n",
    "dengue = dengue.drop(columns=['adm_0_name_world'])\n",
    "front_cols = ['adm_0_name', 'adm_0_iso3']\n",
    "geometry_col = ['adm_0_geometry']\n",
    "other_cols = [col for col in dengue.columns if col not in front_cols + geometry_col]\n",
    "dengue = dengue[front_cols + other_cols + geometry_col]\n",
    "\n",
    "# Fill the years so that ISO3s stay as a time series\n",
    "years = pd.Series(range(int(1950), int(dengue['year'].max()) + 1))\n",
    "unique_iso3 = dengue['adm_0_iso3'].unique()\n",
    "\n",
    "all_combinations = pd.MultiIndex.from_product([unique_iso3, years], names=['adm_0_iso3', 'year']).to_frame(index=False)\n",
    "\n",
    "dengue_ts = all_combinations.merge(dengue, on=['adm_0_iso3', 'year'], how='outer')\n",
    "\n",
    "# List of columns to fill\n",
    "columns_to_fill = ['adm_0_name', 'region_un', 'adm_0_geometry']\n",
    "\n",
    "# Fill NaN values for multiple columns\n",
    "dengue_ts[columns_to_fill] = (\n",
    "    dengue_ts\n",
    "    .groupby('adm_0_iso3')[columns_to_fill]\n",
    "    .transform(lambda x: x.ffill().bfill())\n",
    ")\n",
    "\n",
    "dengue_ts = dengue_ts.dropna(subset=['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcf0ee",
   "metadata": {},
   "source": [
    "#### Load population first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ffa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean population data ---\n",
    "pop = pd.read_csv(\"../data/WPP2022_Demographic_Indicators_Medium.csv\", dtype={'Time': int,'TPopulation1July':float}, low_memory=False)\n",
    "columns = ['ISO3_code','Time','TPopulation1July']\n",
    "pop = pop[columns]\n",
    "pop.rename(columns={\"ISO3_code\": \"adm_0_iso3\", \"Time\": \"year\", \"TPopulation1July\": \"TPopulation1July_per_1000\"}, inplace=True)\n",
    "pop['year'] = pd.to_numeric(pop['year'], errors='coerce').astype('Int64')\n",
    "dengue_ts = dengue_ts.merge(pop, how='outer', on=['adm_0_iso3', 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831a441",
   "metadata": {},
   "source": [
    "#### Calculate incidence rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e85374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = dengue_ts.groupby(['adm_0_iso3', \n",
    "                                'year',\n",
    "                                'region_un',\n",
    "                                'TPopulation1July_per_1000'\n",
    "                                ]).agg({\n",
    "    'dengue_total': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "grouped_df['TPopulation1July'] = grouped_df['TPopulation1July_per_1000'] * 1000\n",
    "grouped_df['incidence'] = grouped_df['dengue_total'] / grouped_df['TPopulation1July']\n",
    "grouped_df['incidence_per_100k'] = grouped_df['incidence'] * 100000\n",
    "grouped_df['log_incidence_per_100k'] = np.log1p(grouped_df['incidence_per_100k'])  # log(x + 1) to handle zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab822c",
   "metadata": {},
   "source": [
    "#### Load other predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e874ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean population density ---\n",
    "pop_dns = pd.read_csv(\"../data/pop_density_wb.csv\")\n",
    "year_cols = [col for col in pop_dns.columns if col.isdigit()]\n",
    "pop_dns = pop_dns.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"pop_density_per_km2\"\n",
    ")\n",
    "pop_dns.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "\n",
    "# --- Clean climate data ---\n",
    "clim = ['era5_tnn_annual_min_temp.csv', 'era5_tas_annual_mean_temp.csv', 'era5_pr_annual_mean_precipitation.csv', 'era5_hur_annual_mean_relative_humidity.csv']\n",
    "\n",
    "climate_df = {}\n",
    "\n",
    "for file in clim:\n",
    "    data = pd.read_csv(f\"../data/{file}\")\n",
    "    \n",
    "    year_cols = [col for col in data.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "    melted = data.melt(\n",
    "        id_vars=[\"code\"],\n",
    "        value_vars=year_cols,\n",
    "        var_name=\"year\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    melted[\"year\"] = melted[\"year\"].str.extract(r\"(\\d{4})\")\n",
    "    melted.rename(columns={\"code\": \"adm_0_iso3\"}, inplace=True)\n",
    "    indicator = file.replace('.csv', '')  \n",
    "    melted.rename(columns={\"value\": indicator}, inplace=True)\n",
    "    climate_df[indicator] = melted\n",
    "\n",
    "# --- Clean urbanization data ---\n",
    "urban = pd.read_csv(\"../data/urban_growth_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in urban.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "urban = urban.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"urban_growth\"\n",
    ")\n",
    "urban.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "urban['year'] = urban['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean urban pop data ---\n",
    "u_pop = pd.read_csv(\"../data/urban_pop_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in u_pop.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "u_pop = u_pop.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"urban_pop\"\n",
    ")\n",
    "u_pop.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "u_pop['year'] = urban['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean GDP data ---\n",
    "gdp = pd.read_csv(\"../data/gdp_ppp_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in gdp.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "gdp = gdp.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"gdp_ppp\"\n",
    ")\n",
    "gdp.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "gdp['year'] = gdp['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean arrivals data ---\n",
    "arr = pd.read_csv(\"../data/tourism_arrivals_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in arr.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "arr = arr.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"tourism_arrivals\"\n",
    ")\n",
    "arr.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "arr['year'] = arr['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean departures data ---\n",
    "dep = pd.read_csv(\"../data/tourism_departures_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in dep.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "dep = dep.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"tourism_departures\"\n",
    ")\n",
    "dep.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "dep['year'] = dep['year'].str.extract(r\"(\\d{4})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8af027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['adm_0_iso3', 'year', 'region_un', 'TPopulation1July_per_1000',\n",
      "       'dengue_total', 'TPopulation1July', 'incidence', 'incidence_per_100k',\n",
      "       'log_incidence_per_100k', 'pop_density_per_km2',\n",
      "       'era5_hur_annual_mean_relative_humidity',\n",
      "       'era5_pr_annual_mean_precipitation', 'era5_tas_annual_mean_temp',\n",
      "       'era5_tnn_annual_min_temp', 'urban_pop', 'gdp_ppp', 'tourism_arrivals',\n",
      "       'tourism_departures'],\n",
      "      dtype='object')\n",
      "(15549, 18)\n"
     ]
    }
   ],
   "source": [
    "predictor_dfs = [pop_dns, \n",
    "                 climate_df['era5_hur_annual_mean_relative_humidity'],\n",
    "                 climate_df['era5_pr_annual_mean_precipitation'],\n",
    "                 climate_df['era5_tas_annual_mean_temp'],\n",
    "                 climate_df['era5_tnn_annual_min_temp'],\n",
    "                 u_pop,\n",
    "                 gdp,\n",
    "                 arr,\n",
    "                 dep]\n",
    "\n",
    "merged_df = grouped_df.copy()\n",
    "\n",
    "for i, predictor in enumerate(predictor_dfs):\n",
    "    predictor['year'] = pd.to_numeric(predictor['year'], errors='coerce').astype('Int64')\n",
    "    predictor = predictor.dropna(subset=['adm_0_iso3', 'year'])\n",
    "    merged_df = merged_df.merge(predictor, how='outer', on=['adm_0_iso3', 'year'])\n",
    "\n",
    "df = merged_df.replace('..', np.nan)\n",
    "df = df[(df['year'] >= 1950) & (df['year'] <= 2022)]\n",
    "df = df.dropna(subset=['region_un', 'adm_0_iso3', 'year'])\n",
    "df.drop_duplicates(subset=['adm_0_iso3', 'year'], inplace=True)\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n",
    "# df.to_csv('../output/data/cleaned_master_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2681f",
   "metadata": {},
   "source": [
    "##### View data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3443e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "heatmap_data = df.pivot(index='adm_0_iso3', columns='year', values='incidence_per_100k').fillna(0)\n",
    "print(len(heatmap_data))\n",
    "rocket_r_cmap = sns.color_palette(\"rocket_r\", as_cmap=True)\n",
    "custom_cmap = ListedColormap([\"white\"] + list(rocket_r_cmap(np.linspace(0.01, 1, 1000))))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 16))  \n",
    "\n",
    "custom_colors = [\"white\", colors[1]]\n",
    "\n",
    "# Plot the heatmap\n",
    "im = sns.heatmap(data=heatmap_data, \n",
    "                 cmap=custom_cmap, \n",
    "                 annot=False, \n",
    "                 fmt='g', \n",
    "                 linewidths=0.1, \n",
    "                 ax=ax, \n",
    "                 vmin=0, \n",
    "                 vmax=1000, \n",
    "                 cbar=True, \n",
    "                 cbar_kws={'orientation': 'horizontal', 'pad': 0.04, 'label':'Incidence rate per 100,000'})\n",
    "\n",
    "# Set tick labels and axis labels\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Country', fontsize=12)\n",
    "ax.set_title(\"Incidence rate per 100,000 over time\",loc='left', fontsize=14)\n",
    "\n",
    "# Set tick parameters\n",
    "ax.tick_params(axis='x', pad=5, length=0, labelsize=6, width=10)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "ax.set_yticks(np.arange(len(heatmap_data.index)))\n",
    "ax.set_yticklabels(heatmap_data.index, fontsize=6)\n",
    "\n",
    "ax.tick_params(top=False, bottom=True, labeltop=False, labelbottom=True)\n",
    "\n",
    "# Hide spines\n",
    "ax.spines.bottom.set_visible(False)\n",
    "ax.spines.left.set_visible(False)\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.tight_layout()\n",
    "filename = f\"../output/figures/open_dengue_incidence_1950-2023_full_covg.png\"\n",
    "plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17857d12",
   "metadata": {},
   "source": [
    "## Aim 1: Classify countries by endemicity status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633f327",
   "metadata": {},
   "source": [
    "#### Cluster with k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f5559",
   "metadata": {},
   "source": [
    "##### Check multicollinearity with VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables have VIF < 5\n"
     ]
    }
   ],
   "source": [
    "# --- Load cleaned data ---\n",
    "df = pd.read_csv('../output/data/cleaned_master_df.csv')\n",
    "\n",
    "# --- VIF filtering ---\n",
    "features = {\n",
    "    'era5_tas_annual_mean_temp': 'Mean temperature (°C)',\n",
    "    'era5_pr_annual_mean_precipitation': 'Mean precipitation (mm)',\n",
    "    'era5_hur_annual_mean_relative_humidity': 'Mean relative humidity (%)',\n",
    "    'urban_pop': 'Urban population (%)',\n",
    "    'gdp_ppp': 'GDP per capita (PPP)',\n",
    "    'tourism_arrivals': 'International tourism arrivals',\n",
    "    'tourism_departures': 'International tourism departures',\n",
    "    'pop_density_per_km2': 'Population density (people/km²)',\n",
    "    'TPopulation1July': 'Population size',\n",
    "}\n",
    "\n",
    "# --- Prepare VIF input (only rows without NaNs) ---\n",
    "df_features = df[list(features.keys())].dropna().rename(columns=features)\n",
    "\n",
    "# --- VIF Filtering Function ---\n",
    "def calculate_vif(df, thresh=5):\n",
    "    variables = df.copy()\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"variable\"] = variables.columns\n",
    "        vif[\"VIF\"] = [variance_inflation_factor(add_constant(variables).values, i + 1)\n",
    "                      for i in range(variables.shape[1])]\n",
    "        max_vif = vif[\"VIF\"].max()\n",
    "        if max_vif > thresh:\n",
    "            drop_var = vif.sort_values(\"VIF\", ascending=False)[\"variable\"].iloc[0]\n",
    "            print(f\"Dropping '{drop_var}' with VIF = {max_vif:.2f}\")\n",
    "            variables = variables.drop(columns=[drop_var])\n",
    "            dropped = True\n",
    "        else:\n",
    "            print(f\"All variables have VIF < {thresh}\")\n",
    "    return variables\n",
    "\n",
    "# --- Apply VIF filtering on clean subset ---\n",
    "df_features_cleaned = calculate_vif(df_features)\n",
    "cleaned_columns = df_features_cleaned.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa2981",
   "metadata": {},
   "source": [
    "##### Cluster with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd42d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables have VIF < 5\n",
      "Silhouette Score: 0.251\n"
     ]
    }
   ],
   "source": [
    "# --- Train scaler and KMeans on clean subset ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_features_cleaned)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_cleaned = df_features_cleaned.copy()\n",
    "df_cleaned['kmeans_cluster'] = clusters\n",
    "\n",
    "# --- Silhouette score on clean data only ---\n",
    "sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# --- PCA visualization ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_2d[:, 0],\n",
    "    y=X_2d[:, 1],\n",
    "    hue=df_cleaned['kmeans_cluster'],\n",
    "    palette=colors[4:7],\n",
    "    ax=ax,\n",
    "    s=50,\n",
    "    legend='full'\n",
    ")\n",
    "\n",
    "ax.set_title(f'K-Means clusters in PCA space (All features)')    \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.legend(title='Cluster', loc='best', frameon=True)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../output/figures/kmeans_pca_clusters_all_features.png\", format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# --- Impute and assign clusters to ALL data ---\n",
    "df_full_features = df[list(features.keys())].rename(columns=features)\n",
    "df_full_imputed = df_full_features[cleaned_columns].copy()\n",
    "df_full_imputed = df_full_imputed.fillna(df_full_imputed.mean())  # mean imputation\n",
    "\n",
    "X_full_scaled = scaler.transform(df_full_imputed)\n",
    "full_clusters = kmeans.predict(X_full_scaled)\n",
    "\n",
    "# --- Add full cluster labels to original data ---\n",
    "df_full_imputed['kmeans_cluster'] = full_clusters\n",
    "df_full_imputed.to_csv('../output/data/all_features_kmeans_cluster_labels_by_country_year.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98903a8d",
   "metadata": {},
   "source": [
    "##### Cluster with only incidence-correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "837b967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.503\n"
     ]
    }
   ],
   "source": [
    "# Select only the two features that were highly correlated with incidence (primary measure of endemicity)\n",
    "corr_features = {\n",
    "    'era5_tas_annual_mean_temp': 'Mean temperature (°C)',\n",
    "    'era5_pr_annual_mean_precipitation': 'Mean precipitation (mm)'\n",
    "}\n",
    "\n",
    "df_corr_features = df[list(corr_features.keys())].dropna().rename(columns=corr_features)\n",
    "\n",
    "# --- K-Means clustering ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_corr_features)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_corr_clusters = df_corr_features.copy()\n",
    "df_corr_clusters['kmeans_cluster'] = clusters\n",
    "\n",
    "sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# --- PCA for visualization ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_2d[:, 0],\n",
    "    y=X_2d[:, 1],\n",
    "    hue=df_corr_clusters['kmeans_cluster'],\n",
    "    palette=colors[4:7],\n",
    "    ax=ax,\n",
    "    s=50,\n",
    "    legend='full'\n",
    ")\n",
    "\n",
    "ax.set_title(f'K-Means clusters in PCA space (Incidence-correlated features)')    \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "legend = ax.legend(title='Cluster', loc='best', frameon=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "filename = f\"../output/figures/kmeans_pca_clusters_incidence_correlated_features.png\"\n",
    "plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "df_with_clusters = df.loc[df_corr_clusters.index, ['adm_0_iso3', 'year']].copy()\n",
    "\n",
    "df_with_clusters['kmeans_cluster'] = df_corr_clusters['kmeans_cluster']\n",
    "\n",
    "df_with_clusters = pd.merge(df, df_with_clusters, on=['adm_0_iso3', 'year'], how='left')\n",
    "\n",
    "df_with_clusters.to_csv(f'../output/data/incidence_correlated_kmeans_cluster_labels_by_country_year.csv', index=False)\n",
    "\n",
    "cluster_palette = {0: colors[6], 1: colors[5], 2: colors[4]}  \n",
    "\n",
    "legend_labels = [\"Endemic\", \"Hypo-endemic\", \"Non-endemic\"]\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    data=df_with_clusters,\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    hue='kmeans_cluster',\n",
    "    size='log_incidence_per_100k',\n",
    "    sizes=(1, 200),\n",
    "    palette=cluster_palette,\n",
    "    alpha=0.7,\n",
    "    edgecolor=chart[1],\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.title(\"Cluster membership over time by country (incidence correlated features)\", loc ='left')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Country\", labelpad=10)\n",
    "\n",
    "handles, labels = scatter.get_legend_handles_labels()\n",
    "\n",
    "hue_title_idx = labels.index(\"kmeans_cluster\")\n",
    "size_title_idx = labels.index(\"log_incidence_per_100k\")\n",
    "\n",
    "hue_handles = handles[hue_title_idx+1:size_title_idx]\n",
    "size_handles = handles[size_title_idx+1:]\n",
    "\n",
    "new_labels = legend_labels\n",
    "\n",
    "legend1 = plt.legend(\n",
    "    handles=hue_handles,\n",
    "    labels=new_labels,\n",
    "    title=\"Cluster\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    handletextpad=0.4,    \n",
    "    borderaxespad=0.1,    \n",
    "    alignment='left'\n",
    ")     \n",
    "\n",
    "legend2 = plt.legend(\n",
    "    handles=size_handles,\n",
    "    labels=labels[size_title_idx+1:],\n",
    "    title=\"Log incidence per 100k\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 0.92),\n",
    "    handletextpad=0.4,\n",
    "    borderaxespad=0.1,\n",
    "    alignment='left',\n",
    ")\n",
    "\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/incidence_correlated_timeline_cluster_plot.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514e4c4",
   "metadata": {},
   "source": [
    "##### Cluster with only incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cc7f3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.917\n"
     ]
    }
   ],
   "source": [
    "# --- Extract log incidence feature ---\n",
    "target_col = \"log_incidence_per_100k\"\n",
    "df_inc = df[[target_col]]\n",
    "\n",
    "# --- Impute missing values for full dataset ---\n",
    "df_inc_imputed = df_inc.copy()\n",
    "df_inc_imputed[target_col] = df_inc_imputed[target_col].fillna(df_inc_imputed[target_col].mean())  # Mean imputation\n",
    "\n",
    "# --- Train KMeans on non-missing values ---\n",
    "df_inc_clean = df_inc.dropna()  # Only use rows where the incidence is available\n",
    "\n",
    "# --- Fit scaler and model ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_inc_clean)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# --- Evaluate clustering ---\n",
    "sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# --- Save cluster labels for clean data ---\n",
    "df_inc_cleaned = df_inc_clean.copy()\n",
    "df_inc_cleaned['kmeans_cluster'] = clusters\n",
    "\n",
    "# --- Apply model to the full imputed data ---\n",
    "X_full_scaled = scaler.transform(df_inc_imputed)\n",
    "full_clusters = kmeans.predict(X_full_scaled)\n",
    "\n",
    "# --- Merge cluster labels back into original dataframe ---\n",
    "df_with_inc_clusters = df.copy()\n",
    "df_with_inc_clusters['kmeans_cluster'] = full_clusters\n",
    "\n",
    "df_with_inc_clusters.to_csv(f'../output/data/incidence_kmeans_cluster_labels_by_country_year.csv', index=False)\n",
    "\n",
    "# --- Timeline plot ---\n",
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "cluster_palette = {0: colors[6], 1: colors[4], 2: colors[5]}  \n",
    "legend_labels = [\"Endemic\", \"Non-endemic\", \"Hypo-endemic\"]\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    data=df_with_inc_clusters,\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    hue='kmeans_cluster',\n",
    "    size='log_incidence_per_100k',\n",
    "    sizes=(1, 200),\n",
    "    palette=cluster_palette,\n",
    "    alpha=0.7,\n",
    "    edgecolor=chart[1],\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.title(\"Cluster membership over time by country (incidence)\", loc ='left')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Country\", labelpad=10)\n",
    "\n",
    "handles, labels = scatter.get_legend_handles_labels()\n",
    "\n",
    "hue_title_idx = labels.index(\"kmeans_cluster\")\n",
    "size_title_idx = labels.index(\"log_incidence_per_100k\")\n",
    "\n",
    "hue_handles = handles[hue_title_idx+1:size_title_idx]\n",
    "size_handles = handles[size_title_idx+1:]\n",
    "\n",
    "new_labels = legend_labels\n",
    "\n",
    "legend1 = plt.legend(\n",
    "    handles=hue_handles,\n",
    "    labels=new_labels,\n",
    "    title=\"Cluster\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    handletextpad=0.4,    \n",
    "    borderaxespad=0.1,    \n",
    "    alignment='left'\n",
    ")     \n",
    "\n",
    "legend2 = plt.legend(\n",
    "    handles=size_handles,\n",
    "    labels=labels[size_title_idx+1:],\n",
    "    title=\"Log incidence per 100k\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 0.92),\n",
    "    handletextpad=0.4,\n",
    "    borderaxespad=0.1,\n",
    "    alignment='left',\n",
    ")\n",
    "\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/incidence_timeline_kmeans_cluster_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# --- Scatter for visualization ---\n",
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df_with_inc_clusters.dropna(subset=['log_incidence_per_100k', 'kmeans_cluster']),\n",
    "    x='log_incidence_per_100k',\n",
    "    y='adm_0_iso3',\n",
    "    hue='kmeans_cluster',\n",
    "    palette=cluster_palette,\n",
    "    alpha=0.7,\n",
    "    s=30,\n",
    "    edgecolor=chart[1],\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.title(\"Incidence cluster distribution\", loc='left')\n",
    "plt.xlabel(\"log(incidence per 100k)\")\n",
    "plt.ylabel(\"Country\")\n",
    "\n",
    "plt.legend(\n",
    "    title=\"Cluster\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 0.92)\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/incidence_kmeans_cluster_scatter_by_country.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# --- 3D scatter for visualization ---\n",
    "plot_df = df_with_inc_clusters.dropna(subset=['log_incidence_per_100k', 'kmeans_cluster']).copy()\n",
    "\n",
    "country_order = {country: i for i, country in enumerate(sorted(plot_df['adm_0_iso3'].unique()))}\n",
    "plot_df['country_num'] = plot_df['adm_0_iso3'].map(country_order)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "custom_cluster_colors = [colors[3], colors[5], colors[6]]  # Instead of 4, 5, 6\n",
    "\n",
    "cluster_labels = sorted(plot_df['kmeans_cluster'].unique())\n",
    "cluster_palette = {\n",
    "    label: custom_cluster_colors[i] for i, label in enumerate(cluster_labels)\n",
    "}\n",
    "\n",
    "plot_df['cluster_color'] = plot_df['kmeans_cluster'].map(cluster_palette)\n",
    "\n",
    "# 3D Scatter using custom color list\n",
    "sc = ax.scatter(\n",
    "    plot_df['year'],\n",
    "    plot_df['country_num'],\n",
    "    plot_df['log_incidence_per_100k'],\n",
    "    color=plot_df['cluster_color'],  # Use actual RGB colors\n",
    "    s=10,\n",
    "    alpha=0.8,\n",
    "    edgecolor='k',\n",
    "    linewidth=0.2\n",
    ")\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel('Year', labelpad=10)\n",
    "ax.set_ylabel('Country')\n",
    "ax.set_zlabel('log(incidence per 100k)', labelpad=10)\n",
    "\n",
    "# Set y-ticks to country codes\n",
    "yticks = list(country_order.values())\n",
    "yticklabels = list(country_order.keys())\n",
    "\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_yticklabels(yticklabels, fontsize=2) \n",
    "\n",
    "for label in ax.get_yticklabels():\n",
    "    label.set_rotation(0)  \n",
    "\n",
    "ax.set_ylim(-1, len(country_order))\n",
    "\n",
    "# Define your legend labels\n",
    "legend_labels = [\"Endemic\", \"Non-endemic\", \"Hypo-endemic\"]\n",
    "\n",
    "legend_handles = [\n",
    "    Patch(facecolor=custom_cluster_colors[i], label=legend_labels[i])\n",
    "    for i in range(len(legend_labels))\n",
    "]\n",
    "\n",
    "# Add legend to the plot\n",
    "ax.legend(\n",
    "    handles=legend_handles,\n",
    "    title='Cluster',\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1)\n",
    ")\n",
    "\n",
    "plt.title(\"3D Scatter of Incidence Clusters by Country and Year\", loc='left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/incidence_clusters_3d_scatter.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140aefb1",
   "metadata": {},
   "source": [
    "#### Cluster with mean shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bce22",
   "metadata": {},
   "source": [
    "##### All predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45090409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.408\n"
     ]
    }
   ],
   "source": [
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_features_cleaned)\n",
    "\n",
    "# --- K-Means clustering ---\n",
    "meanshift = MeanShift()\n",
    "clusters = meanshift.fit_predict(X_scaled)\n",
    "df_features_cleaned['meanshift_cluster'] = clusters\n",
    "\n",
    "sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# --- PCA for visualization ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_2d[:, 0],\n",
    "    y=X_2d[:, 1],\n",
    "    hue=df_features_cleaned['meanshift_cluster'],\n",
    "    palette=colors[::2],\n",
    "    ax=ax,\n",
    "    s=50,\n",
    "    legend='full'\n",
    ")\n",
    "\n",
    "ax.set_title(f'Meanshift clusters in PCA space (All features)')    \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "legend = ax.legend(title='Cluster', loc='best', frameon=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "filename = f\"../output/figures/meanshift_pca_clusters_all_features.png\"\n",
    "plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8862ffb",
   "metadata": {},
   "source": [
    "##### Incidence-correlated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc89323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.439\n"
     ]
    }
   ],
   "source": [
    "# --- K-Means clustering ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_corr_features)\n",
    "\n",
    "meanshift = MeanShift()\n",
    "clusters = meanshift.fit_predict(X_scaled)\n",
    "df_corr_features['meanshift_cluster'] = clusters\n",
    "\n",
    "if len(np.unique(clusters)) > 1:\n",
    "    sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "    print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "else:\n",
    "    print(\"Silhouette Score: Cannot compute (only 1 cluster found)\")\n",
    "\n",
    "\n",
    "# --- PCA for visualization ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_2d[:, 0],\n",
    "    y=X_2d[:, 1],\n",
    "    hue=df_corr_features['meanshift_cluster'],\n",
    "    palette=colors[::2],\n",
    "    ax=ax,\n",
    "    s=50,\n",
    "    legend='full'\n",
    ")\n",
    "\n",
    "ax.set_title(f'Mean shift clusters in PCA space (Incidence-correlated features)')    \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "legend = ax.legend(title='Cluster ', loc='best', frameon=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "filename = f\"../output/figures/meanshift_pca_clusters_incidence_correlated_features.png\"\n",
    "plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "df_with_meanshift_clusters = df.loc[df_corr_features.index, ['adm_0_iso3', 'year']].copy()\n",
    "\n",
    "df_with_meanshift_clusters['meanshift_cluster'] = df_corr_features['meanshift_cluster']\n",
    "\n",
    "df_with_meanshift_clusters = pd.merge(df, df_with_meanshift_clusters, on=['adm_0_iso3', 'year'], how='left')\n",
    "\n",
    "df_with_meanshift_clusters.to_csv(f'../output/data/incidence_correlated_meanshift_cluster_labels_by_country_year.csv', index=False)\n",
    "\n",
    "every_other_color = colors[::2]\n",
    "cluster_palette = {\n",
    "    cluster: every_other_color[i]\n",
    "    for i, cluster in enumerate(np.unique(clusters))\n",
    "}\n",
    "\n",
    "\n",
    "legend_labels = [f\"Cluster {i}\" for i in np.unique(clusters)]\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    data=df_with_meanshift_clusters,\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    hue='meanshift_cluster',\n",
    "    size='log_incidence_per_100k',\n",
    "    sizes=(1, 200),\n",
    "    palette=cluster_palette,\n",
    "    alpha=0.7,\n",
    "    edgecolor=chart[1],\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.title(\"Cluster membership over time by country (incidence correlated features)\", loc ='left')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Country\", labelpad=10)\n",
    "\n",
    "handles, labels = scatter.get_legend_handles_labels()\n",
    "\n",
    "hue_title_idx = labels.index(\"meanshift_cluster\")\n",
    "size_title_idx = labels.index(\"log_incidence_per_100k\")\n",
    "\n",
    "hue_handles = handles[hue_title_idx+1:size_title_idx]\n",
    "size_handles = handles[size_title_idx+1:]\n",
    "\n",
    "new_labels = legend_labels\n",
    "\n",
    "legend1 = plt.legend(\n",
    "    handles=hue_handles,\n",
    "    labels=new_labels,\n",
    "    title=\"Cluster\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    handletextpad=0.4,    \n",
    "    borderaxespad=0.1,    \n",
    "    alignment='left'\n",
    ")     \n",
    "\n",
    "legend2 = plt.legend(\n",
    "    handles=size_handles,\n",
    "    labels=labels[size_title_idx+1:],\n",
    "    title=\"Log incidence per 100k\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 0.92),\n",
    "    handletextpad=0.4,\n",
    "    borderaxespad=0.1,\n",
    "    alignment='left',\n",
    ")\n",
    "\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/incidence_correlated_timeline_meanshift_cluster_plot.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536346b5",
   "metadata": {},
   "source": [
    "##### Incidence only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db40e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.911\n"
     ]
    }
   ],
   "source": [
    "# --- K-Means clustering ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_inc)\n",
    "meanshift = MeanShift()\n",
    "clusters = meanshift.fit_predict(X_scaled)\n",
    "\n",
    "df_inc_cleaned = df_inc.copy()  \n",
    "df_inc_cleaned['meanshift_cluster'] = clusters\n",
    "\n",
    "sil_score = skmetrics.silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# # --- PCA for visualization ---\n",
    "# pca_2d = PCA(n_components=2)\n",
    "# X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# sns.scatterplot(\n",
    "#     x=X_2d[:, 0],\n",
    "#     y=X_2d[:, 1],\n",
    "#     hue=df_corr_cleaned['meanshift_cluster'],\n",
    "#     palette=colors[4:7],\n",
    "#     ax=ax,\n",
    "#     s=50,\n",
    "#     legend='full'\n",
    "# )\n",
    "\n",
    "# ax.set_title(f'K-Means clusters in PCA space (Incidence and temperature)')    \n",
    "# ax.set_xlabel('PC1')\n",
    "# ax.set_ylabel('PC2')\n",
    "\n",
    "# legend = ax.legend(title='Cluster', loc='best', frameon=True)\n",
    "\n",
    "# fig.tight_layout()\n",
    "# filename = f\"../output/figures/meanshift_pca_clusters_temp_precipitation.png\"\n",
    "# plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "# plt.close()\n",
    "\n",
    "# --- Timeline plot ---\n",
    "df_with_inc_clusters = df.loc[df_inc_cleaned.index, ['adm_0_iso3', 'year']].copy()\n",
    "\n",
    "df_with_inc_clusters['meanshift_cluster'] = df_inc_cleaned['meanshift_cluster']\n",
    "\n",
    "\n",
    "df_with_inc_clusters = pd.merge(df, df_with_inc_clusters, on=['adm_0_iso3', 'year'], how='left')\n",
    "\n",
    "df_with_inc_clusters.to_csv(f'../output/data/incidence_meanshift_cluster_labels_by_country_year.csv', index=False)\n",
    "\n",
    "\n",
    "cluster_palette = {\n",
    "    cluster: colors[i]\n",
    "    for i, cluster in enumerate(np.unique(clusters))\n",
    "}\n",
    "\n",
    "\n",
    "legend_labels = [f\"Cluster {i}\" for i in np.unique(clusters)]\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    data=df_with_inc_clusters,\n",
    "    x='year',\n",
    "    y='adm_0_iso3',\n",
    "    hue='meanshift_cluster',\n",
    "    size='log_incidence_per_100k',\n",
    "    sizes=(1, 200),\n",
    "    palette=cluster_palette,\n",
    "    alpha=0.7,\n",
    "    edgecolor=chart[1],\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.title(\"Cluster membership over time by country (incidence)\", loc ='left')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Country\", labelpad=10)\n",
    "\n",
    "handles, labels = scatter.get_legend_handles_labels()\n",
    "\n",
    "hue_title_idx = labels.index(\"meanshift_cluster\")\n",
    "size_title_idx = labels.index(\"log_incidence_per_100k\")\n",
    "\n",
    "hue_handles = handles[hue_title_idx+1:size_title_idx]\n",
    "size_handles = handles[size_title_idx+1:]\n",
    "\n",
    "new_labels = legend_labels\n",
    "\n",
    "legend1 = plt.legend(\n",
    "    handles=hue_handles,\n",
    "    labels=new_labels,\n",
    "    title=\"Cluster\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    handletextpad=0.4,    \n",
    "    borderaxespad=0.1,    \n",
    "    alignment='left'\n",
    ")     \n",
    "\n",
    "legend2 = plt.legend(\n",
    "    handles=size_handles,\n",
    "    labels=labels[size_title_idx+1:],\n",
    "    title=\"Log incidence per 100k\",\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 0.90),\n",
    "    handletextpad=0.4,\n",
    "    borderaxespad=0.1,\n",
    "    alignment='left',\n",
    ")\n",
    "\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/incidence_timeline_meanshift_cluster_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e530b3b",
   "metadata": {},
   "source": [
    "## Aim 2: Predict endemicity status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a56b0",
   "metadata": {},
   "source": [
    "### Predict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "630cd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_labels = pd.read_csv('../output/data/incidence_kmeans_cluster_labels_by_country_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cb18781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['adm_0_iso3', 'year', 'region_un', 'TPopulation1July_per_1000',\n",
       "       'dengue_total', 'TPopulation1July', 'incidence', 'incidence_per_100k',\n",
       "       'log_incidence_per_100k', 'pop_density_per_km2',\n",
       "       'era5_hur_annual_mean_relative_humidity',\n",
       "       'era5_pr_annual_mean_precipitation', 'era5_tas_annual_mean_temp',\n",
       "       'era5_tnn_annual_min_temp', 'urban_pop', 'gdp_ppp', 'tourism_arrivals',\n",
       "       'tourism_departures', 'kmeans_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_labels.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d98d8d",
   "metadata": {},
   "source": [
    "##### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91798c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data (15549 samples, 10 feature dimensions)\n",
      "Training set mean accuracy: 0.8700\n",
      "Validation set mean accuracy: 0.8757\n",
      "SGD logistic regression validation accuracy: 0.8772\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocess data ---\n",
    "target = 'kmeans_cluster'\n",
    "class_features = [\n",
    "    'TPopulation1July',\n",
    "    'pop_density_per_km2',\n",
    "    'era5_hur_annual_mean_relative_humidity',\n",
    "    'era5_pr_annual_mean_precipitation', \n",
    "    'era5_tas_annual_mean_temp',\n",
    "    'era5_tnn_annual_min_temp', \n",
    "    'urban_pop', \n",
    "    'gdp_ppp', \n",
    "    'tourism_arrivals',\n",
    "    'tourism_departures'\n",
    "]\n",
    "\n",
    "df_model = df_with_labels[df_with_labels[target].notna()].copy()\n",
    "X = df_model[class_features]\n",
    "y = df_model[target].astype(int)\n",
    "\n",
    "y_unique = np.unique(y)\n",
    "y_names = ['Endemic', 'Non-endemic', 'Hypo-endemic'] \n",
    "\n",
    "print(\"Preprocessing data ({} samples, {} feature dimensions)\".format(X.shape[0], X.shape[1]))\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X))\n",
    "\n",
    "train_split_idx = int(0.7 * len(X))\n",
    "\n",
    "train_indices = shuffled_indices[0:train_split_idx]\n",
    "val_indices = shuffled_indices[train_split_idx:]\n",
    "\n",
    "# Select the examples from X and y to construct our training, validation, testing sets\n",
    "X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "X_val, y_val = X.iloc[val_indices], y.iloc[val_indices]\n",
    "\n",
    "# --- Impute missing values in training and validation data ---\n",
    "imputer = SimpleImputer(fill_value=-999, strategy='constant') \n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "\n",
    "# --- Apply Standard Scaling ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_train_class_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_val_class_scaled = scaler.transform(X_val_imputed)\n",
    "\n",
    "# --- Train and validate model ---\n",
    "\n",
    "class_model = LogisticRegression(penalty=None)\n",
    "class_model.fit(X_train_class_scaled, y_train)\n",
    "\n",
    "predictions_train = class_model.predict(X_train_class_scaled)\n",
    "score_train = skmetrics.accuracy_score(y_train, predictions_train)\n",
    "print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
    "\n",
    "predictions_val = class_model.predict(X_val_class_scaled)\n",
    "score_val = skmetrics.accuracy_score(y_val, predictions_val)\n",
    "print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "confusion_matrix = skmetrics.confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "confusion_matrix_plot = skmetrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=y_names)\n",
    "\n",
    "confusion_matrix_plot.plot()\n",
    "\n",
    "plt.savefig(\"../output/figures/confusion_matrix_all_features.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# --- ROC curve ---\n",
    "\n",
    "probabilities_val = class_model.predict_proba(X_val_class_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "one_hot_val = skpreprocessing.label_binarize(y_val, classes=y_unique)\n",
    "\n",
    "for class_id, color, label in zip(range(len(y_unique)), colors[4:7][::-1], y_names):\n",
    "    skmetrics.RocCurveDisplay.from_predictions(\n",
    "        one_hot_val[:, class_id],\n",
    "        probabilities_val[:, class_id],\n",
    "        name=f\"ROC curve for {label}\",\n",
    "        color=color,\n",
    "        ax=ax\n",
    "    )\n",
    "plt.savefig(\"../output/figures/roc_curve_all_features.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# --- SGD comparison ---\n",
    "sgd_model = SGDClassifier(\n",
    "    loss='log_loss',         \n",
    "    eta0=1e-3,\n",
    "    penalty=None,\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "sgd_model.fit(X_train_class_scaled, y_train)\n",
    "\n",
    "sgd_val_predictions = sgd_model.predict(X_val_class_scaled)\n",
    "sgd_val_accuracy = skmetrics.accuracy_score(y_val, sgd_val_predictions)\n",
    "print(\"SGD logistic regression validation accuracy: {:.4f}\".format(sgd_val_accuracy))\n",
    "\n",
    "# --- Compare accuracies ---\n",
    "models = ['LogisticRegression', 'SGDClassifier']\n",
    "accuracies = [score_val, sgd_val_accuracy]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(models, accuracies, color=[colors[1], colors[6]])\n",
    "plt.ylim(0, 1.1)\n",
    "plt.axhline(0.70, color=chart[0], linestyle='--', linewidth=1, label='Target accuracy (0.70)')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.title('Validation accuracy comparison')\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/validation_accuracy_comparison_all_features.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e802da",
   "metadata": {},
   "source": [
    "##### Incidence correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "332de261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data (15549 samples, 2 feature dimensions)\n",
      "Training set mean accuracy: 0.8747\n",
      "Validation set mean accuracy: 0.8699\n",
      "SGD logistic regression validation accuracy: 0.8673\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocess data ---\n",
    "target = 'kmeans_cluster'\n",
    "features = [\n",
    "    'era5_pr_annual_mean_precipitation',\n",
    "    'era5_tas_annual_mean_temp'\n",
    "]\n",
    "\n",
    "df_model = df_with_labels[df_with_labels[target].notna()].copy()\n",
    "X = df_model[features]\n",
    "y = df_model[target].astype(int)\n",
    "\n",
    "y_unique = np.unique(y)\n",
    "y_names = ['Endemic', 'Hypo-endemic', 'Non-endemic'] \n",
    "\n",
    "print(\"Preprocessing data ({} samples, {} feature dimensions)\".format(X.shape[0], X.shape[1]))\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X))\n",
    "\n",
    "train_split_idx = int(0.7 * len(X))\n",
    "\n",
    "train_indices = shuffled_indices[0:train_split_idx]\n",
    "val_indices = shuffled_indices[train_split_idx:]\n",
    "\n",
    "# Select the examples from X and y to construct our training, validation, testing sets\n",
    "X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "X_val, y_val = X.iloc[val_indices], y.iloc[val_indices]\n",
    "\n",
    "\n",
    "# --- Impute missing values in training and validation data ---\n",
    "imputer = SimpleImputer(fill_value=-999, strategy='constant') \n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "\n",
    "# --- Apply Standard Scaling ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)\n",
    "\n",
    "# --- Train and validate model ---\n",
    "model_scikit = LogisticRegression(penalty=None)\n",
    "model_scikit.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions_train = model_scikit.predict(X_train_scaled)\n",
    "score_train = skmetrics.accuracy_score(y_train, predictions_train)\n",
    "print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
    "\n",
    "predictions_val = model_scikit.predict(X_val_scaled)\n",
    "score_val = skmetrics.accuracy_score(y_val, predictions_val)\n",
    "print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "confusion_matrix = skmetrics.confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "confusion_matrix_plot = skmetrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=y_names)\n",
    "\n",
    "confusion_matrix_plot.plot()\n",
    "\n",
    "plt.savefig(\"../output/figures/confusion_matrix_incidence_correlated_features.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# --- ROC curve ---\n",
    "\n",
    "probabilities_val = model_scikit.predict_proba(X_val_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=figure_size)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "one_hot_val = skpreprocessing.label_binarize(y_val, classes=y_unique)\n",
    "\n",
    "for class_id, color, label in zip(range(len(y_unique)), colors[4:7][::-1], y_names):\n",
    "    skmetrics.RocCurveDisplay.from_predictions(\n",
    "        one_hot_val[:, class_id],\n",
    "        probabilities_val[:, class_id],\n",
    "        name=f\"ROC curve for {label}\",\n",
    "        color=color,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "plt.savefig(\"../output/figures/roc_curve_incidence_correlated_features.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# --- SGD comparison ---\n",
    "sgd_model = SGDClassifier(\n",
    "    loss='log_loss',         \n",
    "    eta0=1e-3,\n",
    "    penalty=None,\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "sgd_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "sgd_val_predictions = sgd_model.predict(X_val_scaled)\n",
    "sgd_val_accuracy = skmetrics.accuracy_score(y_val, sgd_val_predictions)\n",
    "print(\"SGD logistic regression validation accuracy: {:.4f}\".format(sgd_val_accuracy))\n",
    "\n",
    "# --- Compare accuracies ---\n",
    "models = ['LogisticRegression', 'SGDClassifier']\n",
    "accuracies = [score_val, sgd_val_accuracy]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(models, accuracies, color=[colors[1], colors[6]])\n",
    "plt.ylim(0, 1.1)\n",
    "plt.axhline(0.70, color=chart[0], linestyle='--', linewidth=1, label='Target accuracy (0.70)')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.title('Validation accuracy comparison')\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/validation_accuracy_comparison_incidence_cor.png\", dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434ddb0",
   "metadata": {},
   "source": [
    "### Predict incidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a2d115",
   "metadata": {},
   "source": [
    "#### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b717d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.5620\n",
      "Train R^2: 0.3775\n",
      "Validation MSE: 1.6361\n",
      "Validation R^2: 0.3659\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocess data ---\n",
    "target = 'log_incidence_per_100k'  \n",
    "incidence_features = [\n",
    "    'TPopulation1July',\n",
    "    'pop_density_per_km2',\n",
    "    'era5_hur_annual_mean_relative_humidity',\n",
    "    'era5_pr_annual_mean_precipitation', \n",
    "    'era5_tas_annual_mean_temp',\n",
    "    'era5_tnn_annual_min_temp', \n",
    "    'urban_pop', \n",
    "    'gdp_ppp', \n",
    "    'tourism_arrivals',\n",
    "    'tourism_departures'\n",
    "]\n",
    "\n",
    "df_model = df_with_labels[df_with_labels[target].notna()].copy()\n",
    "X = df_model[incidence_features]\n",
    "y = df_model[target]\n",
    "\n",
    "# Train-test split\n",
    "shuffled_indices = np.random.permutation(len(X))\n",
    "train_split_idx = int(0.8 * len(X))\n",
    "train_indices = shuffled_indices[:train_split_idx]\n",
    "val_indices = shuffled_indices[train_split_idx:]\n",
    "\n",
    "X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "X_val, y_val = X.iloc[val_indices], y.iloc[val_indices]\n",
    "\n",
    "# Impute missing values \n",
    "imputer = SimpleImputer(fill_value=-999, strategy='constant')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "\n",
    "# --- Generate polynomial features ---\n",
    "poly = skpreprocessing.PolynomialFeatures(degree=3, include_bias=True)  \n",
    "X_train_poly = poly.fit_transform(X_train_imputed)\n",
    "poly_feature_names = poly.get_feature_names_out().tolist()\n",
    "X_val_poly = poly.transform(X_val_imputed)\n",
    "\n",
    "# --- Scale ---\n",
    "scaler = skpreprocessing.StandardScaler()\n",
    "X_train_incidence_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_val_incidence_scaled = scaler.transform(X_val_poly)\n",
    "\n",
    "# --- Fit polynomial regression ---\n",
    "incidence_model = Ridge(alpha=1.0)\n",
    "incidence_model.fit(X_train_incidence_scaled, y_train)\n",
    "\n",
    "# --- Training Predictions ---\n",
    "train_predictions = incidence_model.predict(X_train_incidence_scaled)\n",
    "\n",
    "# --- Training Evaluation ---\n",
    "train_mse = skmetrics.mean_squared_error(y_train, train_predictions)\n",
    "train_r2 = skmetrics.r2_score(y_train, train_predictions)\n",
    "\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Train R^2: {train_r2:.4f}\")\n",
    "\n",
    "\n",
    "# --- Predictions ---\n",
    "val_predictions = incidence_model.predict(X_val_incidence_scaled)\n",
    "\n",
    "# --- Evaluate ---\n",
    "mse = skmetrics.mean_squared_error(y_val, val_predictions)\n",
    "r2 = skmetrics.r2_score(y_val, val_predictions)\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation R^2: {r2:.4f}\")\n",
    "\n",
    "# --- Residuals ---\n",
    "residuals = y_val - val_predictions\n",
    "\n",
    "# Residual plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(val_predictions, residuals, alpha=0.6, edgecolor=chart[1], c=colors[0])\n",
    "plt.axhline(0, color=colors[8], linestyle='--', linewidth=1)\n",
    "plt.xlabel('Predicted Log Incidence per 100k')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Polynomial Regression)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/residuals_poly_regression_all_features.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Residual distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(residuals, kde=True, bins=30, color=colors[0])\n",
    "plt.axvline(0, color=colors[8], linestyle='--')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/figures/residual_distribution_poly_regression_all_features.png\", dpi=300)\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf012a",
   "metadata": {},
   "source": [
    "## Aim 3: Use explainable AI to interpret predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = class_model\n",
    "explainer = shap.Explainer(model, X_train_class_scaled)  \n",
    "shap_values = explainer(X_val_class_scaled)  \n",
    "plt.figure(figsize=(8,6))\n",
    "shap.summary_plot(shap_values, X_val_class_scaled, feature_names=class_features, show=False)\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('../output/figures/classification_shap_values_beeswarm_customized.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac05f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
