{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a91bc",
   "metadata": {},
   "source": [
    "**Beyond sporadic outbreaks: Classifying and explaining dengue endemicity over scenarios of global change.**\n",
    "\n",
    "*CPSC 581: Machine Learning*\n",
    "\n",
    "*Yale University*\n",
    "\n",
    "*Instructor: Alex Wong*\n",
    "\n",
    "*Student: Hailey Robertson*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7b335",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1065aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import font_manager\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon \n",
    "from shapely.geometry.base import BaseGeometry \n",
    "import country_converter as coco\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import xarray as xr\n",
    "import regionmask\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d043a2",
   "metadata": {},
   "source": [
    "#### Nice defaults for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e283e851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAABuCAYAAAATf49HAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABEVJREFUeJzt3M1KVHEcxvGj+DaGS6H3cIJC3NVG8BZcW7fRvm2X0S14CUHUxkWQW4tQLIjKCLIZM+VEEdRKzzk2/Hnk89m4Ocz8eBYDX2ZwrK7rugIAAIBQ46UPAAAAgLMQtgAAAEQTtgAAAEQTtgAAAEQTtgAAAEQTtgAAAEQTtgAAAEQTtgAAAESbaPrglfuPRnvJOXR9aab0CZFW794qfUKc168elz4h0uqdxh+B/LEyPVf6hEjHG7ulT4jz9cV86RMibexeK31CnGc/jkqfEGnx0lbpE+KsPHhY+oRIy8vLjZ7zjS0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRhC0AAADRJpo8NBwOq/HB3uivOWeO9qZLnxBpb3e29Alx9j8OS58Q6d1Oo49A/rE1NVb6hEjHHw5LnxBnsD8ofUKk94dfSp8QZ//ouPQJkT4NDkqfEGd7e7v0CZHm5uaqfr9f9Xq9E58bq+u6Pu3FXm5uVvfW1v7nfQAAAHCq9fX1amlp6exh+2nvc/Xk6fOqf+Nq1ZvxLWRTw4Pv1Zudt3ZrwWbd2K09m3Vjt/Zs1o3d2rNZN3Zrz2bd2K27Jt/YNvod3q8XWVjoV4u3b1YXZk9+Qf76NhhW9fiU3VqwWTd2a89m3ditPZt1Y7f2bNaN3dqzWTd2Gy3/PAoAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIBowhYAAIDzH7aTkxPV5Yvzv//SnN3as1k3dmvPZt3YrT2bdWO39mzWjd3as1k3dhutsbqu6xG/BwAAAIyMnyIDAAAQTdgCAAAQTdgCAAAQTdgCAAAQTdgCAAAQTdgCAAAQTdgCAAAQTdgCAABQJfsJDC3GeBIlsl0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAABuCAYAAABBY1qRAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAnNJREFUeJzt3DFqFGEYx+EvYaOugmARCJogGQvRjZW9x4h3SJXCc+0RhIAHMBdIEKzEWCib0caRiGCa7P/bdcM4+jzNFvMxvLzFjxkWZq3ruq4AcKX1qy8BcEEoAQKhBAiEEiAQSoBAKAECoQQIhBIgGJVKz/b2ao/yy9PnL/oeYZBeHR70PcLg7Gw/6HuEQdravFd1zhMlQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgSjUqFt2/K962qOcsn57HPfIwzS6elJ3yMMTjv70vcIg3T24W5pmqaMx+O559a6Lhfw7fFxebm/v8r5AP4K0+m0TCaTPw/lx7NP5fXRm9I83C7jWzdXOeM/rf36rZy8e29vC7Cz5djb8mqeKKtevS9usrvblCePH5U7t+ffkN9m523p1m/Y2wLsbDn2dr38mQMQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACrCOXGxqjc39r8+Us9e1ucnS3H3q5X1deDAP5nXr0BAqEECIQSIBBKgEAoAQKhBAiEEiAQSoAy3w9wAlJ4cflk8AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['#1A5784', '#38828C', '#84B5B2', '#60904F', '#ACC253', '#E7C960', '#E49243', '#D75F58', '#A04275', '#633D71', '#8c564b', '#c7c7c7']\n",
    "sns.palplot(sns.color_palette(colors))\n",
    "\n",
    "# Define chart color palette\n",
    "chart = ['#2C2B2B','#565E69','#CACED3','#E7EAEE']\n",
    "sns.palplot(sns.color_palette(chart))\n",
    "\n",
    "# Define constants\n",
    "figure_size = (20,6)\n",
    "\n",
    "# Set background\n",
    "sns.set_context('talk') #change the size from small to medium\n",
    "sns.set_style('white') #change bg to white\n",
    "\n",
    "# Add every font at the specified location\n",
    "font_dir = ['/Users/haileyrobertson/Library/Fonts']\n",
    "for font in font_manager.findSystemFonts(font_dir):\n",
    "    font_manager.fontManager.addfont(font)\n",
    "    \n",
    "# Set font family globally\n",
    "plt.rcParams['font.family'] = 'Open Sans'\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "\n",
    "# Set margins\n",
    "plt.rcParams['axes.xmargin'] = 0.1\n",
    "plt.rcParams['axes.ymargin'] = 0.1\n",
    "\n",
    "# Define list of date formats\n",
    "zfmts = ['', '%Y','%b\\n%Y', '%b', '%b-%d', '%H:%M', '%H:%M']\n",
    "\n",
    "# Format axes \n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.bottom'] = True\n",
    "plt.rcParams['axes.spines.left'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.titlepad'] = 30\n",
    "\n",
    "# Format ticks\n",
    "plt.rcParams[\"xtick.direction\"] = \"out\"\n",
    "plt.rcParams['xtick.major.size'] = 10\n",
    "plt.rcParams['xtick.minor.size'] = 10\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['xtick.color'] = chart[2]\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['xtick.labelcolor'] = chart[1]\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.major.pad\"] = 0\n",
    "plt.rcParams[\"ytick.minor.pad\"] = 0\n",
    "plt.rcParams[\"ytick.major.size\"] = 10\n",
    "plt.rcParams[\"ytick.minor.size\"] = 10\n",
    "plt.rcParams[\"ytick.color\"] = chart[2]\n",
    "plt.rcParams[\"ytick.major.width\"] = 0.1\n",
    "plt.rcParams[\"ytick.minor.width\"] = 0.1\n",
    "plt.rcParams[\"ytick.labelcolor\"] = chart[1]\n",
    "plt.rcParams[\"ytick.labelsize\"] = 8\n",
    "\n",
    "\n",
    "# Adjust fontdict for title\n",
    "titlefont = {'family': 'Open Sans',\n",
    "             'color':  chart[0], \n",
    "             'weight': 400,\n",
    "             'size': 20}\n",
    "\n",
    "# Set grid style\n",
    "plt.rcParams['grid.color'] = chart[2]\n",
    "plt.rcParams['grid.linestyle'] = 'dashed'\n",
    "plt.rcParams['grid.linewidth']=0.5\n",
    "\n",
    "# Set legend style\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['legend.handlelength'] = 1\n",
    "plt.rcParams['legend.handleheight'] = 1.125\n",
    "\n",
    "\n",
    "\n",
    "# Set axis labels\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.labelcolor'] = chart[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d2104",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be54bd8",
   "metadata": {},
   "source": [
    "#### Load Open Dengue and align country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cb7e2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load dengue data ---\n",
    "url = 'https://raw.githubusercontent.com/OpenDengue/master-repo/refs/heads/main/data/raw_data/masterDB_V1.2.csv'\n",
    "open_dengue = pd.read_csv(url, index_col=0, encoding='latin-1').reset_index()\n",
    "\n",
    "open_dengue[\"adm_0_iso3\"] = coco.convert(\n",
    "    names=open_dengue[\"adm_0_name\"],\n",
    "    to='ISO3',\n",
    "    not_found=\"missing\"\n",
    ")\n",
    "\n",
    "date_cols = ['calendar_start_date', 'calendar_end_date']\n",
    "open_dengue[date_cols] = open_dengue[date_cols].apply(pd.to_datetime)\n",
    "open_dengue['year'] = open_dengue['calendar_start_date'].dt.year\n",
    "open_dengue['month'] = open_dengue['calendar_start_date'].dt.month\n",
    "\n",
    "#  Not all periods are the same length – some places report every year, some every month, some every week\n",
    "open_dengue[\"date_diff\"] = (open_dengue[\"calendar_end_date\"] - open_dengue[\"calendar_start_date\"]).dt.days\n",
    "\n",
    "# Find geo resolution and combine places\n",
    "def highest_geo_resolution(row):\n",
    "    if pd.notna(row['adm_2_name']) and row['adm_2_name'] != '':\n",
    "        return 'adm_2'\n",
    "    elif pd.notna(row['adm_1_name']) and row['adm_1_name'] != '':\n",
    "        return 'adm_1'\n",
    "    else:\n",
    "        return 'adm_0'\n",
    "\n",
    "open_dengue['geo_resolution'] = open_dengue.apply(highest_geo_resolution, axis=1)\n",
    "\n",
    "open_dengue['combined_place'] = open_dengue[['adm_2_name', 'adm_1_name', 'adm_0_name']].apply(\n",
    "    lambda x: ', '.join([str(place) for place in x if pd.notna(place) and place != '']), axis=1)\n",
    "\n",
    "# --- Load world geometry ---\n",
    "world = gpd.read_file(\"../data/ne_110m_admin_0_countries\")\n",
    "\n",
    "world = world.rename(columns={\n",
    "    \"ADM0_A3\": \"adm_0_iso3\",\n",
    "    \"ADMIN\": \"adm_0_name\",\n",
    "    \"REGION_UN\": \"region_un\",\n",
    "    \"geometry\": \"adm_0_geometry\"\n",
    "})[[\"adm_0_iso3\", \"adm_0_name\", \"region_un\", \"adm_0_geometry\"]]\n",
    "\n",
    "world = world.sort_values(by=\"adm_0_name\")\n",
    "\n",
    "# --- Fix known issues ---\n",
    "# Split out French Guiana from France\n",
    "france_idx = world['adm_0_name'] == 'France'\n",
    "france_geom = world.loc[france_idx, 'adm_0_geometry'].values[0]\n",
    "\n",
    "if isinstance(france_geom, MultiPolygon):\n",
    "    polygons = list(france_geom.geoms)\n",
    "    french_guiana_polygon = next((poly for poly in polygons if poly.bounds[0] < -50 and poly.bounds[2] > -54), None)\n",
    "\n",
    "    if french_guiana_polygon:\n",
    "        # Remove French Guiana from France\n",
    "        remaining_polygons = [poly for poly in polygons if poly != french_guiana_polygon]\n",
    "        world.loc[france_idx, 'adm_0_geometry'] = MultiPolygon(remaining_polygons)\n",
    "\n",
    "        # Add French Guiana as separate entry\n",
    "        french_guiana_row = {\n",
    "            'adm_0_iso3': 'GUF',\n",
    "            'adm_0_name': 'French Guiana',\n",
    "            'region_un': 'Americas',\n",
    "            'adm_0_geometry': french_guiana_polygon\n",
    "        }\n",
    "        world = pd.concat([world, gpd.GeoDataFrame([french_guiana_row], geometry='adm_0_geometry')], ignore_index=True)\n",
    "\n",
    "# Patch ISO3 codes for special cases\n",
    "world.loc[world['adm_0_name'] == 'Norway', 'adm_0_iso3'] = 'NOR'\n",
    "world.loc[world['adm_0_name'] == 'Somaliland', 'adm_0_iso3'] = 'SOM'\n",
    "world.loc[world['adm_0_name'] == 'Kosovo', 'adm_0_iso3'] = 'RKS'\n",
    "world.loc[world['adm_0_name'] == 'South Sudan', 'adm_0_iso3'] = 'SSD'\n",
    "\n",
    "# --- Merge dengue data with geometry ---\n",
    "dengue = pd.merge(open_dengue, world, on='adm_0_iso3', how='outer', suffixes=('', '_world'))\n",
    "\n",
    "# Fill UN regions with mapping\n",
    "with open('../data/un_regions.json') as f:\n",
    "    countries = json.load(f)\n",
    "dengue['region_un'] = dengue['adm_0_iso3'].map(countries).fillna(\"Other\")\n",
    "\n",
    "# Aesthetics\n",
    "dengue = dengue.drop(columns=['adm_0_name_world'])\n",
    "front_cols = ['adm_0_name', 'adm_0_iso3']\n",
    "geometry_col = ['adm_0_geometry']\n",
    "other_cols = [col for col in dengue.columns if col not in front_cols + geometry_col]\n",
    "dengue = dengue[front_cols + other_cols + geometry_col]\n",
    "\n",
    "# Fill the years so that ISO3s stay as a time series\n",
    "years = pd.Series(range(int(1950), int(dengue['year'].max()) + 1))\n",
    "unique_iso3 = dengue['adm_0_iso3'].unique()\n",
    "\n",
    "all_combinations = pd.MultiIndex.from_product([unique_iso3, years], names=['adm_0_iso3', 'year']).to_frame(index=False)\n",
    "\n",
    "dengue_ts = all_combinations.merge(dengue, on=['adm_0_iso3', 'year'], how='outer')\n",
    "\n",
    "# List of columns to fill\n",
    "columns_to_fill = ['adm_0_name', 'region_un', 'adm_0_geometry', 'geo_resolution', 'combined_place']\n",
    "\n",
    "# Fill NaN values for multiple columns\n",
    "dengue_ts[columns_to_fill] = (\n",
    "    dengue_ts\n",
    "    .groupby('adm_0_iso3')[columns_to_fill]\n",
    "    .transform(lambda x: x.ffill().bfill())\n",
    ")\n",
    "\n",
    "dengue_ts = dengue_ts.dropna(subset=['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab822c",
   "metadata": {},
   "source": [
    "#### Load other predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d7e874ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean population data ---\n",
    "pop = pd.read_csv(\"../data/WPP2022_Demographic_Indicators_Medium.csv\", dtype={'Time': int,'TPopulation1July':float}, low_memory=False)\n",
    "columns = ['ISO3_code','Time','TPopulation1July']\n",
    "pop = pop[columns]\n",
    "pop.rename(columns={\"ISO3_code\": \"adm_0_iso3\", \"Time\": \"year\", \"TPopulation1July\": \"TPopulation1July_per_1000\"}, inplace=True)\n",
    "\n",
    "# --- Clean population density ---\n",
    "pop_dns = pd.read_csv(\"../data/pop_density_wb.csv\")\n",
    "year_cols = [col for col in pop_dns.columns if col.isdigit()]\n",
    "pop_dns = pop_dns.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"pop_density_per_km2\"\n",
    ")\n",
    "pop_dns.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "\n",
    "# --- Clean climate data ---\n",
    "clim = ['era5_tnn_annual_min_temp.csv', 'era5_tas_annual_mean_temp.csv', 'era5_pr_annual_mean_precipitation.csv', 'era5_hur_annual_mean_relative_humidity.csv']\n",
    "\n",
    "climate_df = {}\n",
    "\n",
    "for file in clim:\n",
    "    data = pd.read_csv(f\"../data/{file}\")\n",
    "    \n",
    "    year_cols = [col for col in data.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "    melted = data.melt(\n",
    "        id_vars=[\"code\"],\n",
    "        value_vars=year_cols,\n",
    "        var_name=\"year\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    melted[\"year\"] = melted[\"year\"].str.extract(r\"(\\d{4})\")\n",
    "    melted.rename(columns={\"code\": \"adm_0_iso3\"}, inplace=True)\n",
    "    indicator = file.replace('.csv', '')  \n",
    "    melted.rename(columns={\"value\": indicator}, inplace=True)\n",
    "    climate_df[indicator] = melted\n",
    "\n",
    "# --- Clean urbanization data ---\n",
    "urban = pd.read_csv(\"../data/urban_growth_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in urban.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "urban = urban.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"urban_growth\"\n",
    ")\n",
    "urban.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "urban['year'] = urban['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean GDP data ---\n",
    "gdp = pd.read_csv(\"../data/gdp_ppp_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in gdp.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "gdp = gdp.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"gdp_ppp\"\n",
    ")\n",
    "gdp.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "gdp['year'] = gdp['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean arrivals data ---\n",
    "arr = pd.read_csv(\"../data/tourism_arrivals_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in arr.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "arr = arr.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"tourism_arrivals\"\n",
    ")\n",
    "arr.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "arr['year'] = arr['year'].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "# --- Clean departures data ---\n",
    "dep = pd.read_csv(\"../data/tourism_departures_wb.csv\")\n",
    "\n",
    "year_cols = [col for col in dep.columns if col.startswith('19') or col.startswith('20')]\n",
    "\n",
    "dep = dep.melt(\n",
    "    id_vars=[\"Country Code\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"tourism_departures\"\n",
    ")\n",
    "dep.rename(columns={\"Country Code\": \"adm_0_iso3\"}, inplace=True)\n",
    "dep['year'] = dep['year'].str.extract(r\"(\\d{4})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c8af027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_dfs = [pop, \n",
    "                 pop_dns, \n",
    "                 climate_df['era5_hur_annual_mean_relative_humidity'],\n",
    "                 climate_df['era5_pr_annual_mean_precipitation'],\n",
    "                 climate_df['era5_tas_annual_mean_temp'],\n",
    "                 climate_df['era5_tnn_annual_min_temp'],\n",
    "                 urban,\n",
    "                 gdp,\n",
    "                 arr,\n",
    "                 dep]\n",
    "\n",
    "merged_df = dengue_ts.copy()\n",
    "\n",
    "for i, predictor in enumerate(predictor_dfs):\n",
    "    predictor['year'] = pd.to_numeric(predictor['year'], errors='coerce').astype('Int64')\n",
    "    merged_df = merged_df.merge(predictor, how='outer', on=['adm_0_iso3', 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831a441",
   "metadata": {},
   "source": [
    "#### Calculate incidence rates - get to clean dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85374d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TPopulation1July_per_1000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TPopulation1July_per_1000'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m grouped_df \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madm_0_iso3\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m                                 ])\u001b[38;5;241m.\u001b[39magg({\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdengue_total\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m })\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m----> 7\u001b[0m grouped_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTPopulation1July\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgrouped_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTPopulation1July_per_1000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      8\u001b[0m grouped_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincidence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grouped_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdengue_total\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m grouped_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTPopulation1July\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m grouped_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincidence_per_100k\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grouped_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincidence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100000\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/GitHub/robertson-cpsc581/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TPopulation1July_per_1000'"
     ]
    }
   ],
   "source": [
    "grouped_df = merged_df.groupby(['adm_0_iso3', \n",
    "                                'year',\n",
    "                                'region_un',\n",
    "                                'TPopulation1July_per_1000']).agg({\n",
    "    'dengue_total': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "grouped_df['TPopulation1July'] = grouped_df['TPopulation1July_per_1000'] * 1000\n",
    "grouped_df['incidence'] = grouped_df['dengue_total'] / grouped_df['TPopulation1July']\n",
    "grouped_df['incidence_per_100k'] = grouped_df['incidence'] * 100000\n",
    "grouped_df['log_incidence_per_100k'] = np.log1p(grouped_df['incidence_per_100k'])  # log(x + 1) to handle zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f39b3d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['adm_0_iso3', 'year', 'region_un', 'TPopulation1July_per_1000',\n",
      "       'pop_density_per_km2', 'era5_hur_annual_mean_relative_humidity',\n",
      "       'era5_pr_annual_mean_precipitation', 'era5_tas_annual_mean_temp',\n",
      "       'era5_tnn_annual_min_temp', 'urban_growth', 'gdp_ppp',\n",
      "       'tourism_arrivals', 'tourism_departures', 'dengue_total',\n",
      "       'TPopulation1July', 'incidence', 'incidence_per_100k',\n",
      "       'log_incidence_per_100k'],\n",
      "      dtype='object')\n",
      "(11140, 18)\n"
     ]
    }
   ],
   "source": [
    "##  NOTE: Start here if you don't want to run all of the above :) \n",
    "# grouped_df.to_csv('../output/data/cleaned_master_df.csv', index=False)\n",
    "\n",
    "# --- Load cleaned data ---\n",
    "df = pd.read_csv('../output/data/cleaned_master_df.csv')\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2681f",
   "metadata": {},
   "source": [
    "##### View data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3443e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "heatmap_data = df.pivot(index='adm_0_iso3', columns='year', values='incidence_per_100k').fillna(0)\n",
    "print(len(heatmap_data))\n",
    "rocket_r_cmap = sns.color_palette(\"rocket_r\", as_cmap=True)\n",
    "custom_cmap = ListedColormap([\"white\"] + list(rocket_r_cmap(np.linspace(0.01, 1, 1000))))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 35))  # Adjust size as needed\n",
    "\n",
    "custom_colors = [\"white\", colors[1]]\n",
    "\n",
    "\n",
    "# Plot the heatmap\n",
    "im = sns.heatmap(data=heatmap_data, \n",
    "                 cmap=custom_cmap, \n",
    "                 annot=False, \n",
    "                 fmt='g', \n",
    "                 linewidths=0.1, \n",
    "                 ax=ax, \n",
    "                 vmin=0, \n",
    "                 vmax=1000, \n",
    "                 cbar=True, \n",
    "                 cbar_kws={'orientation': 'horizontal', 'pad': 0.02, 'label':'Incidence rate per 100,000'})\n",
    "\n",
    "# Set tick labels and axis labels\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Country')\n",
    "ax.set_title(\"Incidence rate per 100,000 over time\",loc='left')\n",
    "\n",
    "# Set tick parameters\n",
    "ax.tick_params(axis='x', pad=5, length=0, labelsize=10, width=10)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "ax.tick_params(axis='y', pad=10, width=30, labelsize=10, rotation=0)\n",
    "ax.tick_params(top=False, bottom=True, labeltop=False, labelbottom=True)\n",
    "\n",
    "# Hide spines\n",
    "ax.spines.bottom.set_visible(False)\n",
    "ax.spines.left.set_visible(False)\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.tight_layout()\n",
    "filename = f\"../output/figures/open_dengue_incidence_1950-2023_full_covg.png\"\n",
    "plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17857d12",
   "metadata": {},
   "source": [
    "## Aim 1: Classify countries by endemicity status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ded94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "['ABW' 'AFG' 'ARG' 'ASM' 'ATG' 'AUS' 'BGD' 'BHS' 'BLZ' 'BMU' 'BOL' 'BRA'\n",
      " 'BRB' 'BRN' 'BTN' 'CHL' 'CHN' 'COL' 'CRI' 'CUB' 'CUW' 'CYM' 'DMA' 'DOM'\n",
      " 'ECU' 'FJI' 'FSM' 'GRD' 'GTM' 'GUM' 'GUY' 'HKG' 'HND' 'HTI' 'IDN' 'IND'\n",
      " 'JAM' 'JPN' 'KHM' 'KIR' 'KNA' 'LAO' 'LCA' 'LKA' 'MAC' 'MAF' 'MDV' 'MEX'\n",
      " 'MHL' 'MMR' 'MNG' 'MNP' 'MYS' 'NCL' 'NIC' 'NPL' 'NRU' 'PAK' 'PAN' 'PER'\n",
      " 'PHL' 'PLW' 'PNG' 'PRI' 'PRY' 'PYF' 'SAU' 'SDN' 'SGP' 'SLB' 'SLV' 'SUR'\n",
      " 'SXM' 'TCA' 'THA' 'TLS' 'TON' 'TTO' 'TUV' 'URY' 'USA' 'VCT' 'VEN' 'VGB'\n",
      " 'VIR' 'VNM' 'VUT' 'WSM' 'YEM']\n"
     ]
    }
   ],
   "source": [
    "# Find unique countries that have ever had incidence > 0\n",
    "countries_with_incidence = df[df['incidence'] > 0]['adm_0_iso3'].unique()\n",
    "\n",
    "# Filter the DataFrame to include only rows for these countries\n",
    "any_dengue = df[df['adm_0_iso3'].isin(countries_with_incidence)]\n",
    "any_dengue.sort_values(by=[\"adm_0_iso3\", \"year\"], ascending=[True, True], inplace=True)\n",
    "\n",
    "any_dengue_list = any_dengue.adm_0_iso3.unique()\n",
    "print(len(any_dengue.adm_0_iso3.unique()))\n",
    "print(any_dengue.adm_0_iso3.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf012a",
   "metadata": {},
   "source": [
    "## Aim 2: Use explainable AI to interpret predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d1f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
