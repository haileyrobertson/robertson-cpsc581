{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 7: Ridge Regression and Polynomial Feature Expansion**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "\n",
        "4. Move the 04_exercise_ridge_regression_poly_expansion.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/07_exercise_ridge_regression_poly_expansion.ipynb\n",
        "```\n",
        "\n",
        "In this exercise, we will optimize a linear and ridge regression with polynomial feature expansion to experiment with over and underfitting.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training and testing scores.\n",
        "\n",
        "```\n",
        "Report training and testing scores here.\n",
        "\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: Doe, Jane (Please write names in <Last Name, First Name> format)\n",
        "\n",
        "Collaboration details: Discussed ... implementation details with Jane Doe.\n",
        "```"
      ],
      "metadata": {
        "id": "_0fsGaVMMpwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages"
      ],
      "metadata": {
        "id": "wxeZsiCGC0J8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uumvcyiQ-k21"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "import sklearn.preprocessing as skpreprocess\n",
        "from sklearn.linear_model import LinearRegression as LinearRegressionSciKit\n",
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Ridge Regression with Gradient Descent optimizer"
      ],
      "metadata": {
        "id": "0xOsR-kJIlD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RidgeRegression(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Define private variables\n",
        "        self.__weights = None\n",
        "\n",
        "    def __fit_normal_equation(self, X, y, weight_decay=0):\n",
        "        '''\n",
        "        Fits the model to x and y via normal equation\n",
        "\n",
        "        Arg(s):\n",
        "            X : numpy\n",
        "                N x d feature vector\n",
        "            y : numpy\n",
        "                N x 1 ground-truth label\n",
        "            weight_decay : float\n",
        "                weight of weight decay term\n",
        "        '''\n",
        "\n",
        "        # TODO: Implement the __fit_normal_equation function\n",
        "        self.__weights = None\n",
        "\n",
        "    def fit(self, X, y, weight_decay=0, solver='normal_equation'):\n",
        "        '''\n",
        "        Fits the model to x and y by solving least squares\n",
        "        using normal equation\n",
        "\n",
        "        Arg(s):\n",
        "            X : numpy[float32]\n",
        "                N x d feature vector\n",
        "            y : numpy[float32]\n",
        "                N ground-truth label\n",
        "            weight_decay : float\n",
        "                weight of weight decay term\n",
        "            solver : str\n",
        "                solver types: normal_equation\n",
        "        '''\n",
        "\n",
        "        y = np.expand_dims(y, axis=1)\n",
        "\n",
        "        # TODO: Implement the fit function\n",
        "\n",
        "        if solver == 'normal_equation':\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError('Encountered unsupported solver: {}'.format(solver))\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Predicts the real value for each feature vector x\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                N x d feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : N x 1 real value vector (\\hat{y})\n",
        "        '''\n",
        "\n",
        "        # TODO: Implement the predict function\n",
        "\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "BqpoUg5fIlgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function for plotting"
      ],
      "metadata": {
        "id": "Gcb2TArNKvf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(axis,\n",
        "                 x_values,\n",
        "                 y_values,\n",
        "                 labels,\n",
        "                 colors,\n",
        "                 x_limits,\n",
        "                 y_limits,\n",
        "                 x_label,\n",
        "                 y_label):\n",
        "    '''\n",
        "    Plots x and y values using line plot with labels and colors\n",
        "\n",
        "    Args:\n",
        "        axis :  pyplot.ax\n",
        "            matplotlib subplot axis\n",
        "        x_values : list[numpy[float32]]\n",
        "            list of numpy array of x values\n",
        "        y_values : list[numpy[float32]]\n",
        "            list of numpy array of y values\n",
        "        labels : str\n",
        "            list of names for legend\n",
        "        colors : str\n",
        "            colors for each line\n",
        "        x_limits : list[float32]\n",
        "            min and max values of x axis\n",
        "        y_limits : list[float32]\n",
        "            min and max values of y axis\n",
        "        x_label : list[float32]\n",
        "            name of x axis\n",
        "        y_label : list[float32]\n",
        "            name of y axis\n",
        "    '''\n",
        "\n",
        "    # Iterate through x_values, y_values, labels, and colors and plot them\n",
        "    # with associated legend\n",
        "    for x, y, label, color in zip(x_values, y_values, labels, colors):\n",
        "        axis.plot(x, y, marker='o', color=color, label=label)\n",
        "        axis.legend(loc='best')\n",
        "\n",
        "    # Set x and y limits\n",
        "    axis.set_xlim(x_limits)\n",
        "    axis.set_ylim(y_limits)\n",
        "\n",
        "    # Set x and y labels\n",
        "    axis.set_xlabel(x_label)\n",
        "    axis.set_ylabel(y_label)"
      ],
      "metadata": {
        "id": "nBXk85ASVDOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset"
      ],
      "metadata": {
        "id": "TTvfLetIQmk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create synthetic dataset\n",
        "X, y = skdata.make_friedman1(n_samples=2000, n_features=8, noise=6)\n",
        "\n",
        "# Shuffle the dataset based on sample indices\n",
        "shuffled_indices = np.random.permutation(X.shape[0])\n",
        "\n",
        "# Choose the first 80% as training set and the rest as testing\n",
        "train_split_idx = int(0.80 * X.shape[0])\n",
        "\n",
        "train_indices = shuffled_indices[0:train_split_idx]\n",
        "test_indices = shuffled_indices[train_split_idx:]\n",
        "\n",
        "# Select the examples from x and y to construct our training, validation, testing sets\n",
        "X_train, y_train = X[train_indices, :], y[train_indices]\n",
        "X_test, y_test = X[test_indices, :], y[test_indices]"
      ],
      "metadata": {
        "id": "k9OD26CBQmHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 1: Demonstrate that linear regression will overfit if we use high degrees of polynomial expansion"
      ],
      "metadata": {
        "id": "Ibd3k8JeQvHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Experiment 1: Overfitting Linear Regression with Polynomial Expansion')\n",
        "\n",
        "# TODO: Initialize a list containing 1 to 6 as the degrees for polynomial expansion\n",
        "degrees = []\n",
        "\n",
        "# Initialize empty lists to store scores for MSE\n",
        "scores_mse_linear_overfit_train = []\n",
        "scores_mse_linear_overfit_test = []\n",
        "\n",
        "for degree in degrees:\n",
        "\n",
        "    # TODO: Initialize polynomial expansion\n",
        "    poly_transform = None\n",
        "\n",
        "    # TODO: Compute the polynomial terms needed for the data\n",
        "\n",
        "\n",
        "    # TODO: Transform the data by nonlinear mapping\n",
        "    X_poly_train = None\n",
        "    X_poly_test = None\n",
        "\n",
        "    # TODO: Initialize sci-kit linear regression model\n",
        "    model_linear_overfit = None\n",
        "\n",
        "    # TODO: Train linear regression model\n",
        "\n",
        "\n",
        "    print('Results for linear regression model with degree-{} polynomial expansion'.format(degree))\n",
        "\n",
        "    # TODO: Test model on training set\n",
        "    predictions_train = None\n",
        "    score_mse_linear_overfit_train = 0.0\n",
        "    print('Training set mean squared error: {:.4f}'.format(score_mse_linear_overfit_train))\n",
        "\n",
        "    # TODO: Save MSE training scores\n",
        "\n",
        "\n",
        "    # TODO: Test model on testing set\n",
        "    predictions_test = None\n",
        "    score_mse_linear_overfit_test = 0.0\n",
        "    print('Testing set mean squared error: {:.4f}'.format(score_mse_linear_overfit_test))\n",
        "\n",
        "    # TODO: Save MSE testing scores\n",
        "\n",
        "\n",
        "# Convert each scores to NumPy arrays\n",
        "scores_mse_linear_overfit_train = np.array(scores_mse_linear_overfit_train)\n",
        "scores_mse_linear_overfit_test = np.array(scores_mse_linear_overfit_test)\n",
        "\n",
        "# Create figure for training and testing scores for different features\n",
        "n_experiments = scores_mse_linear_overfit_train.shape[0]\n",
        "\n",
        "labels = ['Training', 'Testing']\n",
        "colors = ['blue', 'red']\n",
        "\n",
        "# TODO: Create a subplot of a 1 by 1 figure to plot MSE for training and testing\n",
        "fig = None\n",
        "ax = None\n",
        "\n",
        "# TODO: Set x and y values\n",
        "x_values = []\n",
        "y_values = []\n",
        "\n",
        "# TODO: Plot MSE scores for training and testing sets\n",
        "# Set labels to ['Training', 'Testing'] and colors based on colors defined above\n",
        "# Set x limits to 0 to number of experiments + 1 and y limits between 0 and 100\n",
        "# Set x label to 'p-degree' and y label to 'MSE'\n",
        "\n",
        "\n",
        "# TODO: Create plot title of 'Overfitting Linear Regression with Various Degrees of Polynomial Expansions'\n"
      ],
      "metadata": {
        "id": "yB29ajtrK8sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 2: Demonstrate that ridge regression will underfit if we use large weight decay ($\\lambda$)"
      ],
      "metadata": {
        "id": "eLvDw6Fvb81A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Experiment 2: Underfitting Ridge Regression with Large Weight Decay')\n",
        "\n",
        "# TODO: Initialize a list containing 1 to 2^15 as the weight for weight decay\n",
        "weight_decays = []\n",
        "\n",
        "# Initialize empty lists to store scores for MSE\n",
        "scores_mse_ridge_underfit_train = []\n",
        "scores_mse_ridge_underfit_test = []\n",
        "\n",
        "for weight_decay in weight_decays:\n",
        "\n",
        "    # TODO: Initialize ridge regression model\n",
        "    model_ridge_underfit = None\n",
        "\n",
        "    # TODO: Train ridge regression model\n",
        "\n",
        "\n",
        "    print('Results for ridge regression model with weight decay of {}'.format(weight_decay))\n",
        "\n",
        "    # TODO: Test model on training set\n",
        "    predictions_train = None\n",
        "    score_mse_ridge_underfit_train = 0.0\n",
        "    print('Training set mean squared error: {:.4f}'.format(score_mse_ridge_underfit_train))\n",
        "\n",
        "    # TODO: Save MSE training scores\n",
        "\n",
        "\n",
        "    # TODO: Test model on testing set\n",
        "    predictions_test = None\n",
        "    score_mse_ridge_underfit_test = 0.0\n",
        "    print('Testing set mean squared error: {:.4f}'.format(score_mse_ridge_underfit_test))\n",
        "\n",
        "    # TODO: Save MSE testing scores\n",
        "\n",
        "\n",
        "# Convert each scores to NumPy arrays\n",
        "scores_mse_ridge_underfit_train = np.array(scores_mse_ridge_underfit_train)\n",
        "scores_mse_ridge_underfit_test = np.array(scores_mse_ridge_underfit_test)\n",
        "\n",
        "# Create figure for training, validation and testing scores for different features\n",
        "n_experiments = scores_mse_ridge_underfit_train.shape[0]\n",
        "\n",
        "labels = ['Training', 'Testing']\n",
        "colors = ['blue', 'red']\n",
        "\n",
        "# TODO: Create a subplot of a 1 by 1 figure to plot MSE for training and testing\n",
        "fig = None\n",
        "ax = None\n",
        "\n",
        "# TODO: Set x values (weight_decays in log base2 scale) and y values (MSE)\n",
        "x_values = []\n",
        "y_values = []\n",
        "\n",
        "# TODO: Plot MSE scores for training and testing sets\n",
        "# Set labels to ['Training', 'Testing'] and colors based on colors defined above\n",
        "# Set x limits to 0 to log of highest weight_decays + 1 and y limits between 0 and 100\n",
        "# Set x label to r'$\\lambda$ (log2 scale)' and y label to 'MSE'\n",
        "\n",
        "\n",
        "# TODO: Create plot title of r'Underfitting Ridge Regression with Various $\\lambda$'\n"
      ],
      "metadata": {
        "id": "Rl9TMJ-uYolZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 3: Demonstrate that ridge regression with various $\\lambda$ prevents overfitting when using polynomial expansion"
      ],
      "metadata": {
        "id": "ZJ3vdQCudjPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(r'Experiment 3: Ridge Regression with Weight Decay and Polynomial Expansion')\n",
        "\n",
        "# Set polynomial expansion\n",
        "degree = 6\n",
        "\n",
        "# TODO: Initialize a list containing 1 to 2^15 as the weight for weight decay\n",
        "weight_decays = []\n",
        "\n",
        "# TODO: Initialize polynomial expansion\n",
        "poly_transform = None\n",
        "\n",
        "# TODO: Compute the polynomial terms needed for the data\n",
        "\n",
        "\n",
        "# TODO: Transform the data by nonlinear mapping\n",
        "x_poly_train = None\n",
        "x_poly_test = None\n",
        "\n",
        "# Initialize empty lists to store scores for MSE\n",
        "scores_mse_ridge_poly_train = []\n",
        "scores_mse_ridge_poly_test = []\n",
        "\n",
        "for weight_decay in weight_decays:\n",
        "\n",
        "    # TODO: Initialize ridge regression model\n",
        "    model_ridge_poly= None\n",
        "\n",
        "    # TODO: Train ridge regression model\n",
        "\n",
        "\n",
        "    print('Results for ridge regression model with weight decay of {} for degree-{} polynomial expansion'.format(weight_decay, degree))\n",
        "\n",
        "    # TODO: Test model on training set\n",
        "    predictions_train = None\n",
        "    score_mse_ridge_poly_train = 0.0\n",
        "    print('Training set mean squared error: {:.4f}'.format(score_mse_ridge_poly_train))\n",
        "\n",
        "    # TODO: Save MSE training scores\n",
        "\n",
        "\n",
        "    # TODO: Test model on testing set\n",
        "    predictions_test = None\n",
        "    score_mse_ridge_poly_test = 0.0\n",
        "    print('Testing set mean squared error: {:.4f}'.format(score_mse_ridge_poly_test))\n",
        "\n",
        "    # TODO: Save MSE testing scores\n",
        "\n",
        "\n",
        "# Convert each scores to NumPy arrays\n",
        "scores_mse_ridge_poly_train = np.array(scores_mse_ridge_poly_train)\n",
        "scores_mse_ridge_poly_test = np.array(scores_mse_ridge_poly_test)\n",
        "\n",
        "# Create figure for training and testing scores for different features\n",
        "n_experiments = scores_mse_ridge_poly_train.shape[0]\n",
        "\n",
        "labels = ['Training', 'Testing']\n",
        "colors = ['blue', 'red']\n",
        "\n",
        "# TODO: Create the first subplot of a 1 by 1 figure to plot MSE for training and testing\n",
        "fig = None\n",
        "ax = None\n",
        "\n",
        "# TODO: Set x values (weight_decays in log base2 scale) and y values (MSE)\n",
        "x_values = []\n",
        "y_values = []\n",
        "\n",
        "# TODO: Plot MSE scores for training and testing sets\n",
        "# Set labels to ['Training', 'Testing'] and colors based on colors defined above\n",
        "# Set x limits to 0 to log of highest weight_decays + 1 and y limits between 0 and 100\n",
        "# Set x label to r'$\\lambda$ (log2 scale)' and y label to 'MSE'\n",
        "\n",
        "\n",
        "# TODO: Create plot title of r'Ridge Regression with various $\\lambda$ for Degree-{} Polynomial Expansion'.format(degree)\n"
      ],
      "metadata": {
        "id": "LVttbFIPc9kc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}