{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 10: Principal Component Analysis**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "\n",
        "4. Move the 10_exercise_pca.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/10_exercise_pca.ipynb\n",
        "```\n",
        "\n",
        "In this exercise, we will using PCA for dimensionality reduction as a mean of visualizing high dimensional data. Then we will test out the loss as we decrease the number of principal components. Finally, we will use it as a feature extractor and show that we can compress the data for the downstream classification task.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your reconstruction loss for training and testing sets and your classification scores for training and validation sets.\n",
        "\n",
        "```\n",
        "Report reconstruction loss and classification scores here.\n",
        "\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: Doe, Jane (Please write names in <Last Name, First Name> format)\n",
        "\n",
        "Collaboration details: Discussed ... implementation details with Jane Doe.\n",
        "```"
      ],
      "metadata": {
        "id": "_0fsGaVMMpwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages"
      ],
      "metadata": {
        "id": "wxeZsiCGC0J8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uumvcyiQ-k21"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load datasets"
      ],
      "metadata": {
        "id": "TTvfLetIQmk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "datasets = [\n",
        "    skdata.load_iris(),\n",
        "    skdata.load_wine()\n",
        "]\n",
        "\n",
        "dataset_names = [\n",
        "    'iris',\n",
        "    'wine'\n",
        "]\n",
        "\n",
        "# Set colors\n",
        "colors = [\n",
        "    'tab:blue',\n",
        "    'tab:green',\n",
        "    'tab:red'\n",
        "]"
      ],
      "metadata": {
        "id": "k9OD26CBQmHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform PCA on datasets and visualize"
      ],
      "metadata": {
        "id": "Ibd3k8JeQvHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip up all dataset options\n",
        "dataset_options = zip(\n",
        "    datasets,\n",
        "    dataset_names)\n",
        "\n",
        "for dataset, dataset_name in dataset_options:\n",
        "\n",
        "    X = dataset.data\n",
        "    y = dataset.target\n",
        "    names = dataset.target_names\n",
        "\n",
        "    n_dim = X.shape[-1]\n",
        "\n",
        "    # TODO: Instantiate PCA with 2 components (dimensions)\n",
        "    pca = None\n",
        "\n",
        "    # TODO: Fit PCA to data\n",
        "\n",
        "\n",
        "    # TODO: Use PCA to project (transform) all data points to lower dimensions\n",
        "    Z = None\n",
        "\n",
        "    # TODO: Create figure\n",
        "    fig = None\n",
        "\n",
        "    # TODO: Create super title 'Visualization of {} dataset projected from {} dimensions to a 2-dimensional subspace'\n",
        "\n",
        "    # TODO: Instantiate axis for subplot of a 1 x 1 figure\n",
        "    ax = None\n",
        "\n",
        "    # Iterate through each class and plot them into the figure as scatter plot with different colors\n",
        "    for label, color, name in zip(np.sort(np.unique(y)), colors, names):\n",
        "\n",
        "        # TODO: Select from projected points the ones belonging to current class\n",
        "        idx = None\n",
        "        Z_label = None\n",
        "\n",
        "        # TODO: Plot using scatter for selected points with associated color\n",
        "        # set the points label as name, set alpha to 0.5\n",
        "\n",
        "    # TODO: Turn on legend and set loc to best\n"
      ],
      "metadata": {
        "id": "yB29ajtrK8sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test generalization of the learned subspace through reconstruction loss"
      ],
      "metadata": {
        "id": "5b_ea2EZK7nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of dimensions of subspace\n",
        "dataset_components_list = [\n",
        "    # For iris dataset\n",
        "    range(1, 5),\n",
        "    # For wine dataset\n",
        "    range(1, 14)\n",
        "]\n",
        "\n",
        "# Zip up all dataset options\n",
        "dataset_options = zip(\n",
        "    datasets,\n",
        "    dataset_components_list,\n",
        "    dataset_names)\n",
        "\n",
        "for dataset, dataset_components, dataset_name in dataset_options:\n",
        "\n",
        "    X = dataset.data\n",
        "    y = dataset.target\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(X.shape[0])\n",
        "\n",
        "    # Choose the first 80% as training set and the next 20% as validation\n",
        "    train_split_idx = int(0.80 * X.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:]\n",
        "\n",
        "    # Select the examples from X and y to construct our training, validation, testing sets\n",
        "    X_train, y_train = X[train_indices, :], y[train_indices]\n",
        "    X_val, y_val = X[val_indices, :], y[val_indices]\n",
        "\n",
        "    # Define empty lists to hold scores for training and validation\n",
        "    mse_scores_train = []\n",
        "    mse_scores_val = []\n",
        "\n",
        "    for components in dataset_components:\n",
        "\n",
        "        print('***** Fitting PCA with {} components on {} dataset *****'.format(components, dataset_name))\n",
        "\n",
        "        # TODO: Instantiate PCA with specified components (dimensions)\n",
        "        pca = None\n",
        "\n",
        "        # TODO: Fit PCA to training data\n",
        "\n",
        "\n",
        "        # TODO: Project the training data, reconstruct them, and measure loss\n",
        "        Z_train = None\n",
        "        X_hat_train = None\n",
        "\n",
        "        mse_score_train = None\n",
        "        print('Training set mean squared error: {:.4f}'.format(mse_score_train))\n",
        "\n",
        "        # TODO: Project the validation data, reconstruct them, and measure loss\n",
        "        Z_val = None\n",
        "        X_hat_val = None\n",
        "\n",
        "        mse_score_val =  None\n",
        "        print('Validation set mean squared error: {:.4f}'.format(mse_score_val))\n",
        "\n",
        "        # TODO: Append training and validation scores to lists of training and validation scores\n",
        "\n",
        "\n",
        "    # TODO: Create figure with figsize=(5, 5)\n",
        "    fig = None\n",
        "\n",
        "    # TODO: Instantiate axis for subplot of a 1 x 1 figure\n",
        "    ax = None\n",
        "\n",
        "    # TODO: Plot the the number of components on the x-axis and training mse scores on the y-axis with color='blue', label='Training'\n",
        "\n",
        "\n",
        "    # TODO: Plot the the number of components on the x-axis and validation mse scores on the y-axis with color='red', label='Validation'\n",
        "\n",
        "\n",
        "    # TODO: Set title to 'Reconstrction Error on {} dataset'\n",
        "\n",
        "\n",
        "    # TODO: Set xlabel to '# of components'\n",
        "\n",
        "\n",
        "    # TODO: Set ylabel to 'MSE'\n",
        "\n",
        "\n",
        "    # TODO: Set legend with loc='upper right'\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "    print('')\n",
        "    print('')"
      ],
      "metadata": {
        "id": "0gcJAhjaGrzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use PCA as a feature extractor for logistic regression"
      ],
      "metadata": {
        "id": "pnZksteqOjXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of dimensions of subspace\n",
        "dataset_components_list = [\n",
        "    # For iris dataset\n",
        "    range(1, 5),\n",
        "    # For wine dataset\n",
        "    range(1, 14)\n",
        "]\n",
        "\n",
        "# Zip up all dataset options\n",
        "dataset_options = zip(\n",
        "    datasets,\n",
        "    dataset_components_list,\n",
        "    dataset_names)\n",
        "\n",
        "for dataset, dataset_components, dataset_name in dataset_options:\n",
        "\n",
        "    X = dataset.data\n",
        "    y = dataset.target\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(X.shape[0])\n",
        "\n",
        "    # Choose the first 80% as training set and the next 20% as validation\n",
        "    train_split_idx = int(0.80 * X.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:]\n",
        "\n",
        "    # Select the examples from X and y to construct our training, validation, testing sets\n",
        "    X_train, y_train = X[train_indices, :], y[train_indices]\n",
        "    X_val, y_val = X[val_indices, :], y[val_indices]\n",
        "\n",
        "    # Define empty lists to hold scores for training and validation\n",
        "    scores_train = []\n",
        "    scores_val = []\n",
        "\n",
        "    for components in dataset_components:\n",
        "\n",
        "        print('***** Results of Logistic Regression using PCA with {} components on {} dataset *****'.format(components, dataset_name))\n",
        "\n",
        "        # TODO: Instantiate PCA with specified components (dimensions)\n",
        "        pca = None\n",
        "\n",
        "        # TODO: Fit PCA to training data\n",
        "\n",
        "\n",
        "        # TODO: Project the training data\n",
        "        Z_train = None\n",
        "\n",
        "        # TODO: Instantiate LogisticRegression with tol=1e-4\n",
        "        logistic = None\n",
        "\n",
        "        # TODO: Train model using projected training data\n",
        "\n",
        "\n",
        "        # TODO: Score model using mean accuracy on training set\n",
        "        predictions_train = None\n",
        "        score_train = None\n",
        "        print('Training set mean accuracy: {:.4f}'.format(score_train))\n",
        "\n",
        "        # TODO: Project the validation data and test model on it\n",
        "        Z_val = None\n",
        "\n",
        "        # TODO: Score model using mean accuracy validation set\n",
        "        predictions_val = None\n",
        "        score_val = None\n",
        "        print('Validation set mean accuracy: {:.4f}'.format(score_val))\n",
        "\n",
        "        # TODO: Append training and validation scores to lists of training and validation scores\n",
        "\n",
        "\n",
        "    # TODO: Create figure with figsize=(5, 5)\n",
        "    fig = None\n",
        "\n",
        "    # TODO: Instantiate axis for subplot of a 1 x 1 figure\n",
        "    ax = None\n",
        "\n",
        "    # TODO: Plot the the number of components on the x-axis and training scores on the y-axis with color='blue', label='Training'\n",
        "\n",
        "    # TODO: Plot the the number of components on the x-axis and validation scores on the y-axis with color='red', label='Validation'\n",
        "\n",
        "    # TODO: Set title to 'Logistic Regression using PCA on {} dataset'\n",
        "\n",
        "    # TODO: Set xlabel to '# of components'\n",
        "\n",
        "    # TODO: Set ylabel to 'Scores'\n",
        "\n",
        "    # TODO: Set legend with loc='upper right'\n",
        "\n",
        "    plt.show()\n",
        "    print('')"
      ],
      "metadata": {
        "id": "5buTXn2ROorV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}