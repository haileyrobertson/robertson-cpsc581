{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0fsGaVMMpwG"
      },
      "source": [
        "**Exercise 4: Gradient Descent for Linear Regression**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "\n",
        "4. Move the 04_exercise_gradient_descent.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/04_exercise_gradient_descent.ipynb\n",
        "```\n",
        "\n",
        "In this exercise, we will optimize a linear function for the regression task using the gradient descent for mean squared and half mean squared losses. We will test them on several datasets.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training, validation, and testing scores.\n",
        "\n",
        "```\n",
        "Report validation and testing scores here.\n",
        "\n",
        "For full credit, your mean squared error scores for models optimized using mean_squared and half_mean_squared losses on Diabetes dataset should be no more than 15% worse the mean squared error scores achieved by sci-kit learn's linear regression model across training, validation and testing splits. Your mean squared error scores on California housing price dataset should be no more than 20% worse.\n",
        "\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: Doe, Jane (Please write names in <Last Name, First Name> format)\n",
        "\n",
        "Collaboration details: Discussed ... implementation details with Jane Doe.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxeZsiCGC0J8"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uumvcyiQ-k21"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn.linear_model import LinearRegression as LinearRegressionSciKit\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ljMielQC7Lg"
      },
      "source": [
        "Implementation of our Gradient Descent optimizer for mean squared and half mean squared loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6wlWiioqDBkG"
      },
      "outputs": [],
      "source": [
        "class GradientDescentOptimizer(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __compute_gradients(self, w, x, y, loss_func):\n",
        "        '''\n",
        "        Returns the gradient of mean squared or half mean squared loss\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss type either mean_squared', or 'half_mean_squared'\n",
        "        Returns:\n",
        "            numpy[float32] : d x 1 gradients\n",
        "        '''\n",
        "\n",
        "        # DONE: Implements the __compute_gradients function\n",
        "        if loss_func == 'mean_squared':\n",
        "            # DONE: Implements gradients for mean squared loss\n",
        "\n",
        "            '''\n",
        "            Using for-loop\n",
        "\n",
        "            gradients = np.zeros(x.shape)\n",
        "\n",
        "            for n in range((x.shape[1])):\n",
        "                x_n = x[:, n]\n",
        "                gradients[:, n] = (np.matmul(w.T, x_n) - y[n]) * x_n\n",
        "            '''\n",
        "\n",
        "            # Using matrix multiplication\n",
        "            gradients = (np.matmul(w.T, x) - y) * x\n",
        "\n",
        "            # Note: Set keepdims=True to keep the dimension of 1 (otherwise it will get squashed by mean operation)\n",
        "            return 2.0 * np.mean(gradients, axis=1, keepdims=True)\n",
        "        elif loss_func == 'half_mean_squared':\n",
        "            # DONE: Implements gradients for half mean squared loss\n",
        "\n",
        "            gradients = (np.matmul(w.T, x) - y) * x\n",
        "\n",
        "            return np.mean(gradients, axis=1, keepdims=True)\n",
        "        else:\n",
        "            raise ValueError('Unsupported loss function: {}'.format(loss_func))\n",
        "\n",
        "    def update(self, w, x, y, alpha, loss_func):\n",
        "        '''\n",
        "        Updates the weight vector based on mean squared or half mean squared loss\n",
        "\n",
        "        Arg(s):\n",
        "            w : numpy[float32]\n",
        "                d x 1 weight vector\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            alpha : float\n",
        "                learning rate\n",
        "            loss_func : str\n",
        "                loss type either 'mean_squared', or 'half_mean_squared'\n",
        "        Returns:\n",
        "            numpy[float32] : d x 1 weights\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement the optimizer update function\n",
        "\n",
        "        return w - alpha * self.__compute_gradients(w, x, y, loss_func)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xOsR-kJIlD3"
      },
      "source": [
        "Implementation of Linear Regression with Gradient Descent optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BqpoUg5fIlgZ"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionGradientDescent(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Define private variables\n",
        "        self.__weights = None\n",
        "        self.__optimizer = GradientDescentOptimizer()\n",
        "\n",
        "    def fit(self, x, y, T, alpha, loss_func='mean_squared'):\n",
        "        '''\n",
        "        Fits the model to x and y by updating the weight vector\n",
        "        using gradient descent\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            T : int\n",
        "                number of iterations to train\n",
        "            alpha : float\n",
        "                learning rate\n",
        "            loss_func : str\n",
        "                loss function to use\n",
        "        '''\n",
        "\n",
        "        # DONE: Implement the fit function\n",
        "        self.__weights = np.zeros([x.shape[0], 1])\n",
        "\n",
        "        for t in range(1, T + 1):\n",
        "\n",
        "            # DONE: Compute loss function\n",
        "            loss = self.__compute_loss(\n",
        "                x=x,\n",
        "                y=y,\n",
        "                loss_func=loss_func)\n",
        "\n",
        "            if (t % 10000) == 0:\n",
        "                print('Step={}  Loss={:.4f}'.format(t, loss))\n",
        "\n",
        "            # DONE: Update weights\n",
        "            self.__weights = self.__optimizer.update(\n",
        "                w=self.__weights,\n",
        "                x=x,\n",
        "                y=y,\n",
        "                alpha=alpha,\n",
        "                loss_func=loss_func)\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predicts the label for each feature vector x\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "        Returns:\n",
        "            numpy[float32] : 1 x N vector\n",
        "        '''\n",
        "\n",
        "        # DONE: Implements the predict function\n",
        "\n",
        "        return np.matmul(self.__weights.T, x)\n",
        "\n",
        "    def __compute_loss(self, x, y, loss_func):\n",
        "        '''\n",
        "        Returns the gradient of the mean squared or half mean squared loss\n",
        "\n",
        "        Arg(s):\n",
        "            x : numpy[float32]\n",
        "                d x N feature vector\n",
        "            y : numpy[float32]\n",
        "                1 x N groundtruth vector\n",
        "            loss_func : str\n",
        "                loss type either 'mean_squared', or 'half_mean_squared'\n",
        "        Returns:\n",
        "            float : loss\n",
        "        '''\n",
        "\n",
        "        # DONE: Implements the __compute_loss function\n",
        "        predictions = self.predict(x)\n",
        "\n",
        "        if loss_func == 'mean_squared':\n",
        "            # DONE: Implements loss for mean squared loss\n",
        "            loss = np.mean((predictions - y) ** 2)\n",
        "        elif loss_func == 'half_mean_squared':\n",
        "            # DONE: Implements loss for half mean squared loss\n",
        "            loss = 0.50 * np.mean((predictions - y) ** 2)\n",
        "        else:\n",
        "            raise ValueError('Unsupported loss function: {}'.format(loss_func))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcb2TArNKvf1"
      },
      "source": [
        "Implementing training and validation loop for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB29ajtrK8sQ",
        "outputId": "0581fdd4-72a8-4a02-b3a2-b7e7843bc9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Results of scikit-learn linear regression model on Diabetes dataset *****\n",
            "Training set mean squared error: 25774.0600\n",
            "Training set r-squared scores: -3.3189\n",
            "Validation set mean squared error: 30328.6709\n",
            "Validation set r-squared scores: -3.8961\n",
            "Testing set mean squared error: 25994.0954\n",
            "Testing set r-squared scores: -4.2553\n",
            "***** Results of our linear regression model trained with mean_squared loss, alpha=0.0001 and T=10000 on Diabetes dataset *****\n",
            "Step=10000  Loss=29332.5459\n",
            "Training set mean squared error: 29332.5371\n",
            "Training set r-squared scores: -3.9152\n",
            "Validation set mean squared error: 32460.1234\n",
            "Validation set r-squared scores: -4.2401\n",
            "Testing set mean squared error: 22931.2206\n",
            "Testing set r-squared scores: -3.6361\n",
            "***** Results of our linear regression model trained with half_mean_squared loss, alpha=0.0002 and T=10000 on Diabetes dataset *****\n",
            "Step=10000  Loss=14666.2730\n",
            "Training set mean squared error: 29332.5371\n",
            "Training set r-squared scores: -3.9152\n",
            "Validation set mean squared error: 32460.1234\n",
            "Validation set r-squared scores: -4.2401\n",
            "Testing set mean squared error: 22931.2206\n",
            "Testing set r-squared scores: -3.6361\n",
            "***** Results of scikit-learn linear regression model on California housing prices dataset *****\n",
            "Training set mean squared error: 0.5951\n",
            "Training set r-squared scores: 0.5504\n",
            "Validation set mean squared error: 0.6964\n",
            "Validation set r-squared scores: 0.4980\n",
            "Testing set mean squared error: 0.5962\n",
            "Testing set r-squared scores: 0.5549\n",
            "***** Results of our linear regression model trained with mean_squared loss, alpha=1e-07 and T=5000000 on California housing prices dataset *****\n",
            "Step=10000  Loss=1.3013\n",
            "Step=20000  Loss=1.2894\n",
            "Step=30000  Loss=1.2782\n",
            "Step=40000  Loss=1.2675\n",
            "Step=50000  Loss=1.2571\n",
            "Step=60000  Loss=1.2469\n",
            "Step=70000  Loss=1.2370\n",
            "Step=80000  Loss=1.2273\n",
            "Step=90000  Loss=1.2178\n",
            "Step=100000  Loss=1.2084\n",
            "Step=110000  Loss=1.1992\n",
            "Step=120000  Loss=1.1902\n",
            "Step=130000  Loss=1.1814\n",
            "Step=140000  Loss=1.1727\n",
            "Step=150000  Loss=1.1642\n",
            "Step=160000  Loss=1.1558\n",
            "Step=170000  Loss=1.1475\n",
            "Step=180000  Loss=1.1394\n",
            "Step=190000  Loss=1.1315\n",
            "Step=200000  Loss=1.1237\n",
            "Step=210000  Loss=1.1160\n",
            "Step=220000  Loss=1.1084\n",
            "Step=230000  Loss=1.1010\n",
            "Step=240000  Loss=1.0937\n",
            "Step=250000  Loss=1.0865\n",
            "Step=260000  Loss=1.0795\n",
            "Step=270000  Loss=1.0726\n",
            "Step=280000  Loss=1.0657\n",
            "Step=290000  Loss=1.0591\n",
            "Step=300000  Loss=1.0525\n",
            "Step=310000  Loss=1.0460\n",
            "Step=320000  Loss=1.0396\n",
            "Step=330000  Loss=1.0334\n",
            "Step=340000  Loss=1.0272\n",
            "Step=350000  Loss=1.0212\n",
            "Step=360000  Loss=1.0152\n",
            "Step=370000  Loss=1.0094\n",
            "Step=380000  Loss=1.0037\n",
            "Step=390000  Loss=0.9980\n",
            "Step=400000  Loss=0.9924\n",
            "Step=410000  Loss=0.9870\n",
            "Step=420000  Loss=0.9816\n",
            "Step=430000  Loss=0.9763\n",
            "Step=440000  Loss=0.9711\n",
            "Step=450000  Loss=0.9660\n",
            "Step=460000  Loss=0.9610\n",
            "Step=470000  Loss=0.9560\n",
            "Step=480000  Loss=0.9511\n",
            "Step=490000  Loss=0.9464\n",
            "Step=500000  Loss=0.9416\n",
            "Step=510000  Loss=0.9370\n",
            "Step=520000  Loss=0.9325\n",
            "Step=530000  Loss=0.9280\n",
            "Step=540000  Loss=0.9236\n",
            "Step=550000  Loss=0.9192\n",
            "Step=560000  Loss=0.9149\n",
            "Step=570000  Loss=0.9107\n",
            "Step=580000  Loss=0.9066\n",
            "Step=590000  Loss=0.9025\n",
            "Step=600000  Loss=0.8985\n",
            "Step=610000  Loss=0.8946\n",
            "Step=620000  Loss=0.8907\n",
            "Step=630000  Loss=0.8869\n",
            "Step=640000  Loss=0.8831\n",
            "Step=650000  Loss=0.8794\n",
            "Step=660000  Loss=0.8758\n",
            "Step=670000  Loss=0.8722\n",
            "Step=680000  Loss=0.8687\n",
            "Step=690000  Loss=0.8652\n",
            "Step=700000  Loss=0.8618\n",
            "Step=710000  Loss=0.8584\n",
            "Step=720000  Loss=0.8551\n",
            "Step=730000  Loss=0.8518\n",
            "Step=740000  Loss=0.8486\n",
            "Step=750000  Loss=0.8454\n",
            "Step=760000  Loss=0.8423\n",
            "Step=770000  Loss=0.8393\n",
            "Step=780000  Loss=0.8363\n",
            "Step=790000  Loss=0.8333\n",
            "Step=800000  Loss=0.8304\n",
            "Step=810000  Loss=0.8275\n",
            "Step=820000  Loss=0.8246\n",
            "Step=830000  Loss=0.8218\n",
            "Step=840000  Loss=0.8191\n",
            "Step=850000  Loss=0.8164\n",
            "Step=860000  Loss=0.8137\n",
            "Step=870000  Loss=0.8111\n",
            "Step=880000  Loss=0.8085\n",
            "Step=890000  Loss=0.8059\n",
            "Step=900000  Loss=0.8034\n",
            "Step=910000  Loss=0.8010\n",
            "Step=920000  Loss=0.7985\n",
            "Step=930000  Loss=0.7961\n",
            "Step=940000  Loss=0.7937\n",
            "Step=950000  Loss=0.7914\n",
            "Step=960000  Loss=0.7891\n",
            "Step=970000  Loss=0.7869\n",
            "Step=980000  Loss=0.7846\n",
            "Step=990000  Loss=0.7824\n",
            "Step=1000000  Loss=0.7803\n",
            "Step=1010000  Loss=0.7781\n",
            "Step=1020000  Loss=0.7760\n",
            "Step=1030000  Loss=0.7740\n",
            "Step=1040000  Loss=0.7719\n",
            "Step=1050000  Loss=0.7699\n",
            "Step=1060000  Loss=0.7679\n",
            "Step=1070000  Loss=0.7660\n",
            "Step=1080000  Loss=0.7641\n",
            "Step=1090000  Loss=0.7622\n",
            "Step=1100000  Loss=0.7603\n",
            "Step=1110000  Loss=0.7585\n",
            "Step=1120000  Loss=0.7567\n",
            "Step=1130000  Loss=0.7549\n",
            "Step=1140000  Loss=0.7531\n",
            "Step=1150000  Loss=0.7514\n",
            "Step=1160000  Loss=0.7497\n",
            "Step=1170000  Loss=0.7480\n",
            "Step=1180000  Loss=0.7463\n",
            "Step=1190000  Loss=0.7447\n",
            "Step=1200000  Loss=0.7431\n",
            "Step=1210000  Loss=0.7415\n",
            "Step=1220000  Loss=0.7399\n",
            "Step=1230000  Loss=0.7383\n",
            "Step=1240000  Loss=0.7368\n",
            "Step=1250000  Loss=0.7353\n",
            "Step=1260000  Loss=0.7338\n",
            "Step=1270000  Loss=0.7324\n",
            "Step=1280000  Loss=0.7309\n",
            "Step=1290000  Loss=0.7295\n",
            "Step=1300000  Loss=0.7281\n",
            "Step=1310000  Loss=0.7267\n",
            "Step=1320000  Loss=0.7254\n",
            "Step=1330000  Loss=0.7240\n",
            "Step=1340000  Loss=0.7227\n",
            "Step=1350000  Loss=0.7214\n",
            "Step=1360000  Loss=0.7201\n",
            "Step=1370000  Loss=0.7188\n",
            "Step=1380000  Loss=0.7176\n",
            "Step=1390000  Loss=0.7163\n",
            "Step=1400000  Loss=0.7151\n",
            "Step=1410000  Loss=0.7139\n",
            "Step=1420000  Loss=0.7127\n",
            "Step=1430000  Loss=0.7116\n",
            "Step=1440000  Loss=0.7104\n",
            "Step=1450000  Loss=0.7093\n",
            "Step=1460000  Loss=0.7082\n",
            "Step=1470000  Loss=0.7071\n",
            "Step=1480000  Loss=0.7060\n",
            "Step=1490000  Loss=0.7049\n",
            "Step=1500000  Loss=0.7038\n",
            "Step=1510000  Loss=0.7028\n",
            "Step=1520000  Loss=0.7017\n",
            "Step=1530000  Loss=0.7007\n",
            "Step=1540000  Loss=0.6997\n",
            "Step=1550000  Loss=0.6987\n",
            "Step=1560000  Loss=0.6978\n",
            "Step=1570000  Loss=0.6968\n",
            "Step=1580000  Loss=0.6958\n",
            "Step=1590000  Loss=0.6949\n",
            "Step=1600000  Loss=0.6940\n",
            "Step=1610000  Loss=0.6931\n",
            "Step=1620000  Loss=0.6922\n",
            "Step=1630000  Loss=0.6913\n",
            "Step=1640000  Loss=0.6904\n",
            "Step=1650000  Loss=0.6895\n",
            "Step=1660000  Loss=0.6887\n",
            "Step=1670000  Loss=0.6878\n",
            "Step=1680000  Loss=0.6870\n",
            "Step=1690000  Loss=0.6862\n",
            "Step=1700000  Loss=0.6854\n",
            "Step=1710000  Loss=0.6846\n",
            "Step=1720000  Loss=0.6838\n",
            "Step=1730000  Loss=0.6830\n",
            "Step=1740000  Loss=0.6822\n",
            "Step=1750000  Loss=0.6815\n",
            "Step=1760000  Loss=0.6807\n",
            "Step=1770000  Loss=0.6800\n",
            "Step=1780000  Loss=0.6793\n",
            "Step=1790000  Loss=0.6785\n",
            "Step=1800000  Loss=0.6778\n",
            "Step=1810000  Loss=0.6771\n",
            "Step=1820000  Loss=0.6764\n",
            "Step=1830000  Loss=0.6758\n",
            "Step=1840000  Loss=0.6751\n",
            "Step=1850000  Loss=0.6744\n",
            "Step=1860000  Loss=0.6738\n",
            "Step=1870000  Loss=0.6731\n",
            "Step=1880000  Loss=0.6725\n",
            "Step=1890000  Loss=0.6719\n",
            "Step=1900000  Loss=0.6712\n",
            "Step=1910000  Loss=0.6706\n",
            "Step=1920000  Loss=0.6700\n",
            "Step=1930000  Loss=0.6694\n",
            "Step=1940000  Loss=0.6688\n",
            "Step=1950000  Loss=0.6682\n",
            "Step=1960000  Loss=0.6677\n",
            "Step=1970000  Loss=0.6671\n",
            "Step=1980000  Loss=0.6665\n",
            "Step=1990000  Loss=0.6660\n",
            "Step=2000000  Loss=0.6654\n",
            "Step=2010000  Loss=0.6649\n",
            "Step=2020000  Loss=0.6644\n",
            "Step=2030000  Loss=0.6638\n",
            "Step=2040000  Loss=0.6633\n",
            "Step=2050000  Loss=0.6628\n",
            "Step=2060000  Loss=0.6623\n",
            "Step=2070000  Loss=0.6618\n",
            "Step=2080000  Loss=0.6613\n",
            "Step=2090000  Loss=0.6608\n",
            "Step=2100000  Loss=0.6603\n",
            "Step=2110000  Loss=0.6599\n",
            "Step=2120000  Loss=0.6594\n",
            "Step=2130000  Loss=0.6589\n",
            "Step=2140000  Loss=0.6585\n",
            "Step=2150000  Loss=0.6580\n",
            "Step=2160000  Loss=0.6576\n",
            "Step=2170000  Loss=0.6571\n",
            "Step=2180000  Loss=0.6567\n",
            "Step=2190000  Loss=0.6563\n",
            "Step=2200000  Loss=0.6559\n",
            "Step=2210000  Loss=0.6554\n",
            "Step=2220000  Loss=0.6550\n",
            "Step=2230000  Loss=0.6546\n",
            "Step=2240000  Loss=0.6542\n",
            "Step=2250000  Loss=0.6538\n",
            "Step=2260000  Loss=0.6534\n",
            "Step=2270000  Loss=0.6530\n",
            "Step=2280000  Loss=0.6527\n",
            "Step=2290000  Loss=0.6523\n",
            "Step=2300000  Loss=0.6519\n",
            "Step=2310000  Loss=0.6515\n",
            "Step=2320000  Loss=0.6512\n",
            "Step=2330000  Loss=0.6508\n",
            "Step=2340000  Loss=0.6505\n",
            "Step=2350000  Loss=0.6501\n",
            "Step=2360000  Loss=0.6498\n",
            "Step=2370000  Loss=0.6494\n",
            "Step=2380000  Loss=0.6491\n",
            "Step=2390000  Loss=0.6488\n",
            "Step=2400000  Loss=0.6484\n",
            "Step=2410000  Loss=0.6481\n",
            "Step=2420000  Loss=0.6478\n",
            "Step=2430000  Loss=0.6475\n",
            "Step=2440000  Loss=0.6471\n",
            "Step=2450000  Loss=0.6468\n",
            "Step=2460000  Loss=0.6465\n",
            "Step=2470000  Loss=0.6462\n",
            "Step=2480000  Loss=0.6459\n",
            "Step=2490000  Loss=0.6456\n",
            "Step=2500000  Loss=0.6453\n",
            "Step=2510000  Loss=0.6451\n",
            "Step=2520000  Loss=0.6448\n",
            "Step=2530000  Loss=0.6445\n",
            "Step=2540000  Loss=0.6442\n",
            "Step=2550000  Loss=0.6439\n",
            "Step=2560000  Loss=0.6437\n",
            "Step=2570000  Loss=0.6434\n",
            "Step=2580000  Loss=0.6431\n",
            "Step=2590000  Loss=0.6429\n",
            "Step=2600000  Loss=0.6426\n",
            "Step=2610000  Loss=0.6424\n",
            "Step=2620000  Loss=0.6421\n",
            "Step=2630000  Loss=0.6419\n",
            "Step=2640000  Loss=0.6416\n",
            "Step=2650000  Loss=0.6414\n",
            "Step=2660000  Loss=0.6411\n",
            "Step=2670000  Loss=0.6409\n",
            "Step=2680000  Loss=0.6407\n",
            "Step=2690000  Loss=0.6404\n",
            "Step=2700000  Loss=0.6402\n",
            "Step=2710000  Loss=0.6400\n",
            "Step=2720000  Loss=0.6398\n",
            "Step=2730000  Loss=0.6396\n",
            "Step=2740000  Loss=0.6393\n",
            "Step=2750000  Loss=0.6391\n",
            "Step=2760000  Loss=0.6389\n",
            "Step=2770000  Loss=0.6387\n",
            "Step=2780000  Loss=0.6385\n",
            "Step=2790000  Loss=0.6383\n",
            "Step=2800000  Loss=0.6381\n",
            "Step=2810000  Loss=0.6379\n",
            "Step=2820000  Loss=0.6377\n",
            "Step=2830000  Loss=0.6375\n",
            "Step=2840000  Loss=0.6373\n",
            "Step=2850000  Loss=0.6371\n",
            "Step=2860000  Loss=0.6369\n",
            "Step=2870000  Loss=0.6367\n",
            "Step=2880000  Loss=0.6366\n",
            "Step=2890000  Loss=0.6364\n",
            "Step=2900000  Loss=0.6362\n",
            "Step=2910000  Loss=0.6360\n",
            "Step=2920000  Loss=0.6358\n",
            "Step=2930000  Loss=0.6357\n",
            "Step=2940000  Loss=0.6355\n",
            "Step=2950000  Loss=0.6353\n",
            "Step=2960000  Loss=0.6352\n",
            "Step=2970000  Loss=0.6350\n",
            "Step=2980000  Loss=0.6348\n",
            "Step=2990000  Loss=0.6347\n",
            "Step=3000000  Loss=0.6345\n",
            "Step=3010000  Loss=0.6344\n",
            "Step=3020000  Loss=0.6342\n",
            "Step=3030000  Loss=0.6341\n",
            "Step=3040000  Loss=0.6339\n",
            "Step=3050000  Loss=0.6338\n",
            "Step=3060000  Loss=0.6336\n",
            "Step=3070000  Loss=0.6335\n",
            "Step=3080000  Loss=0.6333\n",
            "Step=3090000  Loss=0.6332\n",
            "Step=3100000  Loss=0.6330\n",
            "Step=3110000  Loss=0.6329\n",
            "Step=3120000  Loss=0.6328\n",
            "Step=3130000  Loss=0.6326\n",
            "Step=3140000  Loss=0.6325\n",
            "Step=3150000  Loss=0.6323\n",
            "Step=3160000  Loss=0.6322\n",
            "Step=3170000  Loss=0.6321\n",
            "Step=3180000  Loss=0.6320\n",
            "Step=3190000  Loss=0.6318\n",
            "Step=3200000  Loss=0.6317\n",
            "Step=3210000  Loss=0.6316\n",
            "Step=3220000  Loss=0.6315\n",
            "Step=3230000  Loss=0.6313\n",
            "Step=3240000  Loss=0.6312\n",
            "Step=3250000  Loss=0.6311\n",
            "Step=3260000  Loss=0.6310\n",
            "Step=3270000  Loss=0.6309\n",
            "Step=3280000  Loss=0.6308\n",
            "Step=3290000  Loss=0.6306\n",
            "Step=3300000  Loss=0.6305\n",
            "Step=3310000  Loss=0.6304\n",
            "Step=3320000  Loss=0.6303\n",
            "Step=3330000  Loss=0.6302\n",
            "Step=3340000  Loss=0.6301\n",
            "Step=3350000  Loss=0.6300\n",
            "Step=3360000  Loss=0.6299\n",
            "Step=3370000  Loss=0.6298\n",
            "Step=3380000  Loss=0.6297\n",
            "Step=3390000  Loss=0.6296\n",
            "Step=3400000  Loss=0.6295\n",
            "Step=3410000  Loss=0.6294\n",
            "Step=3420000  Loss=0.6293\n",
            "Step=3430000  Loss=0.6292\n",
            "Step=3440000  Loss=0.6291\n",
            "Step=3450000  Loss=0.6290\n",
            "Step=3460000  Loss=0.6289\n",
            "Step=3470000  Loss=0.6288\n",
            "Step=3480000  Loss=0.6287\n",
            "Step=3490000  Loss=0.6286\n",
            "Step=3500000  Loss=0.6285\n",
            "Step=3510000  Loss=0.6285\n",
            "Step=3520000  Loss=0.6284\n",
            "Step=3530000  Loss=0.6283\n",
            "Step=3540000  Loss=0.6282\n",
            "Step=3550000  Loss=0.6281\n",
            "Step=3560000  Loss=0.6280\n",
            "Step=3570000  Loss=0.6280\n",
            "Step=3580000  Loss=0.6279\n",
            "Step=3590000  Loss=0.6278\n",
            "Step=3600000  Loss=0.6277\n",
            "Step=3610000  Loss=0.6276\n",
            "Step=3620000  Loss=0.6276\n",
            "Step=3630000  Loss=0.6275\n",
            "Step=3640000  Loss=0.6274\n",
            "Step=3650000  Loss=0.6273\n",
            "Step=3660000  Loss=0.6273\n",
            "Step=3670000  Loss=0.6272\n",
            "Step=3680000  Loss=0.6271\n",
            "Step=3690000  Loss=0.6270\n",
            "Step=3700000  Loss=0.6270\n",
            "Step=3710000  Loss=0.6269\n",
            "Step=3720000  Loss=0.6268\n",
            "Step=3730000  Loss=0.6268\n",
            "Step=3740000  Loss=0.6267\n",
            "Step=3750000  Loss=0.6266\n",
            "Step=3760000  Loss=0.6266\n",
            "Step=3770000  Loss=0.6265\n",
            "Step=3780000  Loss=0.6264\n",
            "Step=3790000  Loss=0.6264\n",
            "Step=3800000  Loss=0.6263\n",
            "Step=3810000  Loss=0.6262\n",
            "Step=3820000  Loss=0.6262\n",
            "Step=3830000  Loss=0.6261\n",
            "Step=3840000  Loss=0.6261\n",
            "Step=3850000  Loss=0.6260\n",
            "Step=3860000  Loss=0.6259\n",
            "Step=3870000  Loss=0.6259\n",
            "Step=3880000  Loss=0.6258\n",
            "Step=3890000  Loss=0.6258\n",
            "Step=3900000  Loss=0.6257\n",
            "Step=3910000  Loss=0.6256\n",
            "Step=3920000  Loss=0.6256\n",
            "Step=3930000  Loss=0.6255\n",
            "Step=3940000  Loss=0.6255\n",
            "Step=3950000  Loss=0.6254\n",
            "Step=3960000  Loss=0.6254\n",
            "Step=3970000  Loss=0.6253\n",
            "Step=3980000  Loss=0.6253\n",
            "Step=3990000  Loss=0.6252\n",
            "Step=4000000  Loss=0.6252\n",
            "Step=4010000  Loss=0.6251\n",
            "Step=4020000  Loss=0.6251\n",
            "Step=4030000  Loss=0.6250\n",
            "Step=4040000  Loss=0.6250\n",
            "Step=4050000  Loss=0.6249\n",
            "Step=4060000  Loss=0.6249\n",
            "Step=4070000  Loss=0.6248\n",
            "Step=4080000  Loss=0.6248\n",
            "Step=4090000  Loss=0.6247\n",
            "Step=4100000  Loss=0.6247\n",
            "Step=4110000  Loss=0.6246\n",
            "Step=4120000  Loss=0.6246\n",
            "Step=4130000  Loss=0.6246\n",
            "Step=4140000  Loss=0.6245\n",
            "Step=4150000  Loss=0.6245\n",
            "Step=4160000  Loss=0.6244\n",
            "Step=4170000  Loss=0.6244\n",
            "Step=4180000  Loss=0.6243\n",
            "Step=4190000  Loss=0.6243\n",
            "Step=4200000  Loss=0.6243\n",
            "Step=4210000  Loss=0.6242\n",
            "Step=4220000  Loss=0.6242\n",
            "Step=4230000  Loss=0.6241\n",
            "Step=4240000  Loss=0.6241\n",
            "Step=4250000  Loss=0.6241\n",
            "Step=4260000  Loss=0.6240\n",
            "Step=4270000  Loss=0.6240\n",
            "Step=4280000  Loss=0.6239\n",
            "Step=4290000  Loss=0.6239\n",
            "Step=4300000  Loss=0.6239\n",
            "Step=4310000  Loss=0.6238\n",
            "Step=4320000  Loss=0.6238\n",
            "Step=4330000  Loss=0.6238\n",
            "Step=4340000  Loss=0.6237\n",
            "Step=4350000  Loss=0.6237\n",
            "Step=4360000  Loss=0.6237\n",
            "Step=4370000  Loss=0.6236\n",
            "Step=4380000  Loss=0.6236\n",
            "Step=4390000  Loss=0.6236\n",
            "Step=4400000  Loss=0.6235\n",
            "Step=4410000  Loss=0.6235\n",
            "Step=4420000  Loss=0.6235\n",
            "Step=4430000  Loss=0.6234\n",
            "Step=4440000  Loss=0.6234\n",
            "Step=4450000  Loss=0.6234\n",
            "Step=4460000  Loss=0.6233\n",
            "Step=4470000  Loss=0.6233\n",
            "Step=4480000  Loss=0.6233\n",
            "Step=4490000  Loss=0.6232\n",
            "Step=4500000  Loss=0.6232\n",
            "Step=4510000  Loss=0.6232\n",
            "Step=4520000  Loss=0.6232\n",
            "Step=4530000  Loss=0.6231\n",
            "Step=4540000  Loss=0.6231\n",
            "Step=4550000  Loss=0.6231\n",
            "Step=4560000  Loss=0.6230\n",
            "Step=4570000  Loss=0.6230\n",
            "Step=4580000  Loss=0.6230\n",
            "Step=4590000  Loss=0.6230\n",
            "Step=4600000  Loss=0.6229\n",
            "Step=4610000  Loss=0.6229\n",
            "Step=4620000  Loss=0.6229\n",
            "Step=4630000  Loss=0.6228\n",
            "Step=4640000  Loss=0.6228\n",
            "Step=4650000  Loss=0.6228\n",
            "Step=4660000  Loss=0.6228\n",
            "Step=4670000  Loss=0.6227\n",
            "Step=4680000  Loss=0.6227\n",
            "Step=4690000  Loss=0.6227\n",
            "Step=4700000  Loss=0.6227\n",
            "Step=4710000  Loss=0.6226\n",
            "Step=4720000  Loss=0.6226\n",
            "Step=4730000  Loss=0.6226\n",
            "Step=4740000  Loss=0.6226\n",
            "Step=4750000  Loss=0.6226\n",
            "Step=4760000  Loss=0.6225\n",
            "Step=4770000  Loss=0.6225\n",
            "Step=4780000  Loss=0.6225\n",
            "Step=4790000  Loss=0.6225\n",
            "Step=4800000  Loss=0.6224\n",
            "Step=4810000  Loss=0.6224\n",
            "Step=4820000  Loss=0.6224\n",
            "Step=4830000  Loss=0.6224\n",
            "Step=4840000  Loss=0.6224\n",
            "Step=4850000  Loss=0.6223\n",
            "Step=4860000  Loss=0.6223\n",
            "Step=4870000  Loss=0.6223\n",
            "Step=4880000  Loss=0.6223\n",
            "Step=4890000  Loss=0.6222\n",
            "Step=4900000  Loss=0.6222\n",
            "Step=4910000  Loss=0.6222\n",
            "Step=4920000  Loss=0.6222\n",
            "Step=4930000  Loss=0.6222\n",
            "Step=4940000  Loss=0.6221\n",
            "Step=4950000  Loss=0.6221\n",
            "Step=4960000  Loss=0.6221\n",
            "Step=4970000  Loss=0.6221\n",
            "Step=4980000  Loss=0.6221\n",
            "Step=4990000  Loss=0.6221\n",
            "Step=5000000  Loss=0.6220\n",
            "Training set mean squared error: 0.6220\n",
            "Training set r-squared scores: 0.5300\n",
            "Validation set mean squared error: 0.7029\n",
            "Validation set r-squared scores: 0.4933\n",
            "Testing set mean squared error: 0.6274\n",
            "Testing set r-squared scores: 0.5316\n",
            "***** Results of our linear regression model trained with half_mean_squared loss, alpha=2e-07 and T=5000000 on California housing prices dataset *****\n",
            "Step=10000  Loss=0.6506\n",
            "Step=20000  Loss=0.6447\n",
            "Step=30000  Loss=0.6391\n",
            "Step=40000  Loss=0.6337\n",
            "Step=50000  Loss=0.6285\n",
            "Step=60000  Loss=0.6235\n",
            "Step=70000  Loss=0.6185\n",
            "Step=80000  Loss=0.6136\n",
            "Step=90000  Loss=0.6089\n",
            "Step=100000  Loss=0.6042\n",
            "Step=110000  Loss=0.5996\n",
            "Step=120000  Loss=0.5951\n",
            "Step=130000  Loss=0.5907\n",
            "Step=140000  Loss=0.5864\n",
            "Step=150000  Loss=0.5821\n",
            "Step=160000  Loss=0.5779\n",
            "Step=170000  Loss=0.5738\n",
            "Step=180000  Loss=0.5697\n",
            "Step=190000  Loss=0.5657\n",
            "Step=200000  Loss=0.5618\n",
            "Step=210000  Loss=0.5580\n",
            "Step=220000  Loss=0.5542\n",
            "Step=230000  Loss=0.5505\n",
            "Step=240000  Loss=0.5469\n",
            "Step=250000  Loss=0.5433\n",
            "Step=260000  Loss=0.5397\n",
            "Step=270000  Loss=0.5363\n",
            "Step=280000  Loss=0.5329\n",
            "Step=290000  Loss=0.5295\n",
            "Step=300000  Loss=0.5262\n",
            "Step=310000  Loss=0.5230\n",
            "Step=320000  Loss=0.5198\n",
            "Step=330000  Loss=0.5167\n",
            "Step=340000  Loss=0.5136\n",
            "Step=350000  Loss=0.5106\n",
            "Step=360000  Loss=0.5076\n",
            "Step=370000  Loss=0.5047\n",
            "Step=380000  Loss=0.5018\n",
            "Step=390000  Loss=0.4990\n",
            "Step=400000  Loss=0.4962\n",
            "Step=410000  Loss=0.4935\n",
            "Step=420000  Loss=0.4908\n",
            "Step=430000  Loss=0.4882\n",
            "Step=440000  Loss=0.4856\n",
            "Step=450000  Loss=0.4830\n",
            "Step=460000  Loss=0.4805\n",
            "Step=470000  Loss=0.4780\n",
            "Step=480000  Loss=0.4756\n",
            "Step=490000  Loss=0.4732\n",
            "Step=500000  Loss=0.4708\n",
            "Step=510000  Loss=0.4685\n",
            "Step=520000  Loss=0.4662\n",
            "Step=530000  Loss=0.4640\n",
            "Step=540000  Loss=0.4618\n",
            "Step=550000  Loss=0.4596\n",
            "Step=560000  Loss=0.4575\n",
            "Step=570000  Loss=0.4554\n",
            "Step=580000  Loss=0.4533\n",
            "Step=590000  Loss=0.4513\n",
            "Step=600000  Loss=0.4493\n",
            "Step=610000  Loss=0.4473\n",
            "Step=620000  Loss=0.4453\n",
            "Step=630000  Loss=0.4434\n",
            "Step=640000  Loss=0.4416\n",
            "Step=650000  Loss=0.4397\n",
            "Step=660000  Loss=0.4379\n",
            "Step=670000  Loss=0.4361\n",
            "Step=680000  Loss=0.4343\n",
            "Step=690000  Loss=0.4326\n",
            "Step=700000  Loss=0.4309\n",
            "Step=710000  Loss=0.4292\n",
            "Step=720000  Loss=0.4275\n",
            "Step=730000  Loss=0.4259\n",
            "Step=740000  Loss=0.4243\n",
            "Step=750000  Loss=0.4227\n",
            "Step=760000  Loss=0.4212\n",
            "Step=770000  Loss=0.4196\n",
            "Step=780000  Loss=0.4181\n",
            "Step=790000  Loss=0.4166\n",
            "Step=800000  Loss=0.4152\n",
            "Step=810000  Loss=0.4137\n",
            "Step=820000  Loss=0.4123\n",
            "Step=830000  Loss=0.4109\n",
            "Step=840000  Loss=0.4095\n",
            "Step=850000  Loss=0.4082\n",
            "Step=860000  Loss=0.4069\n",
            "Step=870000  Loss=0.4055\n",
            "Step=880000  Loss=0.4042\n",
            "Step=890000  Loss=0.4030\n",
            "Step=900000  Loss=0.4017\n",
            "Step=910000  Loss=0.4005\n",
            "Step=920000  Loss=0.3993\n",
            "Step=930000  Loss=0.3981\n",
            "Step=940000  Loss=0.3969\n",
            "Step=950000  Loss=0.3957\n",
            "Step=960000  Loss=0.3946\n",
            "Step=970000  Loss=0.3934\n",
            "Step=980000  Loss=0.3923\n",
            "Step=990000  Loss=0.3912\n",
            "Step=1000000  Loss=0.3901\n",
            "Step=1010000  Loss=0.3891\n",
            "Step=1020000  Loss=0.3880\n",
            "Step=1030000  Loss=0.3870\n",
            "Step=1040000  Loss=0.3860\n",
            "Step=1050000  Loss=0.3850\n",
            "Step=1060000  Loss=0.3840\n",
            "Step=1070000  Loss=0.3830\n",
            "Step=1080000  Loss=0.3820\n",
            "Step=1090000  Loss=0.3811\n",
            "Step=1100000  Loss=0.3802\n",
            "Step=1110000  Loss=0.3792\n",
            "Step=1120000  Loss=0.3783\n",
            "Step=1130000  Loss=0.3774\n",
            "Step=1140000  Loss=0.3766\n",
            "Step=1150000  Loss=0.3757\n",
            "Step=1160000  Loss=0.3748\n",
            "Step=1170000  Loss=0.3740\n",
            "Step=1180000  Loss=0.3732\n",
            "Step=1190000  Loss=0.3723\n",
            "Step=1200000  Loss=0.3715\n",
            "Step=1210000  Loss=0.3707\n",
            "Step=1220000  Loss=0.3699\n",
            "Step=1230000  Loss=0.3692\n",
            "Step=1240000  Loss=0.3684\n",
            "Step=1250000  Loss=0.3677\n",
            "Step=1260000  Loss=0.3669\n",
            "Step=1270000  Loss=0.3662\n",
            "Step=1280000  Loss=0.3655\n",
            "Step=1290000  Loss=0.3648\n",
            "Step=1300000  Loss=0.3641\n",
            "Step=1310000  Loss=0.3634\n",
            "Step=1320000  Loss=0.3627\n",
            "Step=1330000  Loss=0.3620\n",
            "Step=1340000  Loss=0.3613\n",
            "Step=1350000  Loss=0.3607\n",
            "Step=1360000  Loss=0.3601\n",
            "Step=1370000  Loss=0.3594\n",
            "Step=1380000  Loss=0.3588\n",
            "Step=1390000  Loss=0.3582\n",
            "Step=1400000  Loss=0.3576\n",
            "Step=1410000  Loss=0.3570\n",
            "Step=1420000  Loss=0.3564\n",
            "Step=1430000  Loss=0.3558\n",
            "Step=1440000  Loss=0.3552\n",
            "Step=1450000  Loss=0.3546\n",
            "Step=1460000  Loss=0.3541\n",
            "Step=1470000  Loss=0.3535\n",
            "Step=1480000  Loss=0.3530\n",
            "Step=1490000  Loss=0.3524\n",
            "Step=1500000  Loss=0.3519\n",
            "Step=1510000  Loss=0.3514\n",
            "Step=1520000  Loss=0.3509\n",
            "Step=1530000  Loss=0.3504\n",
            "Step=1540000  Loss=0.3499\n",
            "Step=1550000  Loss=0.3494\n",
            "Step=1560000  Loss=0.3489\n",
            "Step=1570000  Loss=0.3484\n",
            "Step=1580000  Loss=0.3479\n",
            "Step=1590000  Loss=0.3474\n",
            "Step=1600000  Loss=0.3470\n",
            "Step=1610000  Loss=0.3465\n",
            "Step=1620000  Loss=0.3461\n",
            "Step=1630000  Loss=0.3456\n",
            "Step=1640000  Loss=0.3452\n",
            "Step=1650000  Loss=0.3448\n",
            "Step=1660000  Loss=0.3443\n",
            "Step=1670000  Loss=0.3439\n",
            "Step=1680000  Loss=0.3435\n",
            "Step=1690000  Loss=0.3431\n",
            "Step=1700000  Loss=0.3427\n",
            "Step=1710000  Loss=0.3423\n",
            "Step=1720000  Loss=0.3419\n",
            "Step=1730000  Loss=0.3415\n",
            "Step=1740000  Loss=0.3411\n",
            "Step=1750000  Loss=0.3407\n",
            "Step=1760000  Loss=0.3404\n",
            "Step=1770000  Loss=0.3400\n",
            "Step=1780000  Loss=0.3396\n",
            "Step=1790000  Loss=0.3393\n",
            "Step=1800000  Loss=0.3389\n",
            "Step=1810000  Loss=0.3386\n",
            "Step=1820000  Loss=0.3382\n",
            "Step=1830000  Loss=0.3379\n",
            "Step=1840000  Loss=0.3375\n",
            "Step=1850000  Loss=0.3372\n",
            "Step=1860000  Loss=0.3369\n",
            "Step=1870000  Loss=0.3366\n",
            "Step=1880000  Loss=0.3362\n",
            "Step=1890000  Loss=0.3359\n",
            "Step=1900000  Loss=0.3356\n",
            "Step=1910000  Loss=0.3353\n",
            "Step=1920000  Loss=0.3350\n",
            "Step=1930000  Loss=0.3347\n",
            "Step=1940000  Loss=0.3344\n",
            "Step=1950000  Loss=0.3341\n",
            "Step=1960000  Loss=0.3338\n",
            "Step=1970000  Loss=0.3335\n",
            "Step=1980000  Loss=0.3333\n",
            "Step=1990000  Loss=0.3330\n",
            "Step=2000000  Loss=0.3327\n",
            "Step=2010000  Loss=0.3324\n",
            "Step=2020000  Loss=0.3322\n",
            "Step=2030000  Loss=0.3319\n",
            "Step=2040000  Loss=0.3317\n",
            "Step=2050000  Loss=0.3314\n",
            "Step=2060000  Loss=0.3311\n",
            "Step=2070000  Loss=0.3309\n",
            "Step=2080000  Loss=0.3307\n",
            "Step=2090000  Loss=0.3304\n",
            "Step=2100000  Loss=0.3302\n",
            "Step=2110000  Loss=0.3299\n",
            "Step=2120000  Loss=0.3297\n",
            "Step=2130000  Loss=0.3295\n",
            "Step=2140000  Loss=0.3292\n",
            "Step=2150000  Loss=0.3290\n",
            "Step=2160000  Loss=0.3288\n",
            "Step=2170000  Loss=0.3286\n",
            "Step=2180000  Loss=0.3284\n",
            "Step=2190000  Loss=0.3281\n",
            "Step=2200000  Loss=0.3279\n",
            "Step=2210000  Loss=0.3277\n",
            "Step=2220000  Loss=0.3275\n",
            "Step=2230000  Loss=0.3273\n",
            "Step=2240000  Loss=0.3271\n",
            "Step=2250000  Loss=0.3269\n",
            "Step=2260000  Loss=0.3267\n",
            "Step=2270000  Loss=0.3265\n",
            "Step=2280000  Loss=0.3263\n",
            "Step=2290000  Loss=0.3261\n",
            "Step=2300000  Loss=0.3260\n",
            "Step=2310000  Loss=0.3258\n",
            "Step=2320000  Loss=0.3256\n",
            "Step=2330000  Loss=0.3254\n",
            "Step=2340000  Loss=0.3252\n",
            "Step=2350000  Loss=0.3251\n",
            "Step=2360000  Loss=0.3249\n",
            "Step=2370000  Loss=0.3247\n",
            "Step=2380000  Loss=0.3245\n",
            "Step=2390000  Loss=0.3244\n",
            "Step=2400000  Loss=0.3242\n",
            "Step=2410000  Loss=0.3240\n",
            "Step=2420000  Loss=0.3239\n",
            "Step=2430000  Loss=0.3237\n",
            "Step=2440000  Loss=0.3236\n",
            "Step=2450000  Loss=0.3234\n",
            "Step=2460000  Loss=0.3233\n",
            "Step=2470000  Loss=0.3231\n",
            "Step=2480000  Loss=0.3230\n",
            "Step=2490000  Loss=0.3228\n",
            "Step=2500000  Loss=0.3227\n",
            "Step=2510000  Loss=0.3225\n",
            "Step=2520000  Loss=0.3224\n",
            "Step=2530000  Loss=0.3222\n",
            "Step=2540000  Loss=0.3221\n",
            "Step=2550000  Loss=0.3220\n",
            "Step=2560000  Loss=0.3218\n",
            "Step=2570000  Loss=0.3217\n",
            "Step=2580000  Loss=0.3216\n",
            "Step=2590000  Loss=0.3214\n",
            "Step=2600000  Loss=0.3213\n",
            "Step=2610000  Loss=0.3212\n",
            "Step=2620000  Loss=0.3211\n",
            "Step=2630000  Loss=0.3209\n",
            "Step=2640000  Loss=0.3208\n",
            "Step=2650000  Loss=0.3207\n",
            "Step=2660000  Loss=0.3206\n",
            "Step=2670000  Loss=0.3205\n",
            "Step=2680000  Loss=0.3203\n",
            "Step=2690000  Loss=0.3202\n",
            "Step=2700000  Loss=0.3201\n",
            "Step=2710000  Loss=0.3200\n",
            "Step=2720000  Loss=0.3199\n",
            "Step=2730000  Loss=0.3198\n",
            "Step=2740000  Loss=0.3197\n",
            "Step=2750000  Loss=0.3196\n",
            "Step=2760000  Loss=0.3195\n",
            "Step=2770000  Loss=0.3193\n",
            "Step=2780000  Loss=0.3192\n",
            "Step=2790000  Loss=0.3191\n",
            "Step=2800000  Loss=0.3190\n",
            "Step=2810000  Loss=0.3189\n",
            "Step=2820000  Loss=0.3188\n",
            "Step=2830000  Loss=0.3187\n",
            "Step=2840000  Loss=0.3187\n",
            "Step=2850000  Loss=0.3186\n",
            "Step=2860000  Loss=0.3185\n",
            "Step=2870000  Loss=0.3184\n",
            "Step=2880000  Loss=0.3183\n",
            "Step=2890000  Loss=0.3182\n",
            "Step=2900000  Loss=0.3181\n",
            "Step=2910000  Loss=0.3180\n",
            "Step=2920000  Loss=0.3179\n",
            "Step=2930000  Loss=0.3178\n",
            "Step=2940000  Loss=0.3178\n",
            "Step=2950000  Loss=0.3177\n",
            "Step=2960000  Loss=0.3176\n",
            "Step=2970000  Loss=0.3175\n",
            "Step=2980000  Loss=0.3174\n",
            "Step=2990000  Loss=0.3173\n",
            "Step=3000000  Loss=0.3173\n",
            "Step=3010000  Loss=0.3172\n",
            "Step=3020000  Loss=0.3171\n",
            "Step=3030000  Loss=0.3170\n",
            "Step=3040000  Loss=0.3170\n",
            "Step=3050000  Loss=0.3169\n",
            "Step=3060000  Loss=0.3168\n",
            "Step=3070000  Loss=0.3167\n",
            "Step=3080000  Loss=0.3167\n",
            "Step=3090000  Loss=0.3166\n",
            "Step=3100000  Loss=0.3165\n",
            "Step=3110000  Loss=0.3164\n",
            "Step=3120000  Loss=0.3164\n",
            "Step=3130000  Loss=0.3163\n",
            "Step=3140000  Loss=0.3162\n",
            "Step=3150000  Loss=0.3162\n",
            "Step=3160000  Loss=0.3161\n",
            "Step=3170000  Loss=0.3160\n",
            "Step=3180000  Loss=0.3160\n",
            "Step=3190000  Loss=0.3159\n",
            "Step=3200000  Loss=0.3159\n",
            "Step=3210000  Loss=0.3158\n",
            "Step=3220000  Loss=0.3157\n",
            "Step=3230000  Loss=0.3157\n",
            "Step=3240000  Loss=0.3156\n",
            "Step=3250000  Loss=0.3156\n",
            "Step=3260000  Loss=0.3155\n",
            "Step=3270000  Loss=0.3154\n",
            "Step=3280000  Loss=0.3154\n",
            "Step=3290000  Loss=0.3153\n",
            "Step=3300000  Loss=0.3153\n",
            "Step=3310000  Loss=0.3152\n",
            "Step=3320000  Loss=0.3152\n",
            "Step=3330000  Loss=0.3151\n",
            "Step=3340000  Loss=0.3150\n",
            "Step=3350000  Loss=0.3150\n",
            "Step=3360000  Loss=0.3149\n",
            "Step=3370000  Loss=0.3149\n",
            "Step=3380000  Loss=0.3148\n",
            "Step=3390000  Loss=0.3148\n",
            "Step=3400000  Loss=0.3147\n",
            "Step=3410000  Loss=0.3147\n",
            "Step=3420000  Loss=0.3146\n",
            "Step=3430000  Loss=0.3146\n",
            "Step=3440000  Loss=0.3145\n",
            "Step=3450000  Loss=0.3145\n",
            "Step=3460000  Loss=0.3145\n",
            "Step=3470000  Loss=0.3144\n",
            "Step=3480000  Loss=0.3144\n",
            "Step=3490000  Loss=0.3143\n",
            "Step=3500000  Loss=0.3143\n",
            "Step=3510000  Loss=0.3142\n",
            "Step=3520000  Loss=0.3142\n",
            "Step=3530000  Loss=0.3141\n",
            "Step=3540000  Loss=0.3141\n",
            "Step=3550000  Loss=0.3141\n",
            "Step=3560000  Loss=0.3140\n",
            "Step=3570000  Loss=0.3140\n",
            "Step=3580000  Loss=0.3139\n",
            "Step=3590000  Loss=0.3139\n",
            "Step=3600000  Loss=0.3139\n",
            "Step=3610000  Loss=0.3138\n",
            "Step=3620000  Loss=0.3138\n",
            "Step=3630000  Loss=0.3137\n",
            "Step=3640000  Loss=0.3137\n",
            "Step=3650000  Loss=0.3137\n",
            "Step=3660000  Loss=0.3136\n",
            "Step=3670000  Loss=0.3136\n",
            "Step=3680000  Loss=0.3136\n",
            "Step=3690000  Loss=0.3135\n",
            "Step=3700000  Loss=0.3135\n",
            "Step=3710000  Loss=0.3134\n",
            "Step=3720000  Loss=0.3134\n",
            "Step=3730000  Loss=0.3134\n",
            "Step=3740000  Loss=0.3133\n",
            "Step=3750000  Loss=0.3133\n",
            "Step=3760000  Loss=0.3133\n",
            "Step=3770000  Loss=0.3132\n",
            "Step=3780000  Loss=0.3132\n",
            "Step=3790000  Loss=0.3132\n",
            "Step=3800000  Loss=0.3132\n",
            "Step=3810000  Loss=0.3131\n",
            "Step=3820000  Loss=0.3131\n",
            "Step=3830000  Loss=0.3131\n",
            "Step=3840000  Loss=0.3130\n",
            "Step=3850000  Loss=0.3130\n",
            "Step=3860000  Loss=0.3130\n",
            "Step=3870000  Loss=0.3129\n",
            "Step=3880000  Loss=0.3129\n",
            "Step=3890000  Loss=0.3129\n",
            "Step=3900000  Loss=0.3129\n",
            "Step=3910000  Loss=0.3128\n",
            "Step=3920000  Loss=0.3128\n",
            "Step=3930000  Loss=0.3128\n",
            "Step=3940000  Loss=0.3127\n",
            "Step=3950000  Loss=0.3127\n",
            "Step=3960000  Loss=0.3127\n",
            "Step=3970000  Loss=0.3127\n",
            "Step=3980000  Loss=0.3126\n",
            "Step=3990000  Loss=0.3126\n",
            "Step=4000000  Loss=0.3126\n",
            "Step=4010000  Loss=0.3126\n",
            "Step=4020000  Loss=0.3125\n",
            "Step=4030000  Loss=0.3125\n",
            "Step=4040000  Loss=0.3125\n",
            "Step=4050000  Loss=0.3125\n",
            "Step=4060000  Loss=0.3124\n",
            "Step=4070000  Loss=0.3124\n",
            "Step=4080000  Loss=0.3124\n",
            "Step=4090000  Loss=0.3124\n",
            "Step=4100000  Loss=0.3123\n",
            "Step=4110000  Loss=0.3123\n",
            "Step=4120000  Loss=0.3123\n",
            "Step=4130000  Loss=0.3123\n",
            "Step=4140000  Loss=0.3123\n",
            "Step=4150000  Loss=0.3122\n",
            "Step=4160000  Loss=0.3122\n",
            "Step=4170000  Loss=0.3122\n",
            "Step=4180000  Loss=0.3122\n",
            "Step=4190000  Loss=0.3122\n",
            "Step=4200000  Loss=0.3121\n",
            "Step=4210000  Loss=0.3121\n",
            "Step=4220000  Loss=0.3121\n",
            "Step=4230000  Loss=0.3121\n",
            "Step=4240000  Loss=0.3121\n",
            "Step=4250000  Loss=0.3120\n",
            "Step=4260000  Loss=0.3120\n",
            "Step=4270000  Loss=0.3120\n",
            "Step=4280000  Loss=0.3120\n",
            "Step=4290000  Loss=0.3120\n",
            "Step=4300000  Loss=0.3119\n",
            "Step=4310000  Loss=0.3119\n",
            "Step=4320000  Loss=0.3119\n",
            "Step=4330000  Loss=0.3119\n",
            "Step=4340000  Loss=0.3119\n",
            "Step=4350000  Loss=0.3118\n",
            "Step=4360000  Loss=0.3118\n",
            "Step=4370000  Loss=0.3118\n",
            "Step=4380000  Loss=0.3118\n",
            "Step=4390000  Loss=0.3118\n",
            "Step=4400000  Loss=0.3118\n",
            "Step=4410000  Loss=0.3117\n",
            "Step=4420000  Loss=0.3117\n",
            "Step=4430000  Loss=0.3117\n",
            "Step=4440000  Loss=0.3117\n",
            "Step=4450000  Loss=0.3117\n",
            "Step=4460000  Loss=0.3117\n",
            "Step=4470000  Loss=0.3117\n",
            "Step=4480000  Loss=0.3116\n",
            "Step=4490000  Loss=0.3116\n",
            "Step=4500000  Loss=0.3116\n",
            "Step=4510000  Loss=0.3116\n",
            "Step=4520000  Loss=0.3116\n",
            "Step=4530000  Loss=0.3116\n",
            "Step=4540000  Loss=0.3115\n",
            "Step=4550000  Loss=0.3115\n",
            "Step=4560000  Loss=0.3115\n",
            "Step=4570000  Loss=0.3115\n",
            "Step=4580000  Loss=0.3115\n",
            "Step=4590000  Loss=0.3115\n",
            "Step=4600000  Loss=0.3115\n",
            "Step=4610000  Loss=0.3115\n",
            "Step=4620000  Loss=0.3114\n",
            "Step=4630000  Loss=0.3114\n",
            "Step=4640000  Loss=0.3114\n",
            "Step=4650000  Loss=0.3114\n",
            "Step=4660000  Loss=0.3114\n",
            "Step=4670000  Loss=0.3114\n",
            "Step=4680000  Loss=0.3114\n",
            "Step=4690000  Loss=0.3113\n",
            "Step=4700000  Loss=0.3113\n",
            "Step=4710000  Loss=0.3113\n",
            "Step=4720000  Loss=0.3113\n",
            "Step=4730000  Loss=0.3113\n",
            "Step=4740000  Loss=0.3113\n",
            "Step=4750000  Loss=0.3113\n",
            "Step=4760000  Loss=0.3113\n",
            "Step=4770000  Loss=0.3113\n",
            "Step=4780000  Loss=0.3112\n",
            "Step=4790000  Loss=0.3112\n",
            "Step=4800000  Loss=0.3112\n",
            "Step=4810000  Loss=0.3112\n",
            "Step=4820000  Loss=0.3112\n",
            "Step=4830000  Loss=0.3112\n",
            "Step=4840000  Loss=0.3112\n",
            "Step=4850000  Loss=0.3112\n",
            "Step=4860000  Loss=0.3112\n",
            "Step=4870000  Loss=0.3111\n",
            "Step=4880000  Loss=0.3111\n",
            "Step=4890000  Loss=0.3111\n",
            "Step=4900000  Loss=0.3111\n",
            "Step=4910000  Loss=0.3111\n",
            "Step=4920000  Loss=0.3111\n",
            "Step=4930000  Loss=0.3111\n",
            "Step=4940000  Loss=0.3111\n",
            "Step=4950000  Loss=0.3111\n",
            "Step=4960000  Loss=0.3111\n",
            "Step=4970000  Loss=0.3110\n",
            "Step=4980000  Loss=0.3110\n",
            "Step=4990000  Loss=0.3110\n",
            "Step=5000000  Loss=0.3110\n",
            "Training set mean squared error: 0.6220\n",
            "Training set r-squared scores: 0.5300\n",
            "Validation set mean squared error: 0.7029\n",
            "Validation set r-squared scores: 0.4933\n",
            "Testing set mean squared error: 0.6274\n",
            "Testing set r-squared scores: 0.5316\n"
          ]
        }
      ],
      "source": [
        "# Load Diabetes and California housing prices dataset\n",
        "datasets = [\n",
        "    skdata.load_diabetes(),\n",
        "    skdata.fetch_california_housing()\n",
        "]\n",
        "dataset_names = [\n",
        "    'Diabetes',\n",
        "    'California housing prices'\n",
        "]\n",
        "\n",
        "# Loss functions to minimize\n",
        "dataset_loss_funcs = [\n",
        "    ['mean_squared', 'half_mean_squared'],\n",
        "    ['mean_squared', 'half_mean_squared']\n",
        "]\n",
        "\n",
        "# TODO: Select learning rates (alpha) for mean squared and half mean squared loss\n",
        "dataset_alphas = [\n",
        "    [1e-4, 2e-4],\n",
        "    [1e-7, 2e-7]\n",
        "]\n",
        "\n",
        "# TODO: Select number of steps (T) to train for mean squared and half mean squared loss\n",
        "dataset_Ts = [\n",
        "    [10000, 10000],\n",
        "    [5000000, 5000000]\n",
        "]\n",
        "\n",
        "for dataset_options in zip(datasets, dataset_names, dataset_loss_funcs, dataset_alphas, dataset_Ts):\n",
        "\n",
        "    dataset, dataset_name, loss_funcs, alphas, Ts = dataset_options\n",
        "\n",
        "    '''\n",
        "    Create the training, validation and testing splits\n",
        "    '''\n",
        "    x = dataset.data\n",
        "    y = dataset.target\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(x.shape[0])\n",
        "\n",
        "    # Choose the first 80% as training set, next 10% as validation and the rest as testing\n",
        "    train_split_idx = int(0.80 * x.shape[0])\n",
        "    val_split_idx = int(0.90 * x.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:val_split_idx]\n",
        "    test_indices = shuffled_indices[val_split_idx:]\n",
        "\n",
        "    # Select the examples from x and y to construct our training, validation, testing sets\n",
        "    x_train, y_train = x[train_indices, :], y[train_indices]\n",
        "    x_val, y_val = x[val_indices, :], y[val_indices]\n",
        "    x_test, y_test = x[test_indices, :], y[test_indices]\n",
        "\n",
        "    '''\n",
        "    Trains and tests Linear Regression model from scikit-learn\n",
        "    '''\n",
        "    # DONE: Initialize scikit-learn linear regression model without bias\n",
        "    model_scikit = LinearRegressionSciKit(fit_intercept=False)\n",
        "\n",
        "    # DONE: Trains scikit-learn linear regression model\n",
        "    model_scikit.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "    print('***** Results of scikit-learn linear regression model on {} dataset *****'.format(\n",
        "        dataset_name))\n",
        "\n",
        "    # DONE: Test model on training set\n",
        "    predictions_train = model_scikit.predict(x_train)\n",
        "\n",
        "    score_mse_train = skmetrics.mean_squared_error(y_train, predictions_train)\n",
        "    print('Training set mean squared error: {:.4f}'.format(score_mse_train))\n",
        "\n",
        "    score_r2_train = skmetrics.r2_score(y_train, predictions_train)\n",
        "    print('Training set r-squared scores: {:.4f}'.format(score_r2_train))\n",
        "\n",
        "    # DONE: Test model on validation set\n",
        "    predictions_val = model_scikit.predict(x_val)\n",
        "\n",
        "    score_mse_val = skmetrics.mean_squared_error(y_val, predictions_val)\n",
        "    print('Validation set mean squared error: {:.4f}'.format(score_mse_val))\n",
        "\n",
        "    score_r2_val = skmetrics.r2_score(y_val, predictions_val)\n",
        "    print('Validation set r-squared scores: {:.4f}'.format(score_r2_val))\n",
        "\n",
        "    # DONE: Test model on testing set\n",
        "    predictions_test = model_scikit.predict(x_test)\n",
        "\n",
        "    score_mse_test = skmetrics.mean_squared_error(y_test, predictions_test)\n",
        "    print('Testing set mean squared error: {:.4f}'.format(score_mse_test))\n",
        "\n",
        "    score_r2_test = skmetrics.r2_score(y_test, predictions_test)\n",
        "    print('Testing set r-squared scores: {:.4f}'.format(score_r2_test))\n",
        "\n",
        "    '''\n",
        "    Trains and tests our linear regression model using different solvers\n",
        "    '''\n",
        "\n",
        "    # Take the transpose of the dataset to match the dimensions discussed in lecture\n",
        "    # i.e., (N x d) to (d x N)\n",
        "    x_train = np.transpose(x_train, axes=(1, 0))\n",
        "    x_val = np.transpose(x_val, axes=(1, 0))\n",
        "    x_test = np.transpose(x_test, axes=(1, 0))\n",
        "    y_train = np.expand_dims(y_train, axis=0)\n",
        "    y_val = np.expand_dims(y_val, axis=0)\n",
        "    y_test = np.expand_dims(y_test, axis=0)\n",
        "\n",
        "    for loss_func, alpha, T in zip(loss_funcs, alphas, Ts):\n",
        "\n",
        "        # DONE: Initialize our linear regression model\n",
        "        model_ours = LinearRegressionGradientDescent()\n",
        "\n",
        "        print('***** Results of our linear regression model trained with {} loss, alpha={} and T={} on {} dataset *****'.format(\n",
        "            loss_func, alpha, T, dataset_name))\n",
        "\n",
        "        # DONE: Train model on training set\n",
        "        model_ours.fit(\n",
        "            x=x_train,\n",
        "            y=y_train,\n",
        "            T=T,\n",
        "            alpha=alpha,\n",
        "            loss_func=loss_func)\n",
        "\n",
        "        # DONE: Test model on training set using mean squared error and r-squared\n",
        "        predictions_train = model_ours.predict(x_train)\n",
        "\n",
        "        # Squeeze to remove extra dimensions before passing to r2_score\n",
        "        y_train_squeezed = np.squeeze(y_train)\n",
        "        predictions_train_squeezed = np.squeeze(predictions_train)\n",
        "\n",
        "        score_mse_train = skmetrics.mean_squared_error(y_train_squeezed, predictions_train_squeezed)\n",
        "        print('Training set mean squared error: {:.4f}'.format(score_mse_train))\n",
        "\n",
        "        score_r2_train = skmetrics.r2_score(y_train_squeezed, predictions_train_squeezed)\n",
        "        print('Training set r-squared scores: {:.4f}'.format(score_r2_train))\n",
        "\n",
        "        # DONE: Test model on validation set using mean squared error and r-squared\n",
        "        predictions_val = model_ours.predict(x_val)\n",
        "\n",
        "        # Squeeze to remove extra dimensions\n",
        "        y_val_squeezed = np.squeeze(y_val)\n",
        "        predictions_val_squeezed = np.squeeze(predictions_val)\n",
        "\n",
        "        score_mse_val = skmetrics.mean_squared_error(y_val_squeezed, predictions_val_squeezed)\n",
        "        print('Validation set mean squared error: {:.4f}'.format(score_mse_val))\n",
        "\n",
        "        score_r2_val = skmetrics.r2_score(y_val_squeezed, predictions_val_squeezed)\n",
        "        print('Validation set r-squared scores: {:.4f}'.format(score_r2_val))\n",
        "\n",
        "        # DONE: Test model on testing set using mean squared error and r-squared\n",
        "        predictions_test = model_ours.predict(x_test)\n",
        "\n",
        "        # Squeeze to remove extra dimensions\n",
        "        y_test_squeezed = np.squeeze(y_test)\n",
        "        predictions_test_squeezed = np.squeeze(predictions_test)\n",
        "\n",
        "        score_mse_test = skmetrics.mean_squared_error(y_test_squeezed, predictions_test_squeezed)\n",
        "        print('Testing set mean squared error: {:.4f}'.format(score_mse_test))\n",
        "\n",
        "        score_r2_test = skmetrics.r2_score(y_test_squeezed, predictions_test_squeezed)\n",
        "        print('Testing set r-squared scores: {:.4f}'.format(score_r2_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
