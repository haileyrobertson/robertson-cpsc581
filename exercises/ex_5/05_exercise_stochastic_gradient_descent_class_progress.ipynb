{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0fsGaVMMpwG"
      },
      "source": [
        "**Exercise 5: Stochastic Gradient Descent**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "*Student: Hailey Robertson*\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "\n",
        "4. Move the 05_exercise_stochastic_gradient_descent.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/05_exercise_stochastic_gradient_descent.ipynb\n",
        "```\n",
        "\n",
        "In this exercise, we will test stochastic gradient descent (SGD) and gradient descent (GD) for linear regression. We will plot out the loss of each model, test the models on training, validation, and testing sets, and benchmark their training time.\n",
        "\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training, and validation/testing scores.\n",
        "\n",
        "```\n",
        "Report validation and testing scores here.\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: Doe, Jane (Please write names in <Last Name, First Name> format)\n",
        "\n",
        "Collaboration details: Discussed ... implementation details with Jane Doe.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxeZsiCGC0J8"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uumvcyiQ-k21"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.metrics as skmetrics\n",
        "import sklearn.preprocessing as skpreprocessing\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "import time, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "np.random.seed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL72VSz90ycJ"
      },
      "source": [
        "Define colors for display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wypdCZOr0wWM"
      },
      "outputs": [],
      "source": [
        "# Create a list of colors for display\n",
        "colors = [\n",
        "    'tab:blue',\n",
        "    'tab:green',\n",
        "    'tab:red',\n",
        "    'tab:orange',\n",
        "    'tab:purple',\n",
        "    'tab:brown',\n",
        "    'tab:pink',\n",
        "    'tab:gray',\n",
        "    'tab:olive'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzKKgGJO-Pwe"
      },
      "source": [
        "Override the partial_fit function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDaN-xGM-PGG"
      },
      "outputs": [],
      "source": [
        "class SGDRegressorMSEVerbose(SGDRegressor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Define a list to hold loss values after each update\n",
        "        self.__losses = []\n",
        "\n",
        "    def partial_fit(self, X, y, sample_weight=None, do_logging=False):\n",
        "        '''\n",
        "        Performs a single update to the parameters\n",
        "\n",
        "        Arg(s):\n",
        "            X : numpy[float32]\n",
        "                N x d feature vector\n",
        "            y : numpy[float32]\n",
        "                N targets\n",
        "            sample_weight : numpy[float32]\n",
        "                Weights applied to individual samples.\n",
        "                If not provided, uniform weights are assumed.\n",
        "                Set this to None\n",
        "            do_logging : boolean\n",
        "                If set to True then log the loss\n",
        "        '''\n",
        "\n",
        "        # Check if coefficients are allocated\n",
        "        if getattr(self, \"coef_\", None) is None:\n",
        "\n",
        "            # Allocate coefficients\n",
        "            self._allocate_parameter_mem(\n",
        "                n_classes=1,\n",
        "                n_features=X.shape[1],\n",
        "                input_dtype=X.dtype,\n",
        "                coef_init=np.zeros([X.shape[1]]),\n",
        "                intercept_init=self.fit_intercept,\n",
        "                one_class=True)\n",
        "\n",
        "            self.intercept_ = self.offset_\n",
        "\n",
        "        # If we are logging\n",
        "        if do_logging:\n",
        "            # TODO: Make predictions on the training examples\n",
        "            y_hat = None\n",
        "\n",
        "            # TODO: Calculate loss (mean squared error)\n",
        "            loss = None\n",
        "\n",
        "            # TODO: Append loss to running loss\n",
        "\n",
        "\n",
        "        # TODO: Call partial_fit from parent class\n",
        "\n",
        "\n",
        "    def get_losses(self):\n",
        "        '''\n",
        "        Fetches the list of loss values\n",
        "\n",
        "        Returns:\n",
        "            list[float] : list of loss values\n",
        "        '''\n",
        "\n",
        "        return self.__losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ljMielQC7Lg"
      },
      "source": [
        "Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnK-8PlHpVyh"
      },
      "outputs": [],
      "source": [
        "# Create a large-scale synthetic dataset\n",
        "X, y = skdata.make_regression(n_samples=100000, n_features=100, noise=2)\n",
        "\n",
        "dataset_name = 'synthetic regression dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcdRJiY304cx"
      },
      "source": [
        "Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4GEJKzY036r"
      },
      "outputs": [],
      "source": [
        "# TODO: Set batch sizes to be 200, 500, 1000, 10000, 100000 (gradient descent)\n",
        "dataset_batch_sizes = [\n",
        "    200, 500, 1000, 10000, 100000\n",
        "]\n",
        "\n",
        "# TODO: Set learning schedules to be 'invscaling', 'invscaling', 'invscaling', 'invscaling', 'constant' (gradient descent)\n",
        "dataset_learning_schedules = [\n",
        "   'invscaling', 'invscaling', 'invscaling', 'invscaling', 'constant'\n",
        "]\n",
        "\n",
        "# Feel free to modify to gauge the behavior\n",
        "learning_rate = 1e-3\n",
        "max_iteration = 1000\n",
        "logging_frequency = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S05WQEejBAFU"
      },
      "source": [
        "Training and validation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enBPdhH2rRs4",
        "outputId": "9f66e84c-f452-41a6-e53f-2db7551adc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- learning rate: 0.0010  batch size: 200 time elapsed: 0.0000s -----\n",
            "Training set mean squared error: 0.0000\n",
            "Training set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "----- learning rate: 0.0010  batch size: 500 time elapsed: 0.0000s -----\n",
            "Training set mean squared error: 0.0000\n",
            "Training set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "----- learning rate: 0.0010  batch size: 1000 time elapsed: 0.0000s -----\n",
            "Training set mean squared error: 0.0000\n",
            "Training set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "----- learning rate: 0.0010  batch size: 10000 time elapsed: 0.0000s -----\n",
            "Training set mean squared error: 0.0000\n",
            "Training set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "----- learning rate: 0.0010  batch size: 100000 time elapsed: 0.0000s -----\n",
            "Training set mean squared error: 0.0000\n",
            "Training set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n",
            "Testing set mean squared error: 0.0000\n",
            "Testing set r-squared scores: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Define a dictionary to hold losses\n",
        "train_losses = {}\n",
        "\n",
        "for batch_size, learning_schedule in zip(dataset_batch_sizes, dataset_learning_schedules):\n",
        "\n",
        "    '''\n",
        "    Create the training, validation and testing splits\n",
        "    '''\n",
        "\n",
        "    # Shuffle the dataset based on sample indices\n",
        "    shuffled_indices = np.random.permutation(X.shape[0])\n",
        "\n",
        "    # Choose the first 80% as training set, next 10% as validation and the rest as testing\n",
        "    train_split_idx = int(0.80 * X.shape[0])\n",
        "    val_split_idx = int(0.90 * X.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[0:train_split_idx]\n",
        "    val_indices = shuffled_indices[train_split_idx:val_split_idx]\n",
        "    test_indices = shuffled_indices[val_split_idx:]\n",
        "\n",
        "    # Select the examples from X and y to construct our training, validation, testing sets\n",
        "    X_train, y_train = X[train_indices, :], y[train_indices]\n",
        "    X_val, y_val = X[val_indices, :], y[val_indices]\n",
        "    X_test, y_test = X[test_indices, :], y[test_indices]\n",
        "\n",
        "    '''\n",
        "    Train and validate linear regression on each dataset\n",
        "    '''\n",
        "    # TODO: Instantiate linear regression model using SGDRegressorMSEVerbose with\n",
        "    # loss='squared_error', penalty=None, alpha=0.0, learning_rate=learning_schedule, eta0=learning_rate\n",
        "    model_scikit = None\n",
        "\n",
        "    # TODO: Mark the starting time\n",
        "    time_start = None\n",
        "\n",
        "    # Iterate through the number of iterations\n",
        "    for iteration in range(max_iteration):\n",
        "\n",
        "        # TODO: Sample batch size number of examples from the training set\n",
        "        batch_indices = None\n",
        "        X_train_batch = None\n",
        "        y_train_batch = None\n",
        "\n",
        "        # TODO: Check if we will log\n",
        "        do_logging = None\n",
        "\n",
        "        # TODO: Perform a single update using the batch\n",
        "\n",
        "\n",
        "    # TODO: Compute the time elapse\n",
        "    time_elapsed = 0.0\n",
        "\n",
        "    # TODO: Get losses logged within the model\n",
        "    losses = None\n",
        "\n",
        "    # TODO: Set losses as value to the key (learning_rate, batch_size, time_elapsed)\n",
        "\n",
        "\n",
        "    print('----- learning rate: {:.4f}  batch size: {} time elapsed: {:.4f}s -----'.format(\n",
        "        learning_rate, batch_size, time_elapsed))\n",
        "\n",
        "    # TODO: Test model on training set\n",
        "    predictions_train = None\n",
        "\n",
        "    score_mse_train = 0.0\n",
        "    print('Training set mean squared error: {:.4f}'.format(score_mse_train))\n",
        "\n",
        "    score_r2_train = 0.0\n",
        "    print('Training set r-squared scores: {:.4f}'.format(score_r2_train))\n",
        "\n",
        "    # TODO: Test model on validation set\n",
        "    predictions_val = None\n",
        "\n",
        "    score_mse_val = 0.0\n",
        "    print('Testing set mean squared error: {:.4f}'.format(score_mse_val))\n",
        "\n",
        "    score_r2_val = 0.0\n",
        "    print('Testing set r-squared scores: {:.4f}'.format(score_r2_val))\n",
        "\n",
        "    # TODO: Test model on testing set\n",
        "    predictions_test = None\n",
        "\n",
        "    score_mse_test = 0.0\n",
        "    print('Testing set mean squared error: {:.4f}'.format(score_mse_test))\n",
        "\n",
        "    score_r2_test = 0.0\n",
        "    print('Testing set r-squared scores: {:.4f}'.format(score_r2_test))\n",
        "\n",
        "# TODO: Create figure of figsize=(10, 10)\n",
        "fig = None\n",
        "ax = None\n",
        "\n",
        "# Iterate through losses\n",
        "for (key, losses), c in zip(train_losses.items(), colors):\n",
        "\n",
        "    # TODO: Unpack key as learning_rate, batch_size, time_elapsed\n",
        "\n",
        "\n",
        "    # TODO: Plot iterations (x-axis), losses (y-axis), with label of 'learning_rate={:.4f}, batch_size={}, time_elapsed={:.2f}s', and color c\n",
        "\n",
        "\n",
        "    # TODO: Set title as 'SGD vs GD on synthetic regression dataset'\n",
        "\n",
        "\n",
        "    # TODO: Set xlabel as 'Iteration'\n",
        "\n",
        "\n",
        "    # TODO: Set ylabel as 'Loss'\n",
        "\n",
        "\n",
        "    # TODO: Show legend\n",
        "    pass\n",
        "\n",
        "# Show plots\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
